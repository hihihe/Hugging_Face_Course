{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T03:50:18.819557Z",
     "iopub.status.busy": "2022-03-05T03:50:18.819557Z",
     "iopub.status.idle": "2022-03-05T03:50:18.841557Z",
     "shell.execute_reply": "2022-03-05T03:50:18.840589Z",
     "shell.execute_reply.started": "2022-03-05T03:50:18.819557Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 更改缓存路径\n",
    "os.environ[\"HF_HOME\"] = \"D:/huggingface\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"D:/huggingface/datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://chushi123.oss-cn-beijing.aliyuncs.com/img/202203051058629.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在将文本拆分为子标记之前（根据其模型），标记器执行两个步骤：规范化和预标记化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "规范化normalization步骤涉及一些常规清理，例如删除不必要的空格、统一为小写和删除重音符号。如果您熟悉Unicode 规范化（例如 NFC 或 NFKC），这也是标记器可能应用的东西。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同的模型架构，规范化过程是不一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://chushi123.oss-cn-beijing.aliyuncs.com/img/202203051137435.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T03:50:23.253146Z",
     "iopub.status.busy": "2022-03-05T03:50:23.253146Z",
     "iopub.status.idle": "2022-03-05T03:50:33.634334Z",
     "shell.execute_reply": "2022-03-05T03:50:33.634334Z",
     "shell.execute_reply.started": "2022-03-05T03:50:23.253146Z"
    },
    "id": "Ye4ecFwLzoZ-",
    "outputId": "8fcd0035-812b-4074-d279-93f4261aacfe",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤗 Transformerstokenizer有一个名为backend_tokenizer的属性，它提供对来自 🤗 Tokenizers 库的底层标记器的访问："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizers.Tokenizer对象的normalizer属性有一个normalize_str()方法，我们可以使用它来查看normalization是如何执行的："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此示例中，由于我们选择了bert-base-uncased检查点，因此normalization应用了小写字母并删除了重音符号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T03:50:37.545608Z",
     "iopub.status.busy": "2022-03-05T03:50:37.544609Z",
     "iopub.status.idle": "2022-03-05T03:50:37.559635Z",
     "shell.execute_reply": "2022-03-05T03:50:37.559635Z",
     "shell.execute_reply.started": "2022-03-05T03:50:37.545608Z"
    },
    "id": "kMz722tpzoZ_",
    "outputId": "1621eb3b-d614-438e-fcc7-02d76e3ce93c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  pre-tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre-tokenization应用一些规则实现了对文本的初步切分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://chushi123.oss-cn-beijing.aliyuncs.com/img/202203051154902.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同模型的pre-tokenization是不一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://chushi123.oss-cn-beijing.aliyuncs.com/img/202203051157963.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T03:52:59.012631Z",
     "iopub.status.busy": "2022-03-05T03:52:59.012631Z",
     "iopub.status.idle": "2022-03-05T03:52:59.030653Z",
     "shell.execute_reply": "2022-03-05T03:52:59.029686Z",
     "shell.execute_reply.started": "2022-03-05T03:52:59.012631Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 同一个output里显示多个输出结果\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bert的分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T03:51:27.677004Z",
     "iopub.status.busy": "2022-03-05T03:51:27.677004Z",
     "iopub.status.idle": "2022-03-05T03:51:27.692030Z",
     "shell.execute_reply": "2022-03-05T03:51:27.691031Z",
     "shell.execute_reply.started": "2022-03-05T03:51:27.677004Z"
    },
    "id": "1PxO-4GuzoaA",
    "outputId": "9b019f53-a6fb-4876-ee11-e28e7f0f9542",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('how', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (16, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'hello', ',', 'how', 'are', 'you', '?', '[SEP]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")\n",
    "tokenizer(\"Hello, how are  you?\").tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert的分词器会将多个空格处理为一个空格，are和you的索引跳跃可以说明这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2的分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T03:51:45.205484Z",
     "iopub.status.busy": "2022-03-05T03:51:45.205484Z",
     "iopub.status.idle": "2022-03-05T03:51:54.630971Z",
     "shell.execute_reply": "2022-03-05T03:51:54.630971Z",
     "shell.execute_reply.started": "2022-03-05T03:51:45.205484Z"
    },
    "id": "6-2zA03wzoaA",
    "outputId": "a9a14071-afcf-4a24-c795-0d4d61dd2ce7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('Ġhow', (6, 10)),\n",
       " ('Ġare', (10, 14)),\n",
       " ('Ġ', (14, 15)),\n",
       " ('Ġyou', (15, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'Ġhow', 'Ġare', 'Ġ', 'Ġyou', '?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")\n",
    "tokenizer(\"Hello, how are  you?\").tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2也会在空格和标点符号上拆分，但它会保留空格并用Ġ符号替换它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5的分词器基于SentencePiece的Unigram subword tokenization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T03:51:59.129950Z",
     "iopub.status.busy": "2022-03-05T03:51:59.129950Z",
     "iopub.status.idle": "2022-03-05T03:52:07.687161Z",
     "shell.execute_reply": "2022-03-05T03:52:07.686415Z",
     "shell.execute_reply.started": "2022-03-05T03:51:59.129950Z"
    },
    "id": "wAUpKA8LzoaB",
    "outputId": "7fdb4899-e295-44c9-d6c0-0082ee357c3d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁Hello,', (0, 6)),\n",
       " ('▁how', (7, 10)),\n",
       " ('▁are', (11, 14)),\n",
       " ('▁you?', (16, 20))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['▁Hello', ',', '▁how', '▁are', '▁you', '?', '</s>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")\n",
    "tokenizer(\"Hello, how are  you?\").tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5分词器与 GPT-2 分词器一样，此分词器保留空格并用特定分号 （_） 替换它们。另请注意，默认情况下，它在句子的开头（在 Hello 之前）添加了一个空格，并忽略了 are 和 you 之间的双倍空格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Normalization and pre-tokenization",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
