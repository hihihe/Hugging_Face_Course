| Model         | 字节对编码 (BPE) 最初是作为一种压缩文本的算法开发的，然后在预训练 GPT 模型时被 OpenAI 用于标记化。( GPT、GPT-2、RoBERTa、BART 和 DeBERTa) | WordPiece 是谷歌为预训练 BERT 而开发的标记化算法( DistilBERT、MobileBERT、Funnel Transformers 和 MPNET) | Unigram 算法经常用于 SentencePiece(AlBERT、T5、mBART、Big Bird 和 XLNet ) |
| ------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Training      | 从小词汇量开始，学习规则合并token                            | 从小词汇量开始，学习规则合并token                            | 从大量词汇开始，学习规则以删除token                          |
| Training step | 合并与最常见对对应的标记                                     | 根据对的频率将对应于对的令牌与最佳分数合并，在每个单独的令牌频率较低的对中赋予特权 | 删除词汇表中的所有标记，以最大程度地减少在整个语料库上计算的损失 |
| Learns        | 合并规则和词汇表                                             | Just a vocabulary                                            | 每个token都有一个分数的词汇表                                |
| Encoding      | 将单词拆分为字符，并应用在训练期间学到的合并                 | 查找从词汇表中开头开始的最长子单词，然后对单词的其余部分执行相同的操作 | 使用训练期间学到的分数查找最有可能拆分为token的标记          |