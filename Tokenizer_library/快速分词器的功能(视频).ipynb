{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "332da31a-29ce-4f1d-94db-c2d2b22f5eda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T04:30:23.638706Z",
     "iopub.status.busy": "2022-03-05T04:30:23.638706Z",
     "iopub.status.idle": "2022-03-05T04:30:23.648681Z",
     "shell.execute_reply": "2022-03-05T04:30:23.648681Z",
     "shell.execute_reply.started": "2022-03-05T04:30:23.638706Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 更改缓存路径\n",
    "os.environ[\"HF_HOME\"] = \"D:/huggingface\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"D:/huggingface/datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44dfd8fb-3597-45f0-b09e-289cb23ff653",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T04:30:24.469340Z",
     "iopub.status.busy": "2022-03-05T04:30:24.469340Z",
     "iopub.status.idle": "2022-03-05T04:30:34.720691Z",
     "shell.execute_reply": "2022-03-05T04:30:34.719934Z",
     "shell.execute_reply.started": "2022-03-05T04:30:24.469340Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42926cff-0ad7-4119-9b70-50ade7a6ed02",
   "metadata": {},
   "source": [
    "# 当执行tokenization时，我们会丢失一些信息，并且也不容易看出token属于哪一个单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe1fb07-f7cc-494c-b007-b20c2964e036",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T04:30:36.129541Z",
     "iopub.status.busy": "2022-03-05T04:30:36.128519Z",
     "iopub.status.idle": "2022-03-05T04:30:36.140520Z",
     "shell.execute_reply": "2022-03-05T04:30:36.140520Z",
     "shell.execute_reply.started": "2022-03-05T04:30:36.128519Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 19082, 117, 1293, 1132, 1128, 136, 102]\n",
      "[101, 19082, 117, 1293, 1132, 1128, 136, 102]\n"
     ]
    }
   ],
   "source": [
    "# bert分词器会删除掉重复的空格\n",
    "print(tokenizer(\"hello, how are you?\")[\"input_ids\"])\n",
    "print(tokenizer(\"hello, how are         you?\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1cf3b9-1953-499e-b596-80997595be17",
   "metadata": {},
   "source": [
    "![](https://chushi123.oss-cn-beijing.aliyuncs.com/img/202203031634218.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7e2ab-0192-4987-9ca2-705cedff6166",
   "metadata": {},
   "source": [
    "# Fast tokenizers能保持单词和token的对应关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8618e85a-1a9f-47d7-bd5b-47417ce8cd1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T08:38:00.331539Z",
     "iopub.status.busy": "2022-03-03T08:38:00.330539Z",
     "iopub.status.idle": "2022-03-03T08:38:08.551735Z",
     "shell.execute_reply": "2022-03-03T08:38:08.551735Z",
     "shell.execute_reply.started": "2022-03-03T08:38:00.331539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Let', \"'\", 's', 'talk', 'about', 'token', '##izer', '##s', 'super', '##power', '##s', '.', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 6, 6, 7, None]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "encoding = tokenizer(\"Let's talk about tokenizers superpowers.\")\n",
    "print(encoding.tokens())\n",
    "print(encoding.word_ids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0ffb9e-d15e-4ba8-9e21-0bd97cf41639",
   "metadata": {},
   "source": [
    "![](https://chushi123.oss-cn-beijing.aliyuncs.com/img/202203031638253.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e15dd-7aa9-4146-a582-8d6722726665",
   "metadata": {},
   "source": [
    " ## 甚至能保持token与原始文本的索引区间对应关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aad7a1c0-8ec8-461b-9491-70d2dfdd5255",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T08:47:49.882677Z",
     "iopub.status.busy": "2022-03-03T08:47:49.881677Z",
     "iopub.status.idle": "2022-03-03T08:47:59.209857Z",
     "shell.execute_reply": "2022-03-03T08:47:59.209857Z",
     "shell.execute_reply.started": "2022-03-03T08:47:49.882677Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Let', \"'\", 's', 'talk', 'about', 'token', '##izer', '##s', 'super', '##power', '##s', '.', '[SEP]']\n",
      "[(0, 0), (0, 3), (3, 4), (4, 5), (6, 10), (11, 16), (17, 22), (22, 26), (26, 27), (28, 33), (33, 38), (38, 39), (39, 40), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "encoding = tokenizer(\n",
    "    \"Let's talk about tokenizers superpowers.\", return_offsets_mapping=True\n",
    ")\n",
    "print(encoding.tokens())\n",
    "print(encoding[\"offset_mapping\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58e1423-b9c5-4e1a-bd60-9666c525d463",
   "metadata": {},
   "source": [
    "![](https://chushi123.oss-cn-beijing.aliyuncs.com/img/202203031646501.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcacacdc-fad9-4f3c-ad60-561d419c87cf",
   "metadata": {},
   "source": [
    "## 获得单词在原始文本的索引区间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a7f116d-afe2-4257-878e-b4a065a9caa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-03T08:52:40.527467Z",
     "iopub.status.busy": "2022-03-03T08:52:40.527467Z",
     "iopub.status.idle": "2022-03-03T08:52:40.532467Z",
     "shell.execute_reply": "2022-03-03T08:52:40.532467Z",
     "shell.execute_reply.started": "2022-03-03T08:52:40.527467Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 27\n"
     ]
    }
   ],
   "source": [
    "# 获得第5个单词tokenizers在原始文本的索引区间\n",
    "start, end = encoding.word_to_chars(5)\n",
    "print(start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cdd65d-7c9d-4e37-9f17-9f5d42bd0e93",
   "metadata": {},
   "source": [
    "## Fast tokenizers保持了原始文本产生的每一个token在原始文本的对应区间索引"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5429b087-d81d-4c14-bd53-7b25dec5d69d",
   "metadata": {},
   "source": [
    "![](https://chushi123.oss-cn-beijing.aliyuncs.com/img/202203031707110.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e459cc8-5aea-43a8-a818-0037392e4660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
