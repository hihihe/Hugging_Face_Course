{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:23:42.550449Z",
     "iopub.status.busy": "2022-03-04T02:23:42.549449Z",
     "iopub.status.idle": "2022-03-04T02:23:42.562452Z",
     "shell.execute_reply": "2022-03-04T02:23:42.561457Z",
     "shell.execute_reply.started": "2022-03-04T02:23:42.550449Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 更改缓存路径\n",
    "os.environ[\"HF_HOME\"] = \"D:/huggingface\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"D:/huggingface/datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizer编码文本的处理过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://chushi123.oss-cn-beijing.aliyuncs.com/img/202203031553359.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tokenizer(sequence)`方法实现了**tokenization**，处理**special tokens**和转化为**input ids**三个过程。\n",
    "- `tokenizer.tokenize(sequence)`来实现**tokenization**这个过程，输出是 list of strings, or tokens。\n",
    "- `tokenizer.convert_tokens_to_ids(tokens)`来实现将**tokens转换为ids**。\n",
    "\n",
    "注意：tokenizer(sequence)方法，可能会在开头添加[CLS]结尾添加[SEP]等特殊词。这是因为模型是用这些进行预训练的，所以为了获得相同的推理结果，我们还需要添加它们。注意有些ckeckpoint不加特殊词，或者加不同的词；模型也可以仅在开头或仅在结尾添加这些特殊词。在任何情况下，tokenizer()都会自动处理这些。。。而使用tokenizer.tokenize(sequence)则不会处理这些。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:23:43.109478Z",
     "iopub.status.busy": "2022-03-04T02:23:43.109478Z",
     "iopub.status.idle": "2022-03-04T02:23:53.251320Z",
     "shell.execute_reply": "2022-03-04T02:23:53.250319Z",
     "shell.execute_reply.started": "2022-03-04T02:23:43.109478Z"
    },
    "id": "9O4cJu_B7NOo",
    "outputId": "7fa8bc73-f011-42a8-e67b-619718ddf77c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "print(type(encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 分词器的输出不是一个简单的 Python 字典；我们得到的实际上是一个特殊的 BatchEncoding 对象。它是字典的子类（这就是为什么我们之前能够毫无问题地索引到该结果），但它具有主要由快速分词器使用的附加方法。\n",
    "* 除了它们的并行化能力之外，快速标记器的关键功能是它们始终跟踪最终标记来自的文本的原始跨度——我们称之为偏移映射的特性。这反过来又解锁了一些功能，例如将每个单词映射到它生成的标记或将原始文本的每个字符映射到它内部的标记，反之亦然。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 检查是否是快速分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们有两种方法来检查我们的分词器是快还是慢。我们可以检查tokenizer的is_fast属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:23:53.252323Z",
     "iopub.status.busy": "2022-03-04T02:23:53.252323Z",
     "iopub.status.idle": "2022-03-04T02:23:53.266320Z",
     "shell.execute_reply": "2022-03-04T02:23:53.266320Z",
     "shell.execute_reply.started": "2022-03-04T02:23:53.252323Z"
    },
    "id": "HJ0uA2zN7NOq",
    "outputId": "f89aec6a-91aa-44bd-f2f5-4d0b25ad9532",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:23:53.267321Z",
     "iopub.status.busy": "2022-03-04T02:23:53.267321Z",
     "iopub.status.idle": "2022-03-04T02:23:53.281321Z",
     "shell.execute_reply": "2022-03-04T02:23:53.281321Z",
     "shell.execute_reply.started": "2022-03-04T02:23:53.267321Z"
    },
    "id": "iA9LUXSb7NOq",
    "outputId": "a6954670-d66e-4cfd-bb23-4d4a9a741ae0",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看分词的token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:23:53.283320Z",
     "iopub.status.busy": "2022-03-04T02:23:53.283320Z",
     "iopub.status.idle": "2022-03-04T02:23:53.296319Z",
     "shell.execute_reply": "2022-03-04T02:23:53.296319Z",
     "shell.execute_reply.started": "2022-03-04T02:23:53.283320Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1422, 1271, 1110, 156, 7777, 2497, 1394, 1105, 146, 1250, 1120, 20164, 10932, 10289, 1107, 6010, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:23:53.297319Z",
     "iopub.status.busy": "2022-03-04T02:23:53.297319Z",
     "iopub.status.idle": "2022-03-04T02:23:53.313321Z",
     "shell.execute_reply": "2022-03-04T02:23:53.312322Z",
     "shell.execute_reply.started": "2022-03-04T02:23:53.297319Z"
    },
    "id": "fdDI6qY87NOr",
    "outputId": "022ace62-47b1-40fc-ec84-91678913aa74",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'S',\n",
       " '##yl',\n",
       " '##va',\n",
       " '##in',\n",
       " 'and',\n",
       " 'I',\n",
       " 'work',\n",
       " 'at',\n",
       " 'Hu',\n",
       " '##gging',\n",
       " 'Face',\n",
       " 'in',\n",
       " 'Brooklyn',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用word_ids()方法获取每个token来自的单词的索引"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这种情况下，索引 5 处的标记是##yl，它是原始句子中单词“Sylvain”的一部分。我们还可以使用word_ids()方法获取每个token来自的单词的索引："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:23:53.315332Z",
     "iopub.status.busy": "2022-03-04T02:23:53.314323Z",
     "iopub.status.idle": "2022-03-04T02:23:53.329320Z",
     "shell.execute_reply": "2022-03-04T02:23:53.328320Z",
     "shell.execute_reply.started": "2022-03-04T02:23:53.315332Z"
    },
    "id": "qfhWXDDP7NOr",
    "outputId": "6a68debc-ac16-4eff-d22e-e944de8b9a06",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**我们可以看到，tokenizer 的特殊标记[CLS]和[SEP]被映射到None，然后每个标记都被映射到它起源的单词。这对于确定标记是否位于单词的开头或两个标记是否在同一个单词中特别有用。我们可以依赖##前缀，但它只适用于类似 BERT 的分词器；此方法适用于任何类型的标记器，只要它是快速的。在下一章中，我们将看到如何使用此功能将每个单词的标签正确地应用于命名实体识别 (NER) 和词性 (POS) 标记等任务中的标记。我们还可以使用它来掩盖来自掩码语言建模（一种称为全词掩码的技术）中来自同一单词的所有标记。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 单词是什么的概念很复杂。例如，“I'll”（“I will”的缩写）算作一个词还是两个词？它实际上取决于分词器和它应用的预分词操作。一些分词器只是在空格上拆分，所以他们会将其视为一个词。其他人在空格顶部使用标点符号，因此将其视为两个词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用word_to_chars()将单词的索引转化为单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:23:53.330319Z",
     "iopub.status.busy": "2022-03-04T02:23:53.330319Z",
     "iopub.status.idle": "2022-03-04T02:23:53.345331Z",
     "shell.execute_reply": "2022-03-04T02:23:53.344319Z",
     "shell.execute_reply.started": "2022-03-04T02:23:53.330319Z"
    },
    "id": "TTJzmBOL7NOs",
    "outputId": "446136a6-0c9e-418e-e285-b4670876bddc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sylvain'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将单词的索引id转化为单词字符串，实际返回的是单词字符串的起始和终止位置索引，然后对原始输入字符串进行切片，就可以看到单词\n",
    "# the word_ids() method told us that ##yl is part of the word at index 3, but which word is it in the sentence? We can find out like this:\n",
    "start, end = encoding.word_to_chars(3)\n",
    "print(start, end)\n",
    "example[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inside the token-classification pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:23:53.346321Z",
     "iopub.status.busy": "2022-03-04T02:23:53.346321Z",
     "iopub.status.idle": "2022-03-04T02:24:08.124642Z",
     "shell.execute_reply": "2022-03-04T02:24:08.124642Z",
     "shell.execute_reply.started": "2022-03-04T02:23:53.346321Z"
    },
    "id": "brrYHV-47NOs",
    "outputId": "bdaf267c-537d-4942-fc9d-b5a0eb3df6cf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': 0.99938285,\n",
       "  'index': 4,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99815494,\n",
       "  'index': 5,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99590707,\n",
       "  'index': 6,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.99923277,\n",
       "  'index': 7,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9738931,\n",
       "  'index': 12,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.976115,\n",
       "  'index': 13,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': 0.9887976,\n",
       "  'index': 14,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9932106,\n",
       "  'index': 16,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该模型正确地将“Sylvain”生成的每个token识别为一个人，将“Hugging Face”生成的每个token识别为一个组织，将“Brooklyn”生成的每个token识别为一个位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**我们还可以要求pipeline将对应于同一实体的标记组合在一起：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:24:08.125643Z",
     "iopub.status.busy": "2022-03-04T02:24:08.125643Z",
     "iopub.status.idle": "2022-03-04T02:24:22.624835Z",
     "shell.execute_reply": "2022-03-04T02:24:22.624835Z",
     "shell.execute_reply.started": "2022-03-04T02:24:08.125643Z"
    },
    "id": "91QEZC337NOt",
    "outputId": "e802c83d-2d68-4682-88dc-d352ec5223fa",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选取的aggregation_strategy将更改为每个分组实体计算的分数。\"simple\"分数只是给定实体中**每个标记的分数的平均值**：例如，“Sylvain”的分数是我们在前面示例中看到的标记、S、##yl、##va和##in的分数的平均值。其他可用的策略是：\n",
    "\n",
    "* \"first\"，其中每个实体的分数是该实体的第一个令牌的分数（因此对于“Sylvain”，它将是 0.993828，令牌的分数S）\n",
    "* \"max\"，其中每个实体的得分是该实体中标记的最大得分（因此对于“Hugging Face”，它将是 0.98879766，即“Face”的得分）\n",
    "* \"average\"，其中每个实体的得分是组成该实体的**单词得分的平均值**（因此，对于“Sylvain”，与\"simple\"策略没有区别，但“Hugging Face”的得分为 0.9819，即“Hugging”的得分为 0.975，“Face”的得分为 0.98879）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:24:22.626831Z",
     "iopub.status.busy": "2022-03-04T02:24:22.626831Z",
     "iopub.status.idle": "2022-03-04T02:24:36.029497Z",
     "shell.execute_reply": "2022-03-04T02:24:36.029497Z",
     "shell.execute_reply.started": "2022-03-04T02:24:22.626831Z"
    },
    "id": "S4NSnwjZ7NOt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:24:36.030498Z",
     "iopub.status.busy": "2022-03-04T02:24:36.030498Z",
     "iopub.status.idle": "2022-03-04T02:24:36.045500Z",
     "shell.execute_reply": "2022-03-04T02:24:36.045500Z",
     "shell.execute_reply.started": "2022-03-04T02:24:36.030498Z"
    },
    "id": "N8PTk9H47NOu",
    "outputId": "3657a7dc-2b11-4114-aa4c-44a1960096fc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 19])\n",
      "torch.Size([1, 19, 9])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"input_ids\"].shape)\n",
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**我们有一个包含 19 个标记的 1 个序列的批次，模型有 9 个不同的标签，因此模型的输出具有 1 x 19 x 9 的形状。与文本分类管道一样，我们使用 softmax 函数来转换这些 logits到概率，我们使用 argmax 来获得预测（注意我们可以在 logits 上使用 argmax，因为 softmax 不会改变顺序）：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:24:36.046499Z",
     "iopub.status.busy": "2022-03-04T02:24:36.046499Z",
     "iopub.status.idle": "2022-03-04T02:24:36.061500Z",
     "shell.execute_reply": "2022-03-04T02:24:36.061500Z",
     "shell.execute_reply.started": "2022-03-04T02:24:36.046499Z"
    },
    "id": "_CfTMWW27NOv",
    "outputId": "284df08a-95bf-4ec7-f44f-13aa15fec40f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in', 'Brooklyn', '.', '[SEP]']\n",
      "19\n",
      "[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()\n",
    "print(inputs.tokens())\n",
    "print(len(inputs.tokens()))\n",
    "print(predictions)\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://chushi123.oss-cn-beijing.aliyuncs.com/img/202203041038648.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model.config.id2label属性包含索引到标签的映射，我们可以使用这些映射来理解预测：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:24:36.062501Z",
     "iopub.status.busy": "2022-03-04T02:24:36.062501Z",
     "iopub.status.idle": "2022-03-04T02:24:36.077500Z",
     "shell.execute_reply": "2022-03-04T02:24:36.077500Z",
     "shell.execute_reply.started": "2022-03-04T02:24:36.062501Z"
    },
    "id": "t1CshPUh7NOw",
    "outputId": "06ca7f3d-2319-46e3-97ce-1cf0a7daaae2",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-MISC',\n",
       " 2: 'I-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-LOC',\n",
       " 8: 'I-LOC'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如我们之前看到的，有 9 个标签：O是不在任何命名实体中的标记的标签（它代表“外部”），然后我们为每种类型的实体（杂项、人员、组织、和位置）。标签B-XXX指示令牌位于实体的开头，XXX标签I-XXX指示令牌位于实体内部XXX。例如，在当前示例中，我们希望我们的模型将标记分类S为B-PER（个人实体的开头）和标记##yl，##va以及##in（I-PER个人实体内部）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这种情况下，您可能会认为该模型是错误的，因为它为所有这四个令牌提供了标签I-PER，但这并不完全正确。这些 B 和 I 标签实际上有两种格式：IOB1 和 IOB2。IOB2 格式（下面的粉红色）是我们介绍的格式，而在 IOB1 格式（蓝色）中，以 B- 开头的标签仅用于分隔同一类型的两个相邻实体。我们使用的模型是在使用该格式的数据集上进行微调的，这就是它将标签 I-PER 分配给 S 令牌的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://chushi123.oss-cn-beijing.aliyuncs.com/img/202203040958489.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:24:36.078501Z",
     "iopub.status.busy": "2022-03-04T02:24:36.078501Z",
     "iopub.status.idle": "2022-03-04T02:24:36.093497Z",
     "shell.execute_reply": "2022-03-04T02:24:36.093497Z",
     "shell.execute_reply.started": "2022-03-04T02:24:36.078501Z"
    },
    "id": "ZznGWjo37NOw",
    "outputId": "c4a38dc8-0577-4994-ec1c-e6622dc2c4c1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S'}, {'entity': 'I-PER', 'score': 0.9981549382209778, 'word': '##yl'}, {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va'}, {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in'}, {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu'}, {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging'}, {'entity': 'I-ORG', 'score': 0.9887976050376892, 'word': 'Face'}, {'entity': 'I-LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn'}]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "tokens = inputs.tokens()\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        results.append(\n",
    "            {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]}\n",
    "        )\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "管道还为我们提供了有关原始句子中每个token的开始和结束的信息。这就是我们的偏移映射将发挥作用的地方。要获得偏移量，我们只需要在将分词器应用于输入时设置return_offsets_mapping=True："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:24:36.094502Z",
     "iopub.status.busy": "2022-03-04T02:24:36.094502Z",
     "iopub.status.idle": "2022-03-04T02:24:36.108500Z",
     "shell.execute_reply": "2022-03-04T02:24:36.108500Z",
     "shell.execute_reply.started": "2022-03-04T02:24:36.094502Z"
    },
    "id": "ecdBcmaL7NOx",
    "outputId": "115df42a-6b40-4978-e12e-61416eedf00c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 2),\n",
       " (3, 7),\n",
       " (8, 10),\n",
       " (11, 12),\n",
       " (12, 14),\n",
       " (14, 16),\n",
       " (16, 18),\n",
       " (19, 22),\n",
       " (23, 24),\n",
       " (25, 29),\n",
       " (30, 32),\n",
       " (33, 35),\n",
       " (35, 40),\n",
       " (41, 45),\n",
       " (46, 48),\n",
       " (49, 57),\n",
       " (57, 58),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "inputs_with_offsets[\"offset_mapping\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个元组是对应于每个标记的文本范围，其中 （0， 0） 是为特殊标记保留的。我们之前看到索引5处的令牌是##yl，此处有（12，14）作为偏移量。如果我们在示例中抓取相应的切片："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:24:36.110500Z",
     "iopub.status.busy": "2022-03-04T02:24:36.109498Z",
     "iopub.status.idle": "2022-03-04T02:24:36.123500Z",
     "shell.execute_reply": "2022-03-04T02:24:36.123500Z",
     "shell.execute_reply.started": "2022-03-04T02:24:36.110500Z"
    },
    "id": "oja1vvZ27NOx",
    "outputId": "48e15fe1-07d8-4663-b004-4a05f17d6c57",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yl'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[12:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:24:36.124501Z",
     "iopub.status.busy": "2022-03-04T02:24:36.124501Z",
     "iopub.status.idle": "2022-03-04T02:24:36.139497Z",
     "shell.execute_reply": "2022-03-04T02:24:36.139497Z",
     "shell.execute_reply.started": "2022-03-04T02:24:36.124501Z"
    },
    "id": "2t85Tvx27NOx",
    "outputId": "d9991b20-9b47-4e5f-cfb2-2b5193da1879",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.9993828535079956, 'word': 'S', 'start': 11, 'end': 12}, {'entity': 'I-PER', 'score': 0.9981549382209778, 'word': '##yl', 'start': 12, 'end': 14}, {'entity': 'I-PER', 'score': 0.995907187461853, 'word': '##va', 'start': 14, 'end': 16}, {'entity': 'I-PER', 'score': 0.9992327690124512, 'word': '##in', 'start': 16, 'end': 18}, {'entity': 'I-ORG', 'score': 0.9738931059837341, 'word': 'Hu', 'start': 33, 'end': 35}, {'entity': 'I-ORG', 'score': 0.9761149883270264, 'word': '##gging', 'start': 35, 'end': 40}, {'entity': 'I-ORG', 'score': 0.9887976050376892, 'word': 'Face', 'start': 41, 'end': 45}, {'entity': 'I-LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "for idx, pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        start, end = offsets[idx]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity\": label,\n",
    "                \"score\": probabilities[idx][pred],\n",
    "                \"word\": tokens[idx],\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 对实体进行分组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用偏移量来确定每个实体的开始键和结束键非常方便，但该信息并不是绝对必要的。但是，当我们想要将实体组合在一起时，偏移量将为我们节省大量混乱的代码。例如，如果我们想将令牌 Hu、##gging 和 Face 组合在一起，我们可以制定特殊规则，规定在删除 ## 时应附加前两个令牌，并且 Face 应添加空格，因为它不以 ## 开头 — 但这仅适用于此特定类型的分词器。我们必须为 SentencePiece 或 Byte-Pair-Encoding 赋能器编写另一组规则（在本章后面讨论）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有了偏移量，所有自定义代码都会消失：我们只需要获取原始文本中以第一个标记开头并以最后一个标记结尾的跨度。因此，对于令牌 Hu、##gging 和 Face，我们应该从字符 33（Hu 的开头）开始，在字符 45（Face 的结尾）之前结束："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:24:36.141498Z",
     "iopub.status.busy": "2022-03-04T02:24:36.140498Z",
     "iopub.status.idle": "2022-03-04T02:24:36.155497Z",
     "shell.execute_reply": "2022-03-04T02:24:36.155497Z",
     "shell.execute_reply.started": "2022-03-04T02:24:36.141498Z"
    },
    "id": "zgmgh3U-7NOy",
    "outputId": "f29d5526-e0bb-4a69-8cf2-d64875ed4ee6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hugging Face'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[33:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要编写在对实体进行分组时对预测进行后处理的代码，我们将对连续的实体进行分组并标记为 I-XXX，但第一个实体除外，它可以标记为 B-XXX 或 I-XXX（因此，当我们获得 O、新类型的实体或告诉我们正在启动相同类型的实体的 B-XXX 时，我们将停止对实体进行分组）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-04T02:24:36.156498Z",
     "iopub.status.busy": "2022-03-04T02:24:36.156498Z",
     "iopub.status.idle": "2022-03-04T02:24:36.171497Z",
     "shell.execute_reply": "2022-03-04T02:24:36.171497Z",
     "shell.execute_reply.started": "2022-03-04T02:24:36.156498Z"
    },
    "id": "WcFDcV6r7NOy",
    "outputId": "9f1a542e-7a56-420f-a6c5-6907b9f4766f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9981694370508194, 'word': 'Sylvain', 'start': 11, 'end': 18}, {'entity_group': 'ORG', 'score': 0.9796018997828165, 'word': 'Hugging Face', 'start': 33, 'end': 45}, {'entity_group': 'LOC', 'score': 0.9932106137275696, 'word': 'Brooklyn', 'start': 49, 'end': 57}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # Remove the B- or I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在QA任务中，这些偏移量非常有用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Fast tokenizers' special powers (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
