{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0c8a26-c63f-46dd-8ecb-5c0caae87466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "582dceeb-bbe2-4117-b53d-22941a5e0989",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [\n",
    "    \"é€‰æ‹©ç æ±ŸèŠ±å›­çš„åŸå› å°±æ˜¯æ–¹ä¾¿ã€‚\",\n",
    "    \"ç¬”è®°æœ¬çš„é”®ç›˜ç¡®å®çˆ½ã€‚\",\n",
    "    \"æˆ¿é—´å¤ªå°ã€‚å…¶ä»–çš„éƒ½ä¸€èˆ¬ã€‚\",\n",
    "    \"ä»Šå¤©æ‰çŸ¥é“è¿™ä¹¦è¿˜æœ‰ç¬¬6å·,çœŸæœ‰ç‚¹éƒé—·.\",\n",
    "    \"æœºå™¨èƒŒé¢ä¼¼ä¹è¢«æ’•äº†å¼ ä»€ä¹ˆæ ‡ç­¾ï¼Œæ®‹èƒ¶è¿˜åœ¨ã€‚\",\n",
    "]\n",
    "\n",
    "# modelçš„è¾“å…¥å¯ä»¥æ˜¯å¤šä¸ªå‚æ•°ï¼Œä½†ä¹Ÿå¯ä»¥åªæœ‰input_idsï¼Œå½¢çŠ¶å¿…é¡»æ˜¯2ç»´å¼ é‡ã€‚\n",
    "inputs = tokenizer(sents, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2153fec-619d-4dbb-8d78-4c9eaa5e6492",
   "metadata": {},
   "source": [
    "# AutoModelæ¶æ„çš„è¾“å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09748931-d3f6-4d1f-ac7b-17ac089245f3",
   "metadata": {},
   "source": [
    "AutoModelè¿™ä¸ªæ¶æ„åªåŒ…å«åŸºæœ¬çš„ Transformer æ¨¡å—ï¼šç»™å®šä¸€äº›è¾“å…¥ï¼Œå®ƒ**è¾“å‡ºæˆ‘ä»¬ç§°ä¹‹ä¸ºéšè—å±‚çš„ä¸œè¥¿**ï¼Œä¹Ÿç§°ä¸ºfeaturesã€‚å¯¹äºæ¯ä¸ªæ¨¡å‹è¾“å…¥ï¼Œæˆ‘ä»¬å°†å¾—åˆ°ä¸€ä¸ªé«˜ç»´å‘é‡ï¼Œè¡¨ç¤ºTransformer æ¨¡å‹å¯¹è¯¥è¾“å…¥çš„ä¸Šä¸‹æ–‡ç†è§£ï¼Œå¯ä»¥ç†è§£æˆæ˜¯åšäº†word embeddingã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755d3bb3-48e5-48d6-a754-4b114be37572",
   "metadata": {},
   "source": [
    "1. è¾“å‡ºä¸ºBaseModelOutputWithPoolingAndCrossAttentionsã€‚\n",
    "2. åŒ…å«'last_hidden_state'å’Œ'pooler_output'ä¸¤ä¸ªå…ƒç´ ã€‚\n",
    "3. 'last_hidden_state'çš„å½¢çŠ¶æ˜¯ï¼ˆbatch size,sequence length,768)\n",
    "4. 'pooler_output'çš„å½¢çŠ¶æ˜¯(batch size,768)\n",
    "> pooler outputæ˜¯å–[CLS]æ ‡è®°å¤„å¯¹åº”çš„å‘é‡åé¢æ¥ä¸ªå…¨è¿æ¥å†æ¥tanhæ¿€æ´»åçš„è¾“å‡ºã€‚\n",
    "è™½ç„¶è¿™äº›éšè—çŠ¶æ€æœ¬èº«å°±å¾ˆæœ‰ç”¨ï¼Œä½†å®ƒä»¬é€šå¸¸æ˜¯æ¨¡å‹å¦ä¸€éƒ¨åˆ†ï¼ˆç§°ä¸ºhead ï¼‰çš„è¾“å…¥ã€‚åœ¨pipelineé‚£ä¸€èŠ‚ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ç›¸åŒçš„ä½“ç³»ç»“æ„æ‰§è¡Œä¸åŒçš„ä»»åŠ¡ï¼Œæ˜¯å› ä¸ºè¿™äº›ä»»åŠ¡ä¸­çš„æ¯ä¸€ä¸ªéƒ½æœ‰ä¸ä¹‹å…³è”çš„ä¸åŒå¤´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005dc398-1fc2-458c-a33a-323546990059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 22, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "outputs = model(**inputs)\n",
    "outputs.keys()\n",
    "outputs.last_hidden_state.shape\n",
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e2464-a0d6-4ac1-b578-5d1053e9dd9b",
   "metadata": {},
   "source": [
    "# AutoModelForMaskedLMæ¶æ„çš„è¾“å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe805f9-c9e5-4dfc-9c2d-29a66aaf2976",
   "metadata": {},
   "source": [
    "1. è¾“å‡ºä¸ºMaskedLMOutput\n",
    "2. åŒ…å«'logits'å…ƒç´ ï¼Œå½¢çŠ¶ä¸º[batch size,sequence length,21128]ï¼Œ21128æ˜¯'vocab_size'ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b66a952-764e-417d-ae72-127a6796e485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 22, 21128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\")\n",
    "outputs = model(**inputs)\n",
    "outputs.keys()\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a0f58-d289-4190-b152-0441efedcf08",
   "metadata": {},
   "source": [
    "# AutoModelForSequenceClassificationæ¶æ„çš„è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4d7ae6-9fd3-4476-955a-51ba660ff139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-chinese\")\n",
    "outputs = model(**inputs)\n",
    "outputs.keys()\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f1219c-dc48-4638-bfdc-36212ed12368",
   "metadata": {},
   "source": [
    "# AutoModelForTokenClassificationæ¶æ„çš„è¾“å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf3993-03cd-4f39-9aa8-ae9f4ce7082d",
   "metadata": {},
   "source": [
    "1. è¾“å‡ºä¸ºTokenClassifierOutput\n",
    "2. åŒ…å«'logits'å…ƒç´ ï¼Œå½¢çŠ¶ä¸º[batch size,sequence length,2]ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7c915f-5fe4-4cf3-88cc-19f600a07f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 22, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-chinese\")\n",
    "outputs = model(**inputs)\n",
    "outputs.keys()\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e442a-51cc-4abd-b2e6-467782794758",
   "metadata": {},
   "source": [
    "# æ¨¡å‹è¾“å‡ºlogitsè§£é‡Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad70ba0a-2388-4d43-9607-24ac7126ede7",
   "metadata": {},
   "source": [
    "logitsï¼Œå³æ¨¡å‹æœ€åä¸€å±‚è¾“å‡ºçš„åŸå§‹çš„ã€éæ ‡å‡†åŒ–çš„åˆ†æ•°ã€‚è¦è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œå®ƒä»¬éœ€è¦ç»è¿‡softmax(**æ‰€æœ‰ğŸ¤—transformersæ¨¡å‹éƒ½ä¼šè¾“å‡ºlogitsï¼Œå› ä¸ºç”¨äºè®­ç»ƒçš„æŸå¤±å‡½æ•°é€šå¸¸ä¼šå°†æœ€åä¸€ä¸ªæ¿€æ´»å‡½æ•°(å¦‚SoftMax)ä¸å®é™…æŸå¤±å‡½æ•°(å¦‚äº¤å‰ç†µ)èåˆåœ¨ä¸€èµ·**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28fe82e-5bcc-483d-98e4-85f77b934559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# è¾“å‡ºç»“æœä¸ºå¯è¯†åˆ«çš„æ¦‚ç‡åˆ†æ•°\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f61dec4-1b2f-4963-b9f3-ee177dd16be5",
   "metadata": {},
   "source": [
    "## è¦è·å¾—æ¯ä¸ªä½ç½®å¯¹åº”çš„æ ‡ç­¾ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥id2labelæ¨¡å‹é…ç½®çš„å±æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc14edd-58a6-45d4-968d-e5d92cb8bbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'LABEL_0', 1: 'LABEL_1'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5737f-e371-4bb0-b1da-339182de52e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
