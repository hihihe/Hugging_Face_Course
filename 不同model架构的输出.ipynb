{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c0c8a26-c63f-46dd-8ecb-5c0caae87466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "582dceeb-bbe2-4117-b53d-22941a5e0989",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [\n",
    "    \"选择珠江花园的原因就是方便。\",\n",
    "    \"笔记本的键盘确实爽。\",\n",
    "    \"房间太小。其他的都一般。\",\n",
    "    \"今天才知道这书还有第6卷,真有点郁闷.\",\n",
    "    \"机器背面似乎被撕了张什么标签，残胶还在。\",\n",
    "]\n",
    "\n",
    "# model的输入可以是多个参数，但也可以只有input_ids，形状必须是2维张量。\n",
    "inputs = tokenizer(sents, padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2153fec-619d-4dbb-8d78-4c9eaa5e6492",
   "metadata": {},
   "source": [
    "# AutoModel架构的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09748931-d3f6-4d1f-ac7b-17ac089245f3",
   "metadata": {},
   "source": [
    "AutoModel这个架构只包含基本的 Transformer 模块：给定一些输入，它**输出我们称之为隐藏层的东西**，也称为features。对于每个模型输入，我们将得到一个高维向量，表示Transformer 模型对该输入的上下文理解，可以理解成是做了word embedding。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755d3bb3-48e5-48d6-a754-4b114be37572",
   "metadata": {},
   "source": [
    "1. 输出为BaseModelOutputWithPoolingAndCrossAttentions。\n",
    "2. 包含'last_hidden_state'和'pooler_output'两个元素。\n",
    "3. 'last_hidden_state'的形状是（batch size,sequence length,768)\n",
    "4. 'pooler_output'的形状是(batch size,768)\n",
    "> pooler output是取[CLS]标记处对应的向量后面接个全连接再接tanh激活后的输出。\n",
    "虽然这些隐藏状态本身就很有用，但它们通常是模型另一部分（称为head ）的输入。在pipeline那一节中，可以使用相同的体系结构执行不同的任务，是因为这些任务中的每一个都有与之关联的不同头。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005dc398-1fc2-458c-a33a-323546990059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 22, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"bert-base-chinese\")\n",
    "outputs = model(**inputs)\n",
    "outputs.keys()\n",
    "outputs.last_hidden_state.shape\n",
    "outputs.pooler_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34e2464-a0d6-4ac1-b578-5d1053e9dd9b",
   "metadata": {},
   "source": [
    "# AutoModelForMaskedLM架构的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe805f9-c9e5-4dfc-9c2d-29a66aaf2976",
   "metadata": {},
   "source": [
    "1. 输出为MaskedLMOutput\n",
    "2. 包含'logits'元素，形状为[batch size,sequence length,21128]，21128是'vocab_size'。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b66a952-764e-417d-ae72-127a6796e485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 22, 21128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\")\n",
    "outputs = model(**inputs)\n",
    "outputs.keys()\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a0f58-d289-4190-b152-0441efedcf08",
   "metadata": {},
   "source": [
    "# AutoModelForSequenceClassification架构的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c4d7ae6-9fd3-4476-955a-51ba660ff139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-chinese\")\n",
    "outputs = model(**inputs)\n",
    "outputs.keys()\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f1219c-dc48-4638-bfdc-36212ed12368",
   "metadata": {},
   "source": [
    "# AutoModelForTokenClassification架构的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf3993-03cd-4f39-9aa8-ae9f4ce7082d",
   "metadata": {},
   "source": [
    "1. 输出为TokenClassifierOutput\n",
    "2. 包含'logits'元素，形状为[batch size,sequence length,2]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7c915f-5fe4-4cf3-88cc-19f600a07f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 22, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-chinese\")\n",
    "outputs = model(**inputs)\n",
    "outputs.keys()\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e442a-51cc-4abd-b2e6-467782794758",
   "metadata": {},
   "source": [
    "# 模型输出logits解释"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad70ba0a-2388-4d43-9607-24ac7126ede7",
   "metadata": {},
   "source": [
    "logits，即模型最后一层输出的原始的、非标准化的分数。要转换为概率，它们需要经过softmax(**所有🤗transformers模型都会输出logits，因为用于训练的损失函数通常会将最后一个激活函数(如SoftMax)与实际损失函数(如交叉熵)融合在一起**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a28fe82e-5bcc-483d-98e4-85f77b934559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# 输出结果为可识别的概率分数\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f61dec4-1b2f-4963-b9f3-ee177dd16be5",
   "metadata": {},
   "source": [
    "## 要获得每个位置对应的标签，我们可以检查id2label模型配置的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc14edd-58a6-45d4-968d-e5d92cb8bbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'LABEL_0', 1: 'LABEL_1'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c5737f-e371-4bb0-b1da-339182de52e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
