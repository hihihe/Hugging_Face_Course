{"cells":[{"cell_type":"markdown","metadata":{"id":"3AA0BD1F65584E40B437969433E2ED7E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"1、Multi-Class：多分类/多元分类（二分类、三分类、多分类等）\n\n```\n二分类：判断邮件属于哪个类别，垃圾或者非垃圾\n二分类：判断新闻属于哪个类别，机器写的或者人写的\n三分类：判断文本情感属于{正面，中立，负面}中的哪一类\n多分类：判断新闻属于哪个类别，如财经、体育、娱乐等\n```\n\n2022搜狐校园 情感分析 × 推荐排序 算法大赛\nhttps://www.biendata.xyz/competition/sohu_2022/data/\n（2代表极正向，1代表正向，0代表中立，-1代表负向，-2代表极负向）\n\n\n2、Multi-Label：多标签分类\n\n- 文本可能同时涉及任何 宗教，政治，金融或教育，也可能不属于任何一种。\n\n- 电影可以根据其摘要内容分为动作，喜剧和浪漫类型。有可能电影属于romcoms [浪漫与喜剧]等多种类型。"},{"cell_type":"markdown","metadata":{"id":"BC87466314C24968B4FB19A556BAEC41","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"### 推特有毒文本多标签分类\n\nToxic Comment Classification Challenge \n\nIdentify and classify toxic online comments\n\nhttps://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n"},{"cell_type":"markdown","metadata":{"id":"m5tUoHe9FRhs","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 导入包"},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"id":"Dr7BCHS-nIRW","jupyter":{"outputs_hidden":false},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport torch\nfrom torch.nn import BCEWithLogitsLoss, BCELoss# 多标签分类loss\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\nimport pickle\nfrom transformers import AutoModel,AutoConfig,AutoTokenizer\nfrom tqdm import tqdm, trange\nfrom ast import literal_eval"},{"cell_type":"markdown","metadata":{"id":"E4E9C6C9E2CF447593B46A049BCCCF63","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"###  查看GPU是否可用"},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"UhjnJEwKnISB","jupyter":{},"outputId":"e342121d-885a-48f1-cd4a-5f8a01f579dc","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found GPU at: /device:GPU:0\n"]}],"source":"device_name = tf.test.gpu_device_name()\nif device_name != '/device:GPU:0':\n  raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))"},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"uorMX_zrnISM","jupyter":{},"outputId":"48a34b4e-183c-465f-a096-da833989b59e","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"data":{"text/plain":["'NVIDIA GeForce RTX 3090'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)"},{"cell_type":"code","execution_count":5,"metadata":{"id":"BEAA88DEFEBA4EF889999857AF8C3899","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":"device"},{"cell_type":"markdown","metadata":{"id":"dLcetMjZFjSH","jupyter":{},"mdEditEnable":false,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 加载数据与预处理"},{"metadata":{"id":"928A483C16FB41DD855A887096908012","notebookId":"624d3cd8e1d37c0017077165","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","outputs":[],"source":"# df = pd.read_csv('data/03/train.csv') #jigsaw-toxic-comment-classification-challenge\n# df.head()","execution_count":null},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"0ecREc7GnISW","jupyter":{},"outputId":"54a37dd4-18da-4120-94f6-9e3f32d0f5ce","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0000997932d777bf</td>\n","      <td>Explanation\\nWhy the edits made under my usern...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>000103f0d9cfb60f</td>\n","      <td>D'aww! He matches this background colour I'm s...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>000113f07ec002fd</td>\n","      <td>Hey man, I'm really not trying to edit war. It...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0001b41b1c6bb37e</td>\n","      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0001d958c54c6e35</td>\n","      <td>You, sir, are my hero. Any chance you remember...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id                                       comment_text  toxic  \\\n","0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n","1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n","2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n","3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n","4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n","\n","   severe_toxic  obscene  threat  insult  identity_hate  \n","0             0        0       0       0              0  \n","1             0        0       0       0              0  \n","2             0        0       0       0              0  \n","3             0        0       0       0              0  \n","4             0        0       0       0              0  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":"df = pd.read_csv('/home/mw/input/task031964/train.csv') #jigsaw-toxic-comment-classification-challenge\ndf.head()"},{"cell_type":"code","execution_count":7,"metadata":{"id":"098F912C0E7A49A78B5587E6192F680F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["0    144277\n","1     15294\n","Name: toxic, dtype: int64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":"df['toxic'].value_counts()"},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":51},"id":"6AhWrzX7nITB","jupyter":{},"outputId":"cd5acbc1-525c-419f-df32-4514130dd3ef","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique comments:  True\n","Null values:  False\n"]}],"source":"print('Unique comments: ', df.comment_text.nunique() == df.shape[0]) \nprint('Null values: ', df.isnull().values.any())\n# df[df.isna().any(axis=1)]"},{"cell_type":"code","execution_count":null,"metadata":{"id":"57FCA4FB87524EF69EE8137EEFB7B543","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# 训练集的每一个文本都是不一样的\n# 训练集中的元素都是非空的"},{"cell_type":"code","execution_count":9,"metadata":{"id":"30AE8E7FFB7840558B4EA42418958C73","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["id               0\n","comment_text     0\n","toxic            0\n","severe_toxic     0\n","obscene          0\n","threat           0\n","insult           0\n","identity_hate    0\n","dtype: int64"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":"df.shape[0]-df.count()# 训练集中的元素都是非空的"},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":51},"id":"OkKpz_9eJRt7","jupyter":{},"outputId":"c473c234-1472-42b3-e9e7-af36a96503f3","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["average sentence length:  67.27352714465661\n","stdev sentence length:  99.2307021928862\n"]}],"source":"# 粗略统计下文本长度分布以及标准差\nprint('average sentence length: ', df.comment_text.str.split().str.len().mean())\nprint('stdev sentence length: ', df.comment_text.str.split().str.len().std())"},{"cell_type":"code","execution_count":10,"metadata":{"id":"43073813E20548F08893DB018D3EF62F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["count    159571.000000\n","mean         67.273527\n","std          99.230702\n","min           1.000000\n","25%          17.000000\n","50%          36.000000\n","75%          75.000000\n","max        1411.000000\n","Name: comment_text, dtype: float64"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":"df.comment_text.str.split().str.len().describe()"},{"cell_type":"code","execution_count":20,"metadata":{"id":"B8B27B46166D4B1A9AEDF8E6922126ED","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["10087"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":"sum(df.comment_text.str.split().str.len()>200) # 统计文本word个数大于200的文本个数"},{"cell_type":"code","execution_count":19,"metadata":{"id":"CBFAE2F388484D8795C6168E01224029","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["(159571, 9)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":"df.shape"},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"UVI59S9VaAfB","jupyter":{},"outputId":"0a3d5338-0cb7-40d1-ffe1-124d436afd9d","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Label columns:  ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n"]}],"source":"cols = df.columns # 数据集的列名\nlabel_cols = list(cols[2:]) # 数据集中的标签列名\nnum_labels = len(label_cols) # 标签个数\nprint('Label columns: ', label_cols)"},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":306},"id":"xzgA5qQgYIBZ","jupyter":{},"outputId":"6a2937a6-445e-421c-dd53-34569e654f63","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Count of 1 per label: \n"," toxic            15294\n","severe_toxic      1595\n","obscene           8449\n","threat             478\n","insult            7877\n","identity_hate     1405\n","dtype: int64 \n","\n","Count of 0 per label: \n"," toxic            144277\n","severe_toxic     157976\n","obscene          151122\n","threat           159093\n","insult           151694\n","identity_hate    158166\n","dtype: int64\n"]}],"source":"print('Count of 1 per label: \\n', df[label_cols].sum(), '\\n') # 统计每个标签为1的个数\nprint('Count of 0 per label: \\n', df[label_cols].eq(0).sum()) # 统计每个标签为0的个数"},{"cell_type":"code","execution_count":15,"metadata":{"id":"uFpSd4JzaAae","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"df = df.sample(frac=1).reset_index(drop=True) #shuffle rows"},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"0DF3ddjej5vd","jupyter":{},"outputId":"b22c40b3-bb87-4911-b21c-d4e4e6beeee0","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","      <th>one_hot_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0c88373b4a0aac92</td>\n","      <td>\"\\n\\nThe removed paragraph \\nSlovakization is ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>cd564a2df8bdd285</td>\n","      <td>Mini-Contra Battle section \\n\\nWould it better...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>012b305351d49596</td>\n","      <td>\", 20 January 2013 (UTC)\\nThe argument at WP:N...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>29b87947f2deedbc</td>\n","      <td>I am unable to thank you for your review using...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>270deaeac50bbdfc</td>\n","      <td>\"/hollyoaks/scoop/a327249/hollyoaks-pj-brennan...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id                                       comment_text  toxic  \\\n","0  0c88373b4a0aac92  \"\\n\\nThe removed paragraph \\nSlovakization is ...      0   \n","1  cd564a2df8bdd285  Mini-Contra Battle section \\n\\nWould it better...      0   \n","2  012b305351d49596  \", 20 January 2013 (UTC)\\nThe argument at WP:N...      0   \n","3  29b87947f2deedbc  I am unable to thank you for your review using...      0   \n","4  270deaeac50bbdfc  \"/hollyoaks/scoop/a327249/hollyoaks-pj-brennan...      0   \n","\n","   severe_toxic  obscene  threat  insult  identity_hate      one_hot_labels  \n","0             0        0       0       0              0  [0, 0, 0, 0, 0, 0]  \n","1             0        0       0       0              0  [0, 0, 0, 0, 0, 0]  \n","2             0        0       0       0              0  [0, 0, 0, 0, 0, 0]  \n","3             0        0       0       0              0  [0, 0, 0, 0, 0, 0]  \n","4             0        0       0       0              0  [0, 0, 0, 0, 0, 0]  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":"df['one_hot_labels'] = list(df[label_cols].values) # 直接将六个标签转为one hot\ndf.head()"},{"cell_type":"code","execution_count":17,"metadata":{"id":"MlhHifh5bW7e","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"labels = list(df.one_hot_labels.values)\ncomments = list(df.comment_text.values)"},{"cell_type":"markdown","metadata":{"id":"IlMHfElhGJzc","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"针对不同模型 使用不同的分词器\n\n```\nBERT:\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) \n\nXLNet:\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=False) \n\nRoBERTa:\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=False)\n```\n"},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["9fc7627a06b347b585e652916bbf6491","52a030b7dbb142cda029cbe017cb0822","fba3c87d7dec4a4eb7ae75e739f3cf6e","138a1ed9015243c7a734ee6a5f7c9609","02d0482ff3d7468d9de60e8ce1ea1733","3f61b3aaca65465c9b54c8513a5555e7","34e411387959477a957d6061f003e1f9","9532d9ec22784d519fc710e6189f89b8"]},"id":"HNsEu-vUur-4","jupyter":{},"outputId":"273a8056-4b12-4c91-8941-c53377b629b5","scrolled":true,"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\yanqiang/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\yanqiang/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at C:\\Users\\yanqiang/.cache\\huggingface\\transformers\\534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\yanqiang/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\yanqiang/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["tokenizer outputs:  dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"]}],"source":"max_length = 150\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # tokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # tokenizer 全部转小写\n# NLP三大Subword模型详解：BPE、WordPiece、ULM\n# https://zhuanlan.zhihu.com/p/191648421\nencodings = tokenizer.batch_encode_plus(\n    comments,# 传入列表\n    max_length=max_length,\n    pad_to_max_length=True\n) # tokenizer's encoding method\nprint('tokenizer outputs: ', encodings.keys()) # 分词之后的输出"},{"cell_type":"code","execution_count":23,"metadata":{"id":"l6CCLSjfur-9","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"input_ids = encodings['input_ids'] # tokenized and encoded sentences\ntoken_type_ids = encodings['token_type_ids'] # token type ids\nattention_masks = encodings['attention_mask'] # attention masks"},{"cell_type":"code","execution_count":25,"metadata":{"scrolled":true,"tags":[],"id":"E87B97AA916244C984D77B85DCED5B0E","jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["[101,\n"," 1000,\n"," 1996,\n"," 3718,\n"," 20423,\n"," 12747,\n"," 3989,\n"," 2003,\n"," 2411,\n"," 11581,\n"," 3550,\n"," 2004,\n"," 1037,\n"," 3433,\n"," 2000,\n"," 3140,\n"," 23848,\n"," 13380,\n"," 3989,\n"," 1010,\n"," 2029,\n"," 3047,\n"," 3701,\n"," 2044,\n"," 1996,\n"," 17151,\n"," 9354,\n"," 7033,\n"," 1006,\n"," 12014,\n"," 1997,\n"," 7517,\n"," 1007,\n"," 1012,\n"," 2096,\n"," 2053,\n"," 2028,\n"," 2085,\n"," 10592,\n"," 23848,\n"," 13380,\n"," 3989,\n"," 1037,\n"," 3893,\n"," 9874,\n"," 1010,\n"," 2005,\n"," 2195,\n"," 4436,\n"," 1996,\n"," 11581,\n"," 3989,\n"," 1997,\n"," 12747,\n"," 3989,\n"," 2004,\n"," 1037,\n"," 3433,\n"," 2000,\n"," 2009,\n"," 2003,\n"," 21068,\n"," 1024,\n"," 23848,\n"," 13380,\n"," 3989,\n"," 3047,\n"," 2076,\n"," 1996,\n"," 2335,\n"," 1997,\n"," 1996,\n"," 12078,\n"," 1010,\n"," 1998,\n"," 2009,\n"," 3047,\n"," 1999,\n"," 1037,\n"," 2051,\n"," 2558,\n"," 2043,\n"," 2529,\n"," 2916,\n"," 2020,\n"," 2025,\n"," 2641,\n"," 3053,\n"," 2004,\n"," 5415,\n"," 2004,\n"," 2085,\n"," 1012,\n"," 12747,\n"," 3989,\n"," 2003,\n"," 6230,\n"," 2144,\n"," 2059,\n"," 1998,\n"," 2009,\n"," 2001,\n"," 2004,\n"," 5729,\n"," 2104,\n"," 4750,\n"," 18944,\n"," 2004,\n"," 2009,\n"," 2003,\n"," 2085,\n"," 1999,\n"," 3537,\n"," 1045,\n"," 2031,\n"," 3718,\n"," 2023,\n"," 20423,\n"," 1010,\n"," 2025,\n"," 2138,\n"," 1045,\n"," 2052,\n"," 9352,\n"," 21090,\n"," 1010,\n"," 2021,\n"," 2138,\n"," 2023,\n"," 2003,\n"," 1037,\n"," 23250,\n"," 20219,\n"," 1006,\n"," 1037,\n"," 1000,\n"," 1000,\n"," 13433,\n"," 2615,\n"," 1000,\n"," 1000,\n"," 1999,\n"," 16948,\n"," 18444,\n"," 1007,\n"," 2025,\n"," 7218,\n"," 2005,\n"," 2019,\n"," 102]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":"input_ids[0]"},{"cell_type":"code","execution_count":26,"metadata":{"scrolled":true,"tags":[],"id":"E9457C24BA31443DADC2930C312298FA","jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["['[CLS]',\n"," '\"',\n"," 'the',\n"," 'removed',\n"," 'paragraph',\n"," 'slovak',\n"," '##ization',\n"," 'is',\n"," 'often',\n"," 'rational',\n"," '##ized',\n"," 'as',\n"," 'a',\n"," 'response',\n"," 'to',\n"," 'forced',\n"," 'mag',\n"," '##yar',\n"," '##ization',\n"," ',',\n"," 'which',\n"," 'happened',\n"," 'mainly',\n"," 'after',\n"," 'the',\n"," 'aus',\n"," '##gle',\n"," '##ich',\n"," '(',\n"," 'compromise',\n"," 'of',\n"," '1867',\n"," ')',\n"," '.',\n"," 'while',\n"," 'no',\n"," 'one',\n"," 'now',\n"," 'considers',\n"," 'mag',\n"," '##yar',\n"," '##ization',\n"," 'a',\n"," 'positive',\n"," 'trend',\n"," ',',\n"," 'for',\n"," 'several',\n"," 'reasons',\n"," 'the',\n"," 'rational',\n"," '##ization',\n"," 'of',\n"," 'slovak',\n"," '##ization',\n"," 'as',\n"," 'a',\n"," 'response',\n"," 'to',\n"," 'it',\n"," 'is',\n"," 'questionable',\n"," ':',\n"," 'mag',\n"," '##yar',\n"," '##ization',\n"," 'happened',\n"," 'during',\n"," 'the',\n"," 'times',\n"," 'of',\n"," 'the',\n"," 'monarchy',\n"," ',',\n"," 'and',\n"," 'it',\n"," 'happened',\n"," 'in',\n"," 'a',\n"," 'time',\n"," 'period',\n"," 'when',\n"," 'human',\n"," 'rights',\n"," 'were',\n"," 'not',\n"," 'considered',\n"," 'nearly',\n"," 'as',\n"," 'universal',\n"," 'as',\n"," 'now',\n"," '.',\n"," 'slovak',\n"," '##ization',\n"," 'is',\n"," 'happening',\n"," 'since',\n"," 'then',\n"," 'and',\n"," 'it',\n"," 'was',\n"," 'as',\n"," 'severe',\n"," 'under',\n"," 'communist',\n"," 'dictatorship',\n"," 'as',\n"," 'it',\n"," 'is',\n"," 'now',\n"," 'in',\n"," 'democratic',\n"," 'i',\n"," 'have',\n"," 'removed',\n"," 'this',\n"," 'paragraph',\n"," ',',\n"," 'not',\n"," 'because',\n"," 'i',\n"," 'would',\n"," 'necessarily',\n"," 'disagree',\n"," ',',\n"," 'but',\n"," 'because',\n"," 'this',\n"," 'is',\n"," 'a',\n"," 'speculative',\n"," 'formulation',\n"," '(',\n"," 'a',\n"," '\"',\n"," '\"',\n"," 'po',\n"," '##v',\n"," '\"',\n"," '\"',\n"," 'in',\n"," 'wikipedia',\n"," 'terminology',\n"," ')',\n"," 'not',\n"," 'suitable',\n"," 'for',\n"," 'an',\n"," '[SEP]']"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":"tokenizer.convert_ids_to_tokens(input_ids[0])"},{"cell_type":"code","execution_count":27,"metadata":{"id":"A8D99BB2F90141D59DD5F3858A673BF6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["'\"\\n\\nThe removed paragraph \\nSlovakization is often rationalized as a response to forced magyarization, which happened mainly after the Ausgleich (Compromise of 1867). While no one now considers magyarization a positive trend, for several reasons the rationalization of slovakization as a response to it is questionable: magyarization happened during the times of the monarchy, and it happened in a time period when human rights were not considered nearly as universal as now. Slovakization is happening since then and it was as severe under communist dictatorship as it is now in democratic\\n\\nI have removed this paragraph, not because I would necessarily disagree, but because this is a speculative formulation (a \"\"POV\"\" in wikipedia terminology) not suitable for an encyclopaedia. If this was an undisputed topic, maybe such formulations could work, but since that is not the case, it is definitely better to stick to pure sourced data.  \"'"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":"comments[0]"},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"vSOFbThlYcpb","jupyter":{},"outputId":"ba3e9ca0-31b4-4e0d-dd34-a6a28c2fc4c0","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["df label indices with only one instance:  [63106, 40974]\n"]}],"source":"label_counts = df.one_hot_labels.astype(str).value_counts()\none_freq = label_counts[label_counts==1].keys()\none_freq_idxs = sorted(list(df[df.one_hot_labels.astype(str).isin(one_freq)].index), reverse=True)\nprint('df label indices with only one instance: ', one_freq_idxs)"},{"cell_type":"code","execution_count":29,"metadata":{"id":"CQQ7CoOag_r7","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"# Gathering single instance inputs to force into the training set after stratified split\none_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\none_freq_token_types = [token_type_ids.pop(i) for i in one_freq_idxs]\none_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\none_freq_labels = [labels.pop(i) for i in one_freq_idxs]"},{"cell_type":"markdown","metadata":{"id":"r9PxAt48HRRj","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"####  划分一些训练集和验证集"},{"cell_type":"code","execution_count":30,"metadata":{"id":"WPFaq4ufnIT2","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"# 训练集和验证集划分\n\ntrain_inputs, validation_inputs, train_labels, validation_labels, train_token_types, validation_token_types, train_masks, validation_masks = train_test_split(input_ids, labels, token_type_ids,attention_masks,random_state=2020, test_size=0.10, stratify = labels)\n\n# Add one frequency data to train data\ntrain_inputs.extend(one_freq_input_ids)\ntrain_labels.extend(one_freq_labels)\ntrain_masks.extend(one_freq_attention_masks)\ntrain_token_types.extend(one_freq_token_types)\n\n# 将原始id转为torch 张量\ntrain_inputs = torch.tensor(train_inputs)\ntrain_labels = torch.tensor(train_labels)\ntrain_masks = torch.tensor(train_masks)\ntrain_token_types = torch.tensor(train_token_types)\n\nvalidation_inputs = torch.tensor(validation_inputs)\nvalidation_labels = torch.tensor(validation_labels)\nvalidation_masks = torch.tensor(validation_masks)\nvalidation_token_types = torch.tensor(validation_token_types)"},{"cell_type":"code","execution_count":31,"metadata":{"id":"ZRnuLna-nIT4","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"# 批数据大小 ：8 16 32 64  128 256\nbatch_size = 32\n\n# 训练集 \ntrain_data = TensorDataset(train_inputs, train_masks, train_labels, train_token_types)\ntrain_sampler = RandomSampler(train_data) # \ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels, validation_token_types)\nvalidation_sampler = SequentialSampler(validation_data) # 按顺序遍历\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"},{"cell_type":"code","execution_count":32,"metadata":{"id":"iiFRnP_ZTBFa","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"# 保存处理好的数据\ntorch.save(validation_dataloader,'validation_data_loader')\ntorch.save(train_dataloader,'train_data_loader')"},{"cell_type":"markdown","metadata":{"id":"ncGteBuSFuZM","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 定义模型以及设置"},{"cell_type":"markdown","metadata":{"id":"Z0dL-Bz_NrGj","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"```\nAutoModel:\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n\nBERT:\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n\nXLNet:\nmodel = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=num_labels)\n\nRoBERTa:\nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)\n```\n\n"},{"cell_type":"code","execution_count":33,"metadata":{"id":"43B41C6596414878B87643D17021C9A8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"from transformers import AutoModelForSequenceClassification"},{"cell_type":"code","execution_count":34,"metadata":{"id":"Ujk4k16DnIT6","scrolled":true,"slideshow":{"slide_type":"slide"},"tags":[],"jupyter":{},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\yanqiang/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-uncased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\yanqiang/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",")"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":"# 加载预训练模型\nmodel = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels) \n# num_labels：6 默认情况2分类\nmodel.cuda()"},{"cell_type":"code","execution_count":null,"metadata":{"id":"AC817B1957614063BE5B516110237176","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# xxxxxlForSequenceClassification\n# 输出有两个\n# loss\n# logits"},{"cell_type":"markdown","metadata":{"id":"jGE4gv9qfhRG","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"设置优化器\n\nhttps://huggingface.co/transformers/main_classes/optimizer_schedules.html"},{"cell_type":"code","execution_count":36,"metadata":{"id":"BDE80FF85B354138A891A9D51B71FEC4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"paras=[para for para in model.named_parameters()]"},{"cell_type":"code","execution_count":38,"metadata":{"scrolled":true,"tags":[],"id":"85A8393147F74DF793215DE55FB0D36E","jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# paras"},{"cell_type":"code","execution_count":41,"metadata":{"id":"C33F73D9D8C2419CA38E1700EB2997EB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"from transformers import AdamW"},{"cell_type":"code","execution_count":42,"metadata":{"id":"GsV8zwWYnIT9","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"# 对不同参数设置weight_decay_rate\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]"},{"cell_type":"code","execution_count":43,"metadata":{"id":"aOomZIEIoHOL","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["F:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":"optimizer = AdamW(optimizer_grouped_parameters,lr=2e-5,correct_bias=True)\n# 1e-5,2e-5,5e-5\n# optimizer = AdamW(model.parameters(),lr=2e-5)  # 默认优化器"},{"cell_type":"markdown","metadata":{"id":"JRQQZ8zIFzLW","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 训练模型"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"id":"uDLZmEC_oKo3","jupyter":{},"outputId":"fba51964-6ff1-44f0-d448-d80c3824b6d9","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Train loss: 0.05082032523087247\n"]},{"name":"stderr","output_type":"stream","text":["Epoch:  33%|███▎      | 1/3 [23:00<46:01, 1380.97s/it]"]},{"name":"stdout","output_type":"stream","text":["F1 Validation Accuracy:  78.08238436665262\n","Flat Validation Accuracy:  92.33565206492449\n","Train loss: 0.03456845487250288\n"]},{"name":"stderr","output_type":"stream","text":["Epoch:  67%|██████▋   | 2/3 [46:01<23:00, 1380.97s/it]"]},{"name":"stdout","output_type":"stream","text":["F1 Validation Accuracy:  78.78084179970972\n","Flat Validation Accuracy:  92.59885943473084\n","Train loss: 0.027323115857782174\n"]},{"name":"stderr","output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [1:09:02<00:00, 1380.97s/it]"]},{"name":"stdout","output_type":"stream","text":["F1 Validation Accuracy:  78.47726262775885\n","Flat Validation Accuracy:  92.8495331202607\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":"# Store our loss and accuracy for plotting\ntrain_loss_set = []\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 3 # 训练轮数，15万训练集 任务比较简单的，最多设置5\n\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch\"):\n\n  # Training\n  \n  # Set our model to training mode (as opposed to evaluation mode)\n  model.train() # 设置训练模式\n\n  # Tracking variables\n  tr_loss = 0 #running loss\n  nb_tr_examples, nb_tr_steps = 0, 0\n  \n  # Train the data for one epoch\n  for step, batch in enumerate(train_dataloader):# 遍历批数据\n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    # 每一批数据展开\n    # train_inputs.extend(one_freq_input_ids)\n    # train_labels.extend(one_freq_labels)\n    # train_masks.extend(one_freq_attention_masks)\n    # train_token_types.extend(one_freq_token_types)\n    # 接收batch的输入\n    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n    # Clear out the gradients (by default they accumulate)\n    optimizer.zero_grad()\n\n    # # Forward pass for multiclass classification\n    # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n    # loss = outputs[0]\n    # logits = outputs[1]\n\n    # Forward pass for multilabel classification\n    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n    logits = outputs[0]\n    loss_func = BCEWithLogitsLoss() # 计算损失\n    loss = loss_func(logits.view(-1,num_labels),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n    # loss_func = BCELoss() \n    # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n    train_loss_set.append(loss.item())# 记录loss    \n\n    # Backward pass\n    loss.backward() # loss反向求导\n    # Update parameters and take a step using the computed gradient\n    optimizer.step()\n    # scheduler.step()\n    # Update tracking variables\n    tr_loss += loss.item()\n    nb_tr_examples += b_input_ids.size(0)\n    nb_tr_steps += 1\n\n  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n\n###############################################################################\n\n  # Validation\n\n  # Put model in evaluation mode to evaluate loss on the validation set\n  model.eval()\n\n  # Variables to gather full output\n  logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n\n  # Predict\n  for i, batch in enumerate(validation_dataloader):\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n    with torch.no_grad():\n      # Forward pass\n      outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n      b_logit_pred = outs[0]\n      pred_label = torch.sigmoid(b_logit_pred)\n\n      b_logit_pred = b_logit_pred.detach().cpu().numpy()\n      pred_label = pred_label.to('cpu').numpy()\n      b_labels = b_labels.to('cpu').numpy()\n\n    tokenized_texts.append(b_input_ids)\n    logit_preds.append(b_logit_pred)\n    true_labels.append(b_labels)\n    pred_labels.append(pred_label)\n\n  # Flatten outputs\n  pred_labels = [item for sublist in pred_labels for item in sublist]\n  true_labels = [item for sublist in true_labels for item in sublist]\n\n  # 计算准确率\n  threshold = 0.50\n  pred_bools = [pl>threshold for pl in pred_labels]\n  true_bools = [tl==1 for tl in true_labels]\n  val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n  val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n\n  print('F1 Validation Accuracy: ', val_f1_accuracy)\n  print('Flat Validation Accuracy: ', val_flat_accuracy)"},{"cell_type":"code","execution_count":null,"metadata":{"id":"aiBeiBSRoOuz","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"torch.save(model.state_dict(), 'bert_model_toxic')"},{"cell_type":"markdown","metadata":{"id":"_7dd2GE3F4yK","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 加载数据以及预处理"},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"G5Q7hC4GFOLJ","jupyter":{},"outputId":"f74c59f3-1030-4ecd-9988-bdbf079b9776","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Null values:  False\n","Same columns between train and test:  True\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00001cee341fdb12</td>\n","      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0000247867823ef7</td>\n","      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00013b17ad220c46</td>\n","      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00017563c3f7919a</td>\n","      <td>:If you have a look back at the source, the in...</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00017695ad8997eb</td>\n","      <td>I don't anonymously edit articles at all.</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","      <td>-1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 id                                       comment_text  toxic  \\\n","0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...     -1   \n","1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...     -1   \n","2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...     -1   \n","3  00017563c3f7919a  :If you have a look back at the source, the in...     -1   \n","4  00017695ad8997eb          I don't anonymously edit articles at all.     -1   \n","\n","   severe_toxic  obscene  threat  insult  identity_hate  \n","0            -1       -1      -1      -1             -1  \n","1            -1       -1      -1      -1             -1  \n","2            -1       -1      -1      -1             -1  \n","3            -1       -1      -1      -1             -1  \n","4            -1       -1      -1      -1             -1  "]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":"test_df = pd.read_csv('data/03/test.csv')\ntest_labels_df = pd.read_csv('data/03/test_labels.csv')\ntest_df = test_df.merge(test_labels_df, on='id', how='left')\ntest_label_cols = list(test_df.columns[2:])\nprint('Null values: ', test_df.isnull().values.any()) #should not be any null sentences or labels\nprint('Same columns between train and test: ', label_cols == test_label_cols) #columns should be the same\ntest_df.head()"},{"cell_type":"code","execution_count":47,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"77rjCrMGpYxz","jupyter":{},"outputId":"c5f53ba7-7433-403f-906e-009dc8efc29f","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>comment_text</th>\n","      <th>toxic</th>\n","      <th>severe_toxic</th>\n","      <th>obscene</th>\n","      <th>threat</th>\n","      <th>insult</th>\n","      <th>identity_hate</th>\n","      <th>one_hot_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>0001ea8717f6de06</td>\n","      <td>Thank you for understanding. I think very high...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>000247e83dcc1211</td>\n","      <td>:Dear god this site is horrible.</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0002f87b16116a7f</td>\n","      <td>\"::: Somebody will invariably try to add Relig...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0003e1cccfd5a40a</td>\n","      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>00059ace3e3e9a53</td>\n","      <td>\" \\n\\n == Before adding a new product to the l...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  id                                       comment_text  \\\n","5   0001ea8717f6de06  Thank you for understanding. I think very high...   \n","7   000247e83dcc1211                   :Dear god this site is horrible.   \n","11  0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...   \n","13  0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...   \n","14  00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...   \n","\n","    toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n","5       0             0        0       0       0              0   \n","7       0             0        0       0       0              0   \n","11      0             0        0       0       0              0   \n","13      0             0        0       0       0              0   \n","14      0             0        0       0       0              0   \n","\n","        one_hot_labels  \n","5   [0, 0, 0, 0, 0, 0]  \n","7   [0, 0, 0, 0, 0, 0]  \n","11  [0, 0, 0, 0, 0, 0]  \n","13  [0, 0, 0, 0, 0, 0]  \n","14  [0, 0, 0, 0, 0, 0]  "]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":"test_df = test_df[~test_df[test_label_cols].eq(-1).any(axis=1)] #remove irrelevant rows/comments with -1 values\ntest_df['one_hot_labels'] = list(test_df[test_label_cols].values)\ntest_df.head()"},{"cell_type":"code","execution_count":48,"metadata":{"id":"1a41OmU2i7qp","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"# Gathering input data\ntest_labels = list(test_df.one_hot_labels.values)\ntest_comments = list(test_df.comment_text.values)"},{"cell_type":"code","execution_count":49,"metadata":{"id":"amySMO8EQzf2","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["F:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2271: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":"# 测试集分词编码\ntest_encodings = tokenizer.batch_encode_plus(test_comments,max_length=max_length,pad_to_max_length=True)\ntest_input_ids = test_encodings['input_ids']\ntest_token_type_ids = test_encodings['token_type_ids']\ntest_attention_masks = test_encodings['attention_mask']"},{"cell_type":"code","execution_count":50,"metadata":{"id":"hqOfi9fkRaRN","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"# Make tensors out of data\ntest_inputs = torch.tensor(test_input_ids)\ntest_labels = torch.tensor(test_labels)\ntest_masks = torch.tensor(test_attention_masks)\ntest_token_types = torch.tensor(test_token_type_ids)\n# Create test dataloader\ntest_data = TensorDataset(test_inputs, test_masks, test_labels, test_token_types)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n# Save test dataloader\ntorch.save(test_dataloader,'test_data_loader')"},{"cell_type":"markdown","metadata":{"id":"PFTWxCA_GBau","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 预测与评估"},{"cell_type":"code","execution_count":null,"metadata":{"id":"NPvrL6OFSQvf","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"# Test\n\n# Put model in evaluation mode to evaluate loss on the validation set\nmodel.eval()\n\n#track variables\nlogit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n\n# Predict\nfor i, batch in enumerate(test_dataloader):\n  batch = tuple(t.to(device) for t in batch)\n  # Unpack the inputs from our dataloader\n  b_input_ids, b_input_mask, b_labels, b_token_types = batch\n  with torch.no_grad():\n    # Forward pass\n    outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n    b_logit_pred = outs[0]\n    pred_label = torch.sigmoid(b_logit_pred)\n\n    b_logit_pred = b_logit_pred.detach().cpu().numpy()\n    pred_label = pred_label.to('cpu').numpy()\n    b_labels = b_labels.to('cpu').numpy()\n\n  tokenized_texts.append(b_input_ids)\n  logit_preds.append(b_logit_pred)\n  true_labels.append(b_labels)\n  pred_labels.append(pred_label)\n\n# Flatten outputs\ntokenized_texts = [item for sublist in tokenized_texts for item in sublist]\npred_labels = [item for sublist in pred_labels for item in sublist]\ntrue_labels = [item for sublist in true_labels for item in sublist]\n# Converting flattened binary values to boolean values\ntrue_bools = [tl==1 for tl in true_labels]"},{"cell_type":"markdown","metadata":{"id":"bQeGWqeMzAoZ","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"我们需要对范围为 [0, 1] 的 sigmoid 函数输出进行阈值处理。 下面我使用 0.50 作为阈值。"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"id":"BZcZUcYOxxmM","jupyter":{},"outputId":"abc926d2-3b6d-432c-83af-76ebb8652e90","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test F1 Accuracy:  0.6792739193783389\n","Test Flat Accuracy:  0.8803651255118947 \n","\n","               precision    recall  f1-score   support\n","\n","        toxic       0.57      0.86      0.69      6090\n"," severe_toxic       0.40      0.47      0.43       367\n","      obscene       0.64      0.78      0.70      3691\n","       threat       0.45      0.70      0.55       211\n","       insult       0.69      0.70      0.69      3427\n","identity_hate       0.75      0.49      0.59       712\n","\n","    micro avg       0.61      0.77      0.68     14498\n","    macro avg       0.58      0.67      0.61     14498\n"," weighted avg       0.62      0.77      0.68     14498\n","  samples avg       0.07      0.07      0.07     14498\n","\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":"pred_bools = [pl>0.50 for pl in pred_labels] #boolean output after thresholding\n\n# Print and save classification report\nprint('Test F1 Accuracy: ', f1_score(true_bools, pred_bools,average='micro'))\nprint('Test Flat Accuracy: ', accuracy_score(true_bools, pred_bools),'\\n')\nclf_report = classification_report(true_bools,pred_bools,target_names=test_label_cols)\npickle.dump(clf_report, open('classification_report.txt','wb')) #save report\nprint(clf_report)"},{"cell_type":"markdown","metadata":{"id":"5rLqrHK87eir","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 结果输出"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"CJBkRdGN1hzx","jupyter":{},"outputId":"b0301daf-0966-4060-c68c-2181cc43b3c3","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{0: 'toxic', 1: 'severe_toxic', 2: 'obscene', 3: 'threat', 4: 'insult', 5: 'identity_hate'}\n"]}],"source":"idx2label = dict(zip(range(6),label_cols))\nprint(idx2label)"},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZUglV_A4BF_","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"# Getting indices of where boolean one hot vector true_bools is True so we can use idx2label to gather label names\ntrue_label_idxs, pred_label_idxs=[],[]\nfor vals in true_bools:\n  true_label_idxs.append(np.where(vals)[0].flatten().tolist())\nfor vals in pred_bools:\n  pred_label_idxs.append(np.where(vals)[0].flatten().tolist())"},{"cell_type":"code","execution_count":null,"metadata":{"id":"OOGhXM3R4a91","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"# Gathering vectors of label names using idx2label\ntrue_label_texts, pred_label_texts = [], []\nfor vals in true_label_idxs:\n  if vals:\n    true_label_texts.append([idx2label[val] for val in vals])\n  else:\n    true_label_texts.append(vals)\n\nfor vals in pred_label_idxs:\n  if vals:\n    pred_label_texts.append([idx2label[val] for val in vals])\n  else:\n    pred_label_texts.append(vals)"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5HaqV6pn_HCG","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"# Decoding input ids to comment text\ncomment_texts = [tokenizer.decode(text,skip_special_tokens=True,clean_up_tokenization_spaces=False) for text in tokenized_texts]"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"R7kk0Mgl1L-T","jupyter":{},"outputId":"651efb03-73d7-4627-e1bf-a74c0e90066e","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>comment_text</th>\n","      <th>true_labels</th>\n","      <th>pred_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>thank you for understanding . i think very hig...</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>: dear god this site is horrible .</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\" : : : somebody will invariably try to add re...</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>\" it says it right there that it is a type . t...</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>\" = = before adding a new product to the list ...</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                        comment_text true_labels pred_labels\n","0  thank you for understanding . i think very hig...          []          []\n","1                 : dear god this site is horrible .          []          []\n","2  \" : : : somebody will invariably try to add re...          []          []\n","3  \" it says it right there that it is a type . t...          []          []\n","4  \" = = before adding a new product to the list ...          []          []"]},"execution_count":null,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":"# Converting lists to df\ncomparisons_df = pd.DataFrame({'comment_text': comment_texts, 'true_labels': true_label_texts, 'pred_labels':pred_label_texts})\ncomparisons_df.to_csv('comparisons.csv')\ncomparisons_df.head()"},{"cell_type":"markdown","metadata":{"id":"PWFd18u3zlF8","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## F1：阈值搜索"},{"cell_type":"code","execution_count":51,"metadata":{"id":"75B119AABE1245409E6FD4579C294C18","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":"macro_thresholds = np.array(range(1,10))/10\nmacro_thresholds"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"id":"PHlhb2lvar8V","jupyter":{},"outputId":"f5773e91-f6b5-47bd-b235-63e1140a178a","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Threshold:  0.6\n","Test F1 Accuracy:  0.6838089672413233\n","Test Flat Accuracy:  0.8889149395104567 \n","\n","               precision    recall  f1-score   support\n","\n","        toxic       0.60      0.83      0.70      6090\n"," severe_toxic       0.47      0.34      0.39       367\n","      obscene       0.68      0.74      0.71      3691\n","       threat       0.52      0.63      0.57       211\n","       insult       0.73      0.63      0.68      3427\n","identity_hate       0.79      0.41      0.54       712\n","\n","    micro avg       0.65      0.73      0.68     14498\n","    macro avg       0.63      0.60      0.60     14498\n"," weighted avg       0.66      0.73      0.68     14498\n","  samples avg       0.07      0.07      0.07     14498\n","\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":"\n\n\nf1_results, flat_acc_results = [], []\nfor th in macro_thresholds:\n  pred_bools = [pl>th for pl in pred_labels]\n  test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n  test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n  f1_results.append(test_f1_accuracy)\n  flat_acc_results.append(test_flat_accuracy)\n\nbest_macro_th = macro_thresholds[np.argmax(f1_results)] #best macro threshold value\n\nmicro_thresholds = (np.array(range(10))/100)+best_macro_th #calculating micro threshold values\n\nf1_results, flat_acc_results = [], []\nfor th in micro_thresholds:\n  pred_bools = [pl>th for pl in pred_labels]\n  test_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')\n  test_flat_accuracy = accuracy_score(true_bools, pred_bools)\n  f1_results.append(test_f1_accuracy)\n  flat_acc_results.append(test_flat_accuracy)\n\nbest_f1_idx = np.argmax(f1_results) #best threshold value\n\n# Printing and saving classification report\nprint('Best Threshold: ', micro_thresholds[best_f1_idx])\nprint('Test F1 Accuracy: ', f1_results[best_f1_idx])\nprint('Test Flat Accuracy: ', flat_acc_results[best_f1_idx], '\\n')\n\nbest_pred_bools = [pl>micro_thresholds[best_f1_idx] for pl in pred_labels]\nclf_report_optimized = classification_report(true_bools,best_pred_bools, target_names=label_cols)\npickle.dump(clf_report_optimized, open('classification_report_optimized.txt','wb'))\nprint(clf_report_optimized)"},{"cell_type":"code","execution_count":null,"metadata":{"id":"oBg6UCYAYtIe","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":4}