{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MyETdB-dkBsX","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"## 基于Bert进行实体识别任务微调"},{"cell_type":"markdown","metadata":{"id":"D4E3702AC7B14F7EA802EE7704E2477D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"致Great，ChallengeHub公众号，微信：1185918903，备注NLP技术交流\n\n知乎：https://www.zhihu.com/people/483684d821a67a8d43ef449ae607ad6b\n\n和鲸主页：https://www.heywhale.com/home/user/profile/58f387e7a686fb29e425d133\n\n和鲸训练营-零基础入门实体识别：https://www.heywhale.com/home/activity/detail/6216f74572960d0017d5e691"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"e7wfLWyYkvDi","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"#### **所需要的pip包**\n\n* pandas\n* numpy\n* sklearn\n* pytorch\n* transformers：\n    https://github.com/huggingface/transformers\n    \n    https://huggingface.co/models\n* seqeval\n\n"},{"cell_type":"code","execution_count":1,"metadata":{"colab":{},"colab_type":"code","id":"d4_YJqjR_Gjw","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"#!pip install transformers seqeval[gpu]"},{"cell_type":"code","execution_count":1,"metadata":{"colab":{},"colab_type":"code","id":"IEnlUbgm8z3B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nimport torch\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertConfig, BertForTokenClassification"},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"Sm1krxJtKxpx","outputId":"c0cf11ba-17ff-4a35-da11-5b7b0c4cb11d","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\nprint(device)"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ahwMsmyG5ZPE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"#### **数据处理**\n"},{"cell_type":"markdown","metadata":{"id":"6B73B57B4E44496F9ACE7303140C51A5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"比赛数据下载地址：商品标题实体识别\nhttps://www.heywhale.com/home/competition/620b34ed28270b0017b823ad"},{"cell_type":"code","execution_count":3,"metadata":{"id":"B323BE08ACCE40AD90B6D624C004502D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   0  1  2\n","0  1  2  3\n","1  4  5  6"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":"pd.DataFrame([[1,2,3],\n             [4,5,6]])"},{"cell_type":"code","execution_count":null,"metadata":{"id":"8A76BDE3851E4822A5148A7AA57656AD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":4,"metadata":{"tags":[],"id":"9308AC0116C34BA894F5066BB4EEDD79","jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████| 2288791/2288791 [00:05<00:00, 401430.65it/s]\n"]}],"source":"# with open('data/05/train_data/train.txt','r',encoding='utf-8') as f:\nwith open('/home/mw/input/task056960/train.txt','r',encoding='utf-8') as f:\n\n    tmp=[]\n    cnt=1\n    for line in tqdm(f.read().split('\\n')):\n        sentence_id=f'train_{cnt}'\n        # print(line)\n        if line!='\\n' and len(line.strip())>0:\n            \n            word_tags=line.split(' ')\n            # print(word_tags,len(word_tags))\n            if len(word_tags)==2:\n                tmp.append([sentence_id]+word_tags)\n            elif len(word_tags)==3: # ['', '', 'O'] 3 空格\n                # word=' '.join(word_tags[:-1])\n                word='[SEP]'\n                tag=word_tags[-1]\n                tmp.append([sentence_id,word,tag])\n        else:\n            cnt+=1"},{"cell_type":"code","execution_count":5,"metadata":{"id":"285E3C3D23F1435A8C10FFFADD5837E7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["['train_1', '手', 'B-40']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":"tmp[0]"},{"cell_type":"code","execution_count":6,"metadata":{"id":"CC1211D3D9FE44B5ADABEC3AA035BAA9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_id</th>\n","      <th>words</th>\n","      <th>tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>train_1</td>\n","      <td>手</td>\n","      <td>B-40</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>train_1</td>\n","      <td>机</td>\n","      <td>I-40</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>train_1</td>\n","      <td>三</td>\n","      <td>B-4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>train_1</td>\n","      <td>脚</td>\n","      <td>I-4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>train_1</td>\n","      <td>架</td>\n","      <td>I-4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  sentence_id words  tags\n","0     train_1     手  B-40\n","1     train_1     机  I-40\n","2     train_1     三   B-4\n","3     train_1     脚   I-4\n","4     train_1     架   I-4"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":"data=pd.DataFrame(tmp,columns=['sentence_id','words','tags'])\ndata.head()"},{"cell_type":"markdown","metadata":{"id":"6CABF2E7433E464B987F90C0C0268743","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"- 验证空格\n```\n外 I-7\n支 B-4\n撑 I-4\n架 I-4\n  O\n【 O\n女 B-16\n```"},{"cell_type":"code","execution_count":7,"metadata":{"id":"1A618D102D7540F288735EA0656CF710","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_id</th>\n","      <th>words</th>\n","      <th>tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>train_1</td>\n","      <td>手</td>\n","      <td>B-40</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>train_1</td>\n","      <td>机</td>\n","      <td>I-40</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>train_1</td>\n","      <td>三</td>\n","      <td>B-4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>train_1</td>\n","      <td>脚</td>\n","      <td>I-4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>train_1</td>\n","      <td>架</td>\n","      <td>I-4</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>61</th>\n","      <td>train_1</td>\n","      <td>+</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>62</th>\n","      <td>train_1</td>\n","      <td>蓝</td>\n","      <td>B-11</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>train_1</td>\n","      <td>牙</td>\n","      <td>I-11</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>train_1</td>\n","      <td>遥</td>\n","      <td>B-11</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>train_1</td>\n","      <td>控</td>\n","      <td>I-11</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>66 rows × 3 columns</p>\n","</div>"],"text/plain":["   sentence_id words  tags\n","0      train_1     手  B-40\n","1      train_1     机  I-40\n","2      train_1     三   B-4\n","3      train_1     脚   I-4\n","4      train_1     架   I-4\n","..         ...   ...   ...\n","61     train_1     +     O\n","62     train_1     蓝  B-11\n","63     train_1     牙  I-11\n","64     train_1     遥  B-11\n","65     train_1     控  I-11\n","\n","[66 rows x 3 columns]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":"data[data['sentence_id']=='train_1']"},{"cell_type":"code","execution_count":9,"metadata":{"id":"F76D8F594A164399B698D043921D62E0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["sentence_id    train_1\n","words            [SEP]\n","tags                 O\n","Name: 50, dtype: object"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":"data.iloc[50]"},{"cell_type":"markdown","metadata":{"id":"2ADBC13343154BB3BE40148046924A23","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"- 转为句子"},{"cell_type":"code","execution_count":8,"metadata":{"id":"8242620B38BF4BA28011CFE73054AD26","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_id</th>\n","      <th>words</th>\n","      <th>tags</th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>train_1</td>\n","      <td>手</td>\n","      <td>B-40</td>\n","      <td>手 机 三 脚 架 网 红 直 播 支 架 桌 面 自 拍 杆 蓝 牙 遥 控 三 脚 架 ...</td>\n","      <td>B-40,I-40,B-4,I-4,I-4,B-14,I-14,B-5,I-5,B-4,I-...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>train_1</td>\n","      <td>机</td>\n","      <td>I-40</td>\n","      <td>手 机 三 脚 架 网 红 直 播 支 架 桌 面 自 拍 杆 蓝 牙 遥 控 三 脚 架 ...</td>\n","      <td>B-40,I-40,B-4,I-4,I-4,B-14,I-14,B-5,I-5,B-4,I-...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>train_1</td>\n","      <td>三</td>\n","      <td>B-4</td>\n","      <td>手 机 三 脚 架 网 红 直 播 支 架 桌 面 自 拍 杆 蓝 牙 遥 控 三 脚 架 ...</td>\n","      <td>B-40,I-40,B-4,I-4,I-4,B-14,I-14,B-5,I-5,B-4,I-...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>train_1</td>\n","      <td>脚</td>\n","      <td>I-4</td>\n","      <td>手 机 三 脚 架 网 红 直 播 支 架 桌 面 自 拍 杆 蓝 牙 遥 控 三 脚 架 ...</td>\n","      <td>B-40,I-40,B-4,I-4,I-4,B-14,I-14,B-5,I-5,B-4,I-...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>train_1</td>\n","      <td>架</td>\n","      <td>I-4</td>\n","      <td>手 机 三 脚 架 网 红 直 播 支 架 桌 面 自 拍 杆 蓝 牙 遥 控 三 脚 架 ...</td>\n","      <td>B-40,I-40,B-4,I-4,I-4,B-14,I-14,B-5,I-5,B-4,I-...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  sentence_id words  tags                                           sentence  \\\n","0     train_1     手  B-40  手 机 三 脚 架 网 红 直 播 支 架 桌 面 自 拍 杆 蓝 牙 遥 控 三 脚 架 ...   \n","1     train_1     机  I-40  手 机 三 脚 架 网 红 直 播 支 架 桌 面 自 拍 杆 蓝 牙 遥 控 三 脚 架 ...   \n","2     train_1     三   B-4  手 机 三 脚 架 网 红 直 播 支 架 桌 面 自 拍 杆 蓝 牙 遥 控 三 脚 架 ...   \n","3     train_1     脚   I-4  手 机 三 脚 架 网 红 直 播 支 架 桌 面 自 拍 杆 蓝 牙 遥 控 三 脚 架 ...   \n","4     train_1     架   I-4  手 机 三 脚 架 网 红 直 播 支 架 桌 面 自 拍 杆 蓝 牙 遥 控 三 脚 架 ...   \n","\n","                                         word_labels  \n","0  B-40,I-40,B-4,I-4,I-4,B-14,I-14,B-5,I-5,B-4,I-...  \n","1  B-40,I-40,B-4,I-4,I-4,B-14,I-14,B-5,I-5,B-4,I-...  \n","2  B-40,I-40,B-4,I-4,I-4,B-14,I-14,B-5,I-5,B-4,I-...  \n","3  B-40,I-40,B-4,I-4,I-4,B-14,I-14,B-5,I-5,B-4,I-...  \n","4  B-40,I-40,B-4,I-4,I-4,B-14,I-14,B-5,I-5,B-4,I-...  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":"data['sentence'] = data[['sentence_id','words','tags']].groupby(['sentence_id'])['words'].transform(lambda x: ' '.join(x))\ndata['word_labels'] = data[['sentence_id','words','tags']].groupby(['sentence_id'])['tags'].transform(lambda x: ','.join(x))\ndata.head()"},{"cell_type":"code","execution_count":9,"metadata":{"id":"E2F7DDBA999847CC8073CC90EE500932","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["(2248790, 5)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":"data.shape"},{"cell_type":"code","execution_count":10,"metadata":{"id":"21F5F27E29B64482A08CAF06987B5007","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["40000"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":"data['sentence_id'].nunique()"},{"cell_type":"code","execution_count":11,"metadata":{"scrolled":true,"tags":[],"id":"D150AB6CF1A14C63AE895012FF789333","jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["{'B-40': 0,\n"," 'I-40': 1,\n"," 'B-4': 2,\n"," 'I-4': 3,\n"," 'B-14': 4,\n"," 'I-14': 5,\n"," 'B-5': 6,\n"," 'I-5': 7,\n"," 'B-7': 8,\n"," 'I-7': 9,\n"," 'B-11': 10,\n"," 'I-11': 11,\n"," 'B-13': 12,\n"," 'I-13': 13,\n"," 'B-8': 14,\n"," 'I-8': 15,\n"," 'O': 16,\n"," 'B-16': 17,\n"," 'I-16': 18,\n"," 'B-29': 19,\n"," 'I-29': 20,\n"," 'B-9': 21,\n"," 'I-9': 22,\n"," 'B-12': 23,\n"," 'I-12': 24,\n"," 'B-18': 25,\n"," 'I-18': 26,\n"," 'B-1': 27,\n"," 'I-1': 28,\n"," 'B-3': 29,\n"," 'I-3': 30,\n"," 'B-22': 31,\n"," 'I-22': 32,\n"," 'B-37': 33,\n"," 'I-37': 34,\n"," 'B-39': 35,\n"," 'I-39': 36,\n"," 'B-10': 37,\n"," 'I-10': 38,\n"," 'B-36': 39,\n"," 'I-36': 40,\n"," 'B-34': 41,\n"," 'I-34': 42,\n"," 'B-31': 43,\n"," 'I-31': 44,\n"," 'B-38': 45,\n"," 'I-38': 46,\n"," 'B-54': 47,\n"," 'I-54': 48,\n"," 'B-6': 49,\n"," 'I-6': 50,\n"," 'B-30': 51,\n"," 'I-30': 52,\n"," 'B-15': 53,\n"," 'I-15': 54,\n"," 'B-2': 55,\n"," 'I-2': 56,\n"," 'B-49': 57,\n"," 'I-49': 58,\n"," 'B-21': 59,\n"," 'I-21': 60,\n"," 'B-47': 61,\n"," 'I-47': 62,\n"," 'B-23': 63,\n"," 'I-23': 64,\n"," 'B-20': 65,\n"," 'I-20': 66,\n"," 'B-50': 67,\n"," 'I-50': 68,\n"," 'B-46': 69,\n"," 'I-46': 70,\n"," 'B-41': 71,\n"," 'I-41': 72,\n"," 'B-43': 73,\n"," 'I-43': 74,\n"," 'B-48': 75,\n"," 'I-48': 76,\n"," 'B-19': 77,\n"," 'I-19': 78,\n"," 'B-52': 79,\n"," 'I-52': 80,\n"," 'B-33': 81,\n"," 'I-33': 82,\n"," 'B-28': 83,\n"," 'I-28': 84,\n"," 'B-32': 85,\n"," 'I-32': 86,\n"," 'B-44': 87,\n"," 'I-44': 88,\n"," 'B-25': 89,\n"," 'I-25': 90,\n"," 'B-17': 91,\n"," 'I-17': 92,\n"," 'B-42': 93,\n"," 'I-42': 94,\n"," 'B-24': 95,\n"," 'I-24': 96,\n"," 'B-53': 97,\n"," 'I-53': 98,\n"," 'B-26': 99,\n"," 'I-26': 100,\n"," 'B-35': 101,\n"," 'I-35': 102,\n"," 'B-51': 103,\n"," 'I-51': 104}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":"labels_to_ids = {k: v for v, k in enumerate(data.tags.unique())}\nids_to_labels = {v: k for v, k in enumerate(data.tags.unique())}\nlabels_to_ids"},{"cell_type":"code","execution_count":12,"metadata":{"id":"90327FE9583B4341B22525126087B9DA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["105"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":"len(labels_to_ids)"},{"cell_type":"code","execution_count":13,"metadata":{"id":"093EA98193D64A4294865A30418E2EDE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>手 机 三 脚 架 网 红 直 播 支 架 桌 面 自 拍 杆 蓝 牙 遥 控 三 脚 架 ...</td>\n","      <td>B-40,I-40,B-4,I-4,I-4,B-14,I-14,B-5,I-5,B-4,I-...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>牛 皮 纸 袋 手 提 袋 定 制 l o g o 烘 焙 购 物 服 装 包 装 外 卖 ...</td>\n","      <td>B-4,I-4,I-4,I-4,B-4,I-4,I-4,B-29,I-29,I-29,I-2...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>彩 色 金 属 镂 空 鱼 尾 夹 长 尾 夹 [SEP] 手 帐 设 计 绘 图 文 具 ...</td>\n","      <td>B-16,I-16,B-12,I-12,B-13,I-13,B-4,I-4,I-4,B-4,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>B o s e [SEP] S o u n d S p o r t [SEP] F r e ...</td>\n","      <td>B-1,I-1,I-1,I-1,O,B-3,I-3,I-3,I-3,I-3,I-3,I-3,...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>壁 挂 炉 专 用 水 空 调 散 热 器 带 风 扇 暖 气 片 水 暖 空 调 明 装 ...</td>\n","      <td>B-4,I-4,I-4,O,O,B-4,I-4,I-4,B-4,I-4,I-4,B-22,I...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence  \\\n","0  手 机 三 脚 架 网 红 直 播 支 架 桌 面 自 拍 杆 蓝 牙 遥 控 三 脚 架 ...   \n","1  牛 皮 纸 袋 手 提 袋 定 制 l o g o 烘 焙 购 物 服 装 包 装 外 卖 ...   \n","2  彩 色 金 属 镂 空 鱼 尾 夹 长 尾 夹 [SEP] 手 帐 设 计 绘 图 文 具 ...   \n","3  B o s e [SEP] S o u n d S p o r t [SEP] F r e ...   \n","4  壁 挂 炉 专 用 水 空 调 散 热 器 带 风 扇 暖 气 片 水 暖 空 调 明 装 ...   \n","\n","                                         word_labels  \n","0  B-40,I-40,B-4,I-4,I-4,B-14,I-14,B-5,I-5,B-4,I-...  \n","1  B-4,I-4,I-4,I-4,B-4,I-4,I-4,B-29,I-29,I-29,I-2...  \n","2  B-16,I-16,B-12,I-12,B-13,I-13,B-4,I-4,I-4,B-4,...  \n","3  B-1,I-1,I-1,I-1,O,B-3,I-3,I-3,I-3,I-3,I-3,I-3,...  \n","4  B-4,I-4,I-4,O,O,B-4,I-4,I-4,B-4,I-4,I-4,B-22,I...  "]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":"data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n# 也可以根据sentence_id去重\ndata.head()"},{"cell_type":"code","execution_count":14,"metadata":{"id":"5B19BCC37F5E43D39C5F770C211A7253","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["39995"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":"len(data)"},{"cell_type":"code","execution_count":15,"metadata":{"id":"5ECAE64E5D3C4BB28E2272E69DC86DAB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["'手 机 三 脚 架 网 红 直 播 支 架 桌 面 自 拍 杆 蓝 牙 遥 控 三 脚 架 摄 影 拍 摄 拍 照 抖 音 看 电 视 神 器 三 角 架 便 携 伸 缩 懒 人 户 外 支 撑 架 [SEP] 【 女 神 粉 】 自 带 三 脚 架 + 蓝 牙 遥 控'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":"data.iloc[0].sentence"},{"cell_type":"code","execution_count":16,"metadata":{"id":"611A6AEA4C0A45C794A7103C192470C7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["'B-40,I-40,B-4,I-4,I-4,B-14,I-14,B-5,I-5,B-4,I-4,B-7,I-7,B-4,I-4,I-4,B-11,I-11,B-11,I-11,B-4,I-4,I-4,B-5,I-5,B-5,I-5,B-5,I-5,B-13,I-13,B-4,I-4,I-4,I-4,I-4,B-4,I-4,I-4,B-11,I-11,B-11,I-11,B-8,I-8,B-7,I-7,B-4,I-4,I-4,O,O,B-16,I-16,I-16,O,O,O,B-4,I-4,I-4,O,B-11,I-11,B-11,I-11'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":"data.iloc[0].word_labels"},{"cell_type":"code","execution_count":17,"metadata":{"id":"33E15C6243B1408E9ED0CA54CB682A72","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["66"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":"len(data['sentence'][0].split(' '))"},{"cell_type":"code","execution_count":18,"metadata":{"id":"17E97631CA934B37B976921976A8D0A0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["count    39995.000000\n","mean        56.220828\n","std         13.473300\n","min          7.000000\n","25%         46.000000\n","50%         56.000000\n","75%         65.000000\n","max        101.000000\n","Name: sentence, dtype: float64"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":"data['sentence'].apply(lambda x:len(x.split(' '))).describe()"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"f5EHpuB78pIa","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"#### **构建DataLoader**"},{"cell_type":"code","execution_count":19,"metadata":{"colab":{},"colab_type":"code","id":"lgNSM8Xz79Mg","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"MAX_LEN = 105 # 120\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 32\nEPOCHS = 10\nLEARNING_RATE = 2e-05\nMAX_GRAD_NORM = 5\nMODEL_NAME='hfl/chinese-roberta-wwm-ext'\ntokenizer = BertTokenizer.from_pretrained(MODEL_NAME) # encode_plus()# 整体"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wPYV2Ld6Tr5I","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"BERT做NER 一个棘手部分是 BERT 依赖于 **wordpiece tokenization**，而不是 word tokenization。 \n\n比如：Washington的标签为 \"b-gpe\",分词之后得到， \"Wash\", \"##ing\", \"##ton\",\"b-gpe\", \"b-gpe\", \"b-gpe\"\n\n\n\n\n"},{"cell_type":"code","execution_count":20,"metadata":{"colab":{},"colab_type":"code","id":"RNzSgZTfGUd8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n    \"\"\"\n  \n    \n    Word piece tokenization使得很难将词标签与单个subword进行匹配。\n    这个函数每次次对每个单词进行一个分词，这样方便为每个subword保留正确的标签。 \n    当然，它的处理时间有点慢，但它会帮助我们的模型达到更高的精度。\n    \"\"\"\n\n    tokenized_sentence = []\n    labels = []\n\n    sentence = sentence.strip()\n\n    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n\n        # 逐字分词\n        tokenized_word = tokenizer.tokenize(word) # id\n        n_subwords = len(tokenized_word) # 1\n\n        # 将单个字分词结果追加到句子分词列表\n        tokenized_sentence.extend(tokenized_word)\n\n        # 标签同样添加n个subword，与原始word标签一致\n        labels.extend([label] * n_subwords)\n\n    return tokenized_sentence, labels"},{"cell_type":"code","execution_count":21,"metadata":{"id":"2564B4C2DE96422F819E7C10B32ADD63","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["sentence       手 机 三 脚 架 网 红 直 播 支 架 桌 面 自 拍 杆 蓝 牙 遥 控 三 脚 架 ...\n","word_labels    B-40,I-40,B-4,I-4,I-4,B-14,I-14,B-5,I-5,B-4,I-...\n","Name: 0, dtype: object"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":"data.iloc[0]"},{"cell_type":"code","execution_count":24,"metadata":{"id":"8949A7455967467BB3338C618BAB8392","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# tokenize_and_preserve_labels(data.iloc[0]['sentence'],data.iloc[0]['word_labels'],tokenizer)"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ez7qlFHl56ZW","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":">这里有其他的处理方式，比如只有第一个subword给定原始标签，其他subword给定一个无关标签"},{"cell_type":"code","execution_count":26,"metadata":{"id":"B43A795239D443A592B248F227AF0E94","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\n# https://arxiv.org/abs/1810.04805"},{"cell_type":"code","execution_count":23,"metadata":{"id":"97BAFF071A4A40A5881CB148681E935C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":"encoding_result=tokenizer.encode_plus('这里有其他的处理方式，比如只有第一个subword给定原始标签，其他subword给定一个无关标签')\nencoding_result.keys()"},{"cell_type":"code","execution_count":28,"metadata":{"id":"9B080435530A4A2582C5DF2C7A607DC4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [101, 6821, 7027, 3300, 1071, 800, 4638, 1905, 4415, 3175, 2466, 8024, 3683, 1963, 1372, 3300, 5018, 671, 702, 11541, 8204, 10184, 5314, 2137, 1333, 1993, 3403, 5041, 8024, 1071, 800, 11541, 8204, 10184, 5314, 2137, 671, 702, 3187, 1068, 3403, 5041, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":"encoding_result"},{"cell_type":"code","execution_count":29,"metadata":{"id":"70A93A2F814047869D34F93BE7A23230","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [101, 122, 102, 123, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":"tokenizer.encode_plus('1[SEP]2')"},{"cell_type":"code","execution_count":30,"metadata":{"id":"517C8B040C264169BB0FEE5A1807B5F4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# tokenizer.convert_ids_to_tokens([101, 6821, 7027, 3300, 1071, 800, 4638, 1905, 4415, 3175, 2466, 8024, 3683, 1963, 1372, 3300, 5018, 671, 702, 11541, 8204, 10184, 5314, 2137, 1333, 1993, 3403, 5041, 8024, 1071, 800, 11541, 8204, 10184, 5314, 2137, 671, 702, 3187, 1068, 3403, 5041, 102])"},{"cell_type":"code","execution_count":25,"metadata":{"colab":{},"colab_type":"code","id":"aJty_Abw8_xK","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"class dataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.len = len(dataframe)\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __getitem__(self, index):\n        # 步骤 1: 对每个句子分词\n        sentence = self.data.sentence[index]  \n        word_labels = self.data.word_labels[index]  \n        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n        \n        # 步骤 2: 添加特殊token并添加对应的标签\n        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n        labels.insert(0, \"O\") # 给[CLS] token添加O标签\n        labels.insert(-1, \"O\") # 给[SEP] token添加O标签\n\n        # 步骤 3: 截断/填充\n        maxlen = self.max_len\n\n        if (len(tokenized_sentence) > maxlen):\n          # 截断\n          tokenized_sentence = tokenized_sentence[:maxlen]\n          labels = labels[:maxlen]\n        else:\n          # 填充\n          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n\n        # 步骤 4: 构建attention mask\n        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n        \n        # 步骤 5: 将分词结果转为词表的id表示\n        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n\n        label_ids = [labels_to_ids[label] for label in labels]\n  \n        \n        return {\n              'ids': torch.tensor(ids, dtype=torch.long),\n              'mask': torch.tensor(attn_mask, dtype=torch.long),\n              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n              'targets': torch.tensor(label_ids, dtype=torch.long)\n        } \n    \n    def __len__(self):\n        return self.len"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hTP7zuWGWGUd","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"按照8：2比列将数据集，划分为训练集和测试集"},{"cell_type":"code","execution_count":26,"metadata":{"id":"D9886A9B950B4B27852CA43CEA9A4212","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"from sklearn.model_selection import train_test_split\n# train_dataset,test_dataset=train_test_split(data,test_size=0.2,random_state=42)"},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67},"colab_type":"code","id":"jrkdZBLYHVcB","outputId":"160536bb-9659-490e-db38-c5c7ff66b351","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["FULL Dataset: (39995, 2)\n","TRAIN Dataset: (35996, 2)\n","TEST Dataset: (3999, 2)\n"]}],"source":"train_size = 0.9\ntrain_dataset = data.sample(frac=train_size,random_state=200) # 训练训练集和验证集 0.9比例\ntest_dataset = data.drop(train_dataset.index).reset_index(drop=True) # 0.1\ntrain_dataset = train_dataset.reset_index(drop=True)\n\nprint(\"FULL Dataset: {}\".format(data.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntraining_set = dataset(train_dataset, tokenizer, MAX_LEN)\ntesting_set = dataset(test_dataset, tokenizer, MAX_LEN)"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ptv5AT_iTb7W","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"下面为第一个样本的分词id与标签："},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"phmPylgAm8Xy","outputId":"27a7cee8-c920-4602-c539-b117ce6236ec","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["{'ids': tensor([ 101, 5988, 1094,  102,  704, 2595, 5011, 5708,  102,  704, 2595, 5011,\n","         1059, 7151, 5052, 1928,  121,  119,  126,  155,  155, 7946, 5682, 5273,\n","         5682, 5905, 5682, 2110, 4495, 5440, 6407,  683, 4500, 4823, 5162, 5011,\n","         3296, 5708,  121,  119,  124,  129, 2110, 4495, 3152, 1072, 5041, 2099,\n","         3717,  102,  523, 6851, 3209,  121,  119,  126,  524,  122,  121,  121,\n","         3118, 5905, 5682, 1928,  116,  123, 3118, 5011,  102,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0]),\n"," 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0]),\n"," 'targets': tensor([16, 16, 16, 16,  2,  3,  3,  3, 16,  2,  3,  3, 12, 13, 13, 13, 25, 26,\n","         26, 26, 26, 17, 18, 17, 18, 17, 18, 14, 15,  6,  7,  7,  7,  2,  3,  3,\n","          3,  3, 25, 26, 26, 26, 14, 15,  2,  3, 16, 16, 16, 16, 16, 17, 18, 25,\n","         26, 26, 16, 25, 26, 26, 26, 12, 13, 13, 16, 16, 16, 16, 16, 16, 16, 16,\n","         16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n","         16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16])}"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":"training_set[0]"},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"DWgnNJrYW2GP","outputId":"2b5b5b04-b93c-4354-b104-10b96c7223ff","scrolled":true,"tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[CLS]       16   O\n","虎           16   O\n","冠           16   O\n","[SEP]       16   O\n","中           2   B-4\n","性           3   I-4\n","笔           3   I-4\n","芯           3   I-4\n","[SEP]       16   O\n","中           2   B-4\n","性           3   I-4\n","笔           3   I-4\n","全           12   B-13\n","针           13   I-13\n","管           13   I-13\n","头           13   I-13\n","0           25   B-18\n",".           26   I-18\n","5           26   I-18\n","m           26   I-18\n","m           26   I-18\n","黑           17   B-16\n","色           18   I-16\n","红           17   B-16\n","色           18   I-16\n","蓝           17   B-16\n","色           18   I-16\n","学           14   B-8\n","生           15   I-8\n","考           6   B-5\n","试           7   I-5\n","专           7   I-5\n","用           7   I-5\n","碳           2   B-4\n","素           3   I-4\n","笔           3   I-4\n","替           3   I-4\n","芯           3   I-4\n","0           25   B-18\n",".           26   I-18\n","3           26   I-18\n","8           26   I-18\n","学           14   B-8\n","生           15   I-8\n","文           2   B-4\n","具           3   I-4\n","签           16   O\n","字           16   O\n","水           16   O\n","[SEP]       16   O\n","【           16   O\n","透           17   B-16\n","明           18   I-16\n","0           25   B-18\n",".           26   I-18\n","5           26   I-18\n","】           16   O\n","1           25   B-18\n","0           26   I-18\n","0           26   I-18\n","支           26   I-18\n","蓝           12   B-13\n","色           13   I-13\n","头           13   I-13\n","+           16   O\n","2           16   O\n","支           16   O\n","笔           16   O\n","[SEP]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n"]}],"source":"for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"ids\"]), training_set[0][\"targets\"]):\n  print('{0:10}  {1}   {2}'.format(token, label,ids_to_labels[label.numpy().tolist()]))"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ky68FcTgWnfN","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"创建Pytorch的DataLoader"},{"cell_type":"code","execution_count":30,"metadata":{"colab":{},"colab_type":"code","id":"KIw793myWOmi","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"73OzU7oXRxR8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"#### **定义网络**"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"T-iGhnhdLNdP","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"- 模型结构：BertForTokenClassification\n\n- 预训练权重： \"bert-base-uncased\""},{"cell_type":"code","execution_count":37,"metadata":{"id":"7068863A6E244E89AA78C5596B8E63BC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["105"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":"len(labels_to_ids)"},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"cB9MR3KcWXUs","outputId":"c0d93a4f-cd40-4f6b-edab-6dd51f2ee33b","scrolled":true,"tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=105, bias=True)\n",")"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":"model = BertForTokenClassification.from_pretrained(MODEL_NAME, num_labels=len(labels_to_ids))\nmodel.to(device)"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Pp7Yl4JyWhDj","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"#### **训练模型**\n\n"},{"cell_type":"code","execution_count":39,"metadata":{"id":"E7C21CA10C7E4C92857ABAC60E25C908","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# ids.shape"},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"eqAN7YVIjKTr","outputId":"0a6acb55-a46e-46b6-e562-e4205a9857cd","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor(4.4463, device='cuda:0', grad_fn=<NllLossBackward>)"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":"ids = training_set[0][\"ids\"].unsqueeze(0)\nmask = training_set[0][\"mask\"].unsqueeze(0)\ntargets = training_set[0][\"targets\"].unsqueeze(0) # 真实标签\nids = ids.to(device)\nmask = mask.to(device)\ntargets = targets.to(device)\noutputs = model(input_ids=ids, attention_mask=mask, labels=targets) # 输出有两个：一个为loss和一个为logits\ninitial_loss = outputs[0]\ninitial_loss"},{"cell_type":"code","execution_count":null,"metadata":{"id":"186007D6C5D04D60B7E75ED9906D5A69","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yLdwsru9Mh7U","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"模型输出logits大小为 (batch_size, sequence_length, num_labels):"},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"X-z6YCpGnvfj","outputId":"b873271e-b7f8-48b6-f4f2-b63c3ccccbe5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([1, 105, 105])"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":"tr_logits = outputs[1]\ntr_logits.shape"},{"cell_type":"code","execution_count":null,"metadata":{"id":"86D9B13352F94B2C8F5C94FFE70FC6FB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kwDLXxOVOCvD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"设置优化器Adam"},{"cell_type":"code","execution_count":42,"metadata":{"colab":{},"colab_type":"code","id":"kznSQfGIWdU4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"},{"cell_type":"code","execution_count":43,"metadata":{"colab":{},"colab_type":"code","id":"GLFivpkwW1HY","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# 训练函数\ndef train(epoch):\n    tr_loss, tr_accuracy = 0, 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    tr_preds, tr_labels = [], []\n    # 将model设置为train模式\n    model.train()\n    \n    for idx, batch in enumerate(training_loader):\n        \n        ids = batch['ids'].to(device, dtype = torch.long) #(4,91)\n        mask = batch['mask'].to(device, dtype = torch.long) #(4,91)\n        targets = batch['targets'].to(device, dtype = torch.long)#(4,91)\n        \n        \n        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n        loss, tr_logits = outputs[0],outputs[1]\n        # print(outputs.keys())\n        # print(loss)\n        tr_loss += loss.item()\n\n        nb_tr_steps += 1\n        nb_tr_examples += targets.size(0)\n        \n        if idx % 500==0:\n            loss_step = tr_loss/nb_tr_steps\n            print(f\"Training loss per 500 training steps: {loss_step}\")\n           \n        # 计算准确率\n        flattened_targets = targets.view(-1) # 真实标签 大小 (batch_size * seq_len,)\n        active_logits = tr_logits.view(-1, model.num_labels) # 模型输出shape (batch_size * seq_len, num_labels)\n        flattened_predictions = torch.argmax(active_logits, axis=1) # 取出每个token对应概率最大的标签索引 shape (batch_size * seq_len,)\n        # MASK：PAD\n        active_accuracy = mask.view(-1) == 1 # shape (batch_size * seq_len,)\n        targets = torch.masked_select(flattened_targets, active_accuracy)\n        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n        \n        tr_preds.extend(predictions)\n        tr_labels.extend(targets)\n        \n        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n        tr_accuracy += tmp_tr_accuracy\n    \n        # 梯度剪切\n        torch.nn.utils.clip_grad_norm_(\n            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n        )\n        \n        # loss反向求导\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    epoch_loss = tr_loss / nb_tr_steps\n    tr_accuracy = tr_accuracy / nb_tr_steps\n    print(f\"Training loss epoch: {epoch_loss}\")\n    print(f\"Training accuracy epoch: {tr_accuracy}\")"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"k2dsCyP7dcF3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"训练模型"},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"colab_type":"code","id":"y07Ybw8rZeZ7","outputId":"25bec966-fa2c-461b-a1cf-647fb26b143f","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training epoch: 1\n","Training loss per 500 training steps: 4.467368125915527\n","Training loss epoch: 1.4484657735824584\n","Training accuracy epoch: 0.4421112756600715\n","Training epoch: 2\n","Training loss per 500 training steps: 0.7548065185546875\n","Training loss epoch: 0.6540753860473633\n","Training accuracy epoch: 0.7134929778351335\n","Training epoch: 3\n","Training loss per 500 training steps: 0.5577887892723083\n","Training loss epoch: 0.5146245424747466\n","Training accuracy epoch: 0.7632178597980472\n","Training epoch: 4\n","Training loss per 500 training steps: 0.5142039060592651\n","Training loss epoch: 0.4485367271900177\n","Training accuracy epoch: 0.7861171425380934\n","Training epoch: 5\n","Training loss per 500 training steps: 0.3622732162475586\n","Training loss epoch: 0.40163199257850646\n","Training accuracy epoch: 0.8036167708797474\n","Training epoch: 6\n","Training loss per 500 training steps: 0.33577480912208557\n","Training loss epoch: 0.3655732891559601\n","Training accuracy epoch: 0.817925539477108\n","Training epoch: 7\n","Training loss per 500 training steps: 0.3556213974952698\n","Training loss epoch: 0.33396338832378386\n","Training accuracy epoch: 0.8321387432772448\n","Training epoch: 8\n","Training loss per 500 training steps: 0.2402544766664505\n","Training loss epoch: 0.3086724944114685\n","Training accuracy epoch: 0.842461614085518\n","Training epoch: 9\n","Training loss per 500 training steps: 0.27871450781822205\n","Training loss epoch: 0.28365160536766054\n","Training accuracy epoch: 0.8547518607224288\n","Training epoch: 10\n","Training loss per 500 training steps: 0.2653774917125702\n","Training loss epoch: 0.26000313901901245\n","Training accuracy epoch: 0.865739406386226\n"]}],"source":"for epoch in range(EPOCHS):\n    print(f\"Training epoch: {epoch + 1}\")\n    train(epoch)"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"r4jcSOJr680a","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"#### **评估模型**"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rYUTuOEUdfFJ","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"验证集评估"},{"cell_type":"code","execution_count":45,"metadata":{"colab":{},"colab_type":"code","id":"RIVVfFHi7Aw7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"def valid(model, testing_loader):\n    # put model in evaluation mode\n    model.eval()\n    \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_examples, nb_eval_steps = 0, 0\n    eval_preds, eval_labels = [], []\n    \n    with torch.no_grad():\n        for idx, batch in enumerate(testing_loader):\n            \n            ids = batch['ids'].to(device, dtype = torch.long)\n            mask = batch['mask'].to(device, dtype = torch.long)\n            targets = batch['targets'].to(device, dtype = torch.long)\n            \n            # loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=targets)\n            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n            loss, eval_logits = outputs[0],outputs[1]\n            eval_loss += loss.item()\n\n            nb_eval_steps += 1\n            nb_eval_examples += targets.size(0)\n        \n            if idx % 100==0:\n                loss_step = eval_loss/nb_eval_steps\n                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n              \n            # 计算准确率\n            flattened_targets = targets.view(-1) # 大小 (batch_size * seq_len,)\n            active_logits = eval_logits.view(-1, model.num_labels) # 大小 (batch_size * seq_len, num_labels)\n            flattened_predictions = torch.argmax(active_logits, axis=1) # 大小 (batch_size * seq_len,)\n            active_accuracy = mask.view(-1) == 1 # 大小 (batch_size * seq_len,)\n            targets = torch.masked_select(flattened_targets, active_accuracy)\n            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n            \n            eval_labels.extend(targets)\n            eval_preds.extend(predictions)\n            \n            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n            eval_accuracy += tmp_eval_accuracy\n    \n    #print(eval_labels)\n    #print(eval_preds)\n\n    labels = [ids_to_labels[id.item()] for id in eval_labels]\n    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n\n    #print(labels)\n    #print(predictions)\n    \n    eval_loss = eval_loss / nb_eval_steps\n    eval_accuracy = eval_accuracy / nb_eval_steps\n    print(f\"Validation Loss: {eval_loss}\")\n    print(f\"Validation Accuracy: {eval_accuracy}\")\n\n    return labels, predictions"},{"cell_type":"code","execution_count":46,"metadata":{"colab":{},"colab_type":"code","id":"2BrxRjvxApY8","outputId":"60f9abbe-8272-41f6-bcaf-03571ecf726b","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation loss per 100 evaluation steps: 0.5618929266929626\n","Validation loss per 100 evaluation steps: 0.48445309949393317\n","Validation loss per 100 evaluation steps: 0.47979988669281576\n","Validation loss per 100 evaluation steps: 0.47257913079768715\n","Validation loss per 100 evaluation steps: 0.4780060602096548\n","Validation loss per 100 evaluation steps: 0.4786392832825522\n","Validation loss per 100 evaluation steps: 0.47741880680677695\n","Validation loss per 100 evaluation steps: 0.4783220507108876\n","Validation loss per 100 evaluation steps: 0.47767649447724464\n","Validation loss per 100 evaluation steps: 0.47792532454584863\n","Validation loss per 100 evaluation steps: 0.4774629267660173\n","Validation loss per 100 evaluation steps: 0.4775117500384865\n","Validation Loss: 0.4775778596666124\n","Validation Accuracy: 0.7795549782141706\n"]}],"source":"labels, predictions = valid(model, testing_loader)"},{"cell_type":"code","execution_count":47,"metadata":{"id":"3EB0EADA5D0B4D3AB951EFDDFFFAEDC2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# len(predictions),len(labels)"},{"cell_type":"code","execution_count":48,"metadata":{"id":"2484D28BBF674187B36EB685635AB18B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["O       319186\n","I-4     312855\n","B-4     167244\n","I-18    141111\n","I-38    131220\n","         ...  \n","B-53         5\n","B-35         3\n","I-35         2\n","B-26         1\n","I-26         1\n","Length: 105, dtype: int64"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":"tmp=[]\nfor tags in data['word_labels']:\n    tmp.extend(tags.split(','))\npd.Series(tmp).value_counts()"},{"cell_type":"code","execution_count":49,"metadata":{"id":"C74C81D62C924CE99956BDED2B80E3E0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["'I-16'"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":"ids_to_labels[18]"},{"cell_type":"code","execution_count":50,"metadata":{"colab":{},"colab_type":"code","id":"0jDNXrjr-6BW","outputId":"782922e5-198f-4007-f658-c973b8dddff8","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["F:\\ProgramData\\Anaconda3\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           1       0.82      0.92      0.87     22532\n","          10       0.41      0.46      0.44      8269\n","          11       0.69      0.77      0.73     55217\n","          12       0.71      0.79      0.75     11104\n","          13       0.63      0.68      0.65     61019\n","          14       0.84      0.89      0.86     20291\n","          15       0.54      0.64      0.59       763\n","          16       0.76      0.86      0.81     23692\n","          17       0.00      0.00      0.00        30\n","          18       0.65      0.69      0.67     55887\n","          19       0.00      0.00      0.00       115\n","           2       0.15      0.20      0.17      2764\n","          20       0.12      0.08      0.10       508\n","          21       0.10      0.32      0.16       539\n","          22       0.28      0.31      0.30      9511\n","          23       0.00      0.00      0.00        23\n","          24       0.00      0.00      0.00         4\n","          25       0.00      0.00      0.00        29\n","          26       0.00      0.00      0.00         1\n","          28       0.00      0.00      0.00        32\n","          29       0.64      0.72      0.68      4049\n","           3       0.41      0.56      0.47      9018\n","          30       0.00      0.00      0.00       494\n","          31       0.52      0.16      0.24       810\n","          32       0.00      0.00      0.00        44\n","          33       0.00      0.00      0.00        14\n","          34       0.00      0.00      0.00       245\n","          35       0.00      0.00      0.00         2\n","          36       0.36      0.50      0.42      3401\n","          37       0.68      0.79      0.73     13696\n","          38       0.49      0.61      0.55     28926\n","          39       0.31      0.38      0.34      4970\n","           4       0.79      0.82      0.80    154488\n","          40       0.61      0.65      0.63     30715\n","          41       0.00      0.00      0.00       456\n","          42       0.00      0.00      0.00        13\n","          43       0.00      0.00      0.00        79\n","          44       0.00      0.00      0.00        32\n","          46       0.00      0.00      0.00        21\n","          47       0.16      0.06      0.09      1222\n","          48       0.00      0.00      0.00       144\n","          49       0.32      0.29      0.30      1270\n","           5       0.67      0.73      0.70     36718\n","          50       0.47      0.49      0.48       352\n","          51       0.00      0.00      0.00        15\n","          52       0.00      0.00      0.00       158\n","          53       0.00      0.00      0.00         5\n","          54       0.65      0.59      0.61      5765\n","           6       0.55      0.67      0.60      1393\n","           7       0.83      0.89      0.86     22404\n","           8       0.81      0.90      0.85     16563\n","           9       0.41      0.52      0.46     11711\n","\n","   micro avg       0.67      0.74      0.70    621523\n","   macro avg       0.32      0.35      0.32    621523\n","weighted avg       0.68      0.74      0.71    621523\n","\n"]}],"source":"from seqeval.metrics import classification_report\n\nprint(classification_report([labels], [predictions])) # [] 避免报错TypeError: Found input variables without list of list."},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4Gz-wHAw3xMk","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"#### **预测**\n\n"},{"cell_type":"code","execution_count":84,"metadata":{"scrolled":true,"tags":[],"id":"B46DF365B3824462934EF45309A0AD92","jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 12213.48it/s]\n"]}],"source":"tmp=[]\ncnt=1\nwith open('/home/mw/input/task056960/sample_per_line_preliminary_A.txt','r',encoding='utf-8') as f:\n    for line in tqdm(f.read().split('\\n')):\n        sentence_id=f'test_{cnt}'\n        for word in line:\n            # print(word,sentence_id,cnt)\n            if word.strip():\n                tmp.append([sentence_id,word,'O'])\n            else:\n                tmp.append([sentence_id,'[SEP]','O'])\n                \n        cnt+=1"},{"cell_type":"code","execution_count":85,"metadata":{"id":"EE44CDD176B14E3C9E3D04C0382AEA78","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_id</th>\n","      <th>words</th>\n","      <th>tags</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>test_1</td>\n","      <td>O</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>test_1</td>\n","      <td>P</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>test_1</td>\n","      <td>P</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>test_1</td>\n","      <td>O</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>test_1</td>\n","      <td>闪</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  sentence_id words tags\n","0      test_1     O    O\n","1      test_1     P    O\n","2      test_1     P    O\n","3      test_1     O    O\n","4      test_1     闪    O"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":"test_data=pd.DataFrame(tmp,columns=['sentence_id','words','tags'])\ntest_data.head()"},{"cell_type":"code","execution_count":86,"metadata":{"id":"684DE4BEDBF7430BB86C775BBE58C537","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence_id</th>\n","      <th>words</th>\n","      <th>tags</th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>test_1</td>\n","      <td>O</td>\n","      <td>O</td>\n","      <td>O P P O 闪 充 充 电 器 [SEP] X 9 0 7 0 [SEP] X 9 0 ...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>test_1</td>\n","      <td>P</td>\n","      <td>O</td>\n","      <td>O P P O 闪 充 充 电 器 [SEP] X 9 0 7 0 [SEP] X 9 0 ...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>test_1</td>\n","      <td>P</td>\n","      <td>O</td>\n","      <td>O P P O 闪 充 充 电 器 [SEP] X 9 0 7 0 [SEP] X 9 0 ...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>test_1</td>\n","      <td>O</td>\n","      <td>O</td>\n","      <td>O P P O 闪 充 充 电 器 [SEP] X 9 0 7 0 [SEP] X 9 0 ...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>test_1</td>\n","      <td>闪</td>\n","      <td>O</td>\n","      <td>O P P O 闪 充 充 电 器 [SEP] X 9 0 7 0 [SEP] X 9 0 ...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  sentence_id words tags                                           sentence  \\\n","0      test_1     O    O  O P P O 闪 充 充 电 器 [SEP] X 9 0 7 0 [SEP] X 9 0 ...   \n","1      test_1     P    O  O P P O 闪 充 充 电 器 [SEP] X 9 0 7 0 [SEP] X 9 0 ...   \n","2      test_1     P    O  O P P O 闪 充 充 电 器 [SEP] X 9 0 7 0 [SEP] X 9 0 ...   \n","3      test_1     O    O  O P P O 闪 充 充 电 器 [SEP] X 9 0 7 0 [SEP] X 9 0 ...   \n","4      test_1     闪    O  O P P O 闪 充 充 电 器 [SEP] X 9 0 7 0 [SEP] X 9 0 ...   \n","\n","                                         word_labels  \n","0  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n","1  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n","2  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n","3  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n","4  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  "]},"execution_count":86,"metadata":{},"output_type":"execute_result"}],"source":"test_data['sentence'] = test_data[['sentence_id','words','tags']].groupby(['sentence_id'])['words'].transform(lambda x: ' '.join(x))\ntest_data['word_labels'] = test_data[['sentence_id','words','tags']].groupby(['sentence_id'])['tags'].transform(lambda x: ','.join(x))\ntest_data.head()"},{"cell_type":"code","execution_count":87,"metadata":{"id":"1EE09ADCCD814FAC8F372E7C161B31B5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>O P P O 闪 充 充 电 器 [SEP] X 9 0 7 0 [SEP] X 9 0 ...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>O W I N 净 水 器 家 用 厨 房 欧 恩 科 技 反 渗 透 纯 水 机 O - ...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>教 学 教 具 磁 条 贴 磁 性 条 背 胶 ( 3 M ) 软 磁 条 [SEP] 黑 ...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>【 限 时 促 销 】 笔 记 本 文 具 创 意 复 古 古 风 本 子 线 装 记 事 ...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2 0 本 装 a 5 笔 记 本 子 文 具 学 生 B 5 软 抄 本 子 记 事 本 ...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence  \\\n","0  O P P O 闪 充 充 电 器 [SEP] X 9 0 7 0 [SEP] X 9 0 ...   \n","1  O W I N 净 水 器 家 用 厨 房 欧 恩 科 技 反 渗 透 纯 水 机 O - ...   \n","2  教 学 教 具 磁 条 贴 磁 性 条 背 胶 ( 3 M ) 软 磁 条 [SEP] 黑 ...   \n","3  【 限 时 促 销 】 笔 记 本 文 具 创 意 复 古 古 风 本 子 线 装 记 事 ...   \n","4  2 0 本 装 a 5 笔 记 本 子 文 具 学 生 B 5 软 抄 本 子 记 事 本 ...   \n","\n","                                         word_labels  \n","0  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n","1  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n","2  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n","3  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  \n","4  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...  "]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":"test_data = test_data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n# 也可以根据sentence_id去重\ntest_data.head()"},{"cell_type":"code","execution_count":88,"metadata":{"id":"D2168D541B5C406E84F984BF512097A7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["'O P P O 闪 充 充 电 器 [SEP] X 9 0 7 0 [SEP] X 9 0 7 7 [SEP] R 5 [SEP] 快 充 头 通 用 手 机 数 据 线 [SEP] 套 餐 【 2 . 4 充 电 头 + 数 据 线 [SEP] 】 [SEP] 安 卓 [SEP] 1 . 5 m'"]},"execution_count":88,"metadata":{},"output_type":"execute_result"}],"source":"test_data['sentence'][0]"},{"cell_type":"code","execution_count":90,"metadata":{"id":"D7EEB17413344B6693096A2FE070E0E4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["'OPPO闪充充电器[SEP]X9070[SEP]X9077[SEP]R5[SEP]快充头通用手机数据线[SEP]套餐【2.4充电头+数据线[SEP]】[SEP]安卓[SEP]1.5m'"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":"''.join( test_data['sentence'][0].split())"},{"cell_type":"code","execution_count":92,"metadata":{"scrolled":true,"tags":[],"id":"B57D4491CD72423ABE204397DCABA5A4","jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["(['o',\n","  'p',\n","  'p',\n","  'o',\n","  '闪',\n","  '充',\n","  '充',\n","  '电',\n","  '器',\n","  '[SEP]',\n","  'x',\n","  '9',\n","  '0',\n","  '7',\n","  '0',\n","  '[SEP]',\n","  'x',\n","  '9',\n","  '0',\n","  '7',\n","  '7',\n","  '[SEP]',\n","  'r',\n","  '5',\n","  '[SEP]',\n","  '快',\n","  '充',\n","  '头',\n","  '通',\n","  '用',\n","  '手',\n","  '机',\n","  '数',\n","  '据',\n","  '线',\n","  '[SEP]',\n","  '套',\n","  '餐',\n","  '【',\n","  '2',\n","  '.',\n","  '4',\n","  '充',\n","  '电',\n","  '头',\n","  '+',\n","  '数',\n","  '据',\n","  '线',\n","  '[SEP]',\n","  '】',\n","  '[SEP]',\n","  '安',\n","  '卓',\n","  '[SEP]',\n","  '1',\n","  '.',\n","  '5',\n","  'm'],\n"," ['O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O',\n","  'O'])"]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":"tokenize_and_preserve_labels(test_data.iloc[0]['sentence'],test_data.iloc[0]['word_labels'],tokenizer)"},{"cell_type":"code","execution_count":93,"metadata":{"id":"A9936AC2291E4473A7A2557826277018","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"test_set = dataset(test_data, tokenizer, MAX_LEN)"},{"cell_type":"code","execution_count":94,"metadata":{"id":"3105356B52D046AA9FFECA68D1E018F0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["{'ids': tensor([ 101,  157,  158,  158,  157, 7306, 1041, 1041, 4510, 1690,  102,  166,\n","          130,  121,  128,  121,  102,  166,  130,  121,  128,  128,  102,  160,\n","          126,  102, 2571, 1041, 1928, 6858, 4500, 2797, 3322, 3144, 2945, 5296,\n","          102, 1947, 7623,  523,  123,  119,  125, 1041, 4510, 1928,  116, 3144,\n","         2945, 5296,  102,  524,  102, 2128, 1294,  102,  122,  119,  126,  155,\n","          102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0]),\n"," 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0]),\n"," 'targets': tensor([16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n","         16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n","         16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n","         16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n","         16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16,\n","         16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16])}"]},"execution_count":94,"metadata":{},"output_type":"execute_result"}],"source":"test_set[0]\n"},{"cell_type":"code","execution_count":95,"metadata":{"scrolled":true,"tags":[],"id":"10DD87BA5ECE48849E00DE4D38BB6D30","jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[CLS]       16   O\n","o           16   O\n","p           16   O\n","p           16   O\n","o           16   O\n","闪           16   O\n","充           16   O\n","充           16   O\n","电           16   O\n","器           16   O\n","[SEP]       16   O\n","x           16   O\n","9           16   O\n","0           16   O\n","7           16   O\n","0           16   O\n","[SEP]       16   O\n","x           16   O\n","9           16   O\n","0           16   O\n","7           16   O\n","7           16   O\n","[SEP]       16   O\n","r           16   O\n","5           16   O\n","[SEP]       16   O\n","快           16   O\n","充           16   O\n","头           16   O\n","通           16   O\n","用           16   O\n","手           16   O\n","机           16   O\n","数           16   O\n","据           16   O\n","线           16   O\n","[SEP]       16   O\n","套           16   O\n","餐           16   O\n","【           16   O\n","2           16   O\n",".           16   O\n","4           16   O\n","充           16   O\n","电           16   O\n","头           16   O\n","+           16   O\n","数           16   O\n","据           16   O\n","线           16   O\n","[SEP]       16   O\n","】           16   O\n","[SEP]       16   O\n","安           16   O\n","卓           16   O\n","[SEP]       16   O\n","1           16   O\n",".           16   O\n","5           16   O\n","m           16   O\n","[SEP]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n","[PAD]       16   O\n"]}],"source":"for token, label in zip(tokenizer.convert_ids_to_tokens(test_set[0][\"ids\"]), test_set[0][\"targets\"]):\n  print('{0:10}  {1}   {2}'.format(token, label,ids_to_labels[label.numpy().tolist()]))"},{"cell_type":"code","execution_count":108,"metadata":{"id":"2D9150E3C63F4738956CFA665679B264","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"test_params = {'batch_size': 1,\n                'shuffle': False,\n                'num_workers': 0\n                }\ntesting_loader = DataLoader(test_set, **test_params)"},{"cell_type":"code","execution_count":121,"metadata":{"id":"4FE9CBA499FE4C4CBAA98F5385DA0CC7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"def predict():\n    # put model in evaluation mode\n    model.eval()\n    \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_examples, nb_eval_steps = 0, 0\n    eval_preds, eval_labels = [], []\n    \n    with torch.no_grad():\n        for idx, batch in tqdm(enumerate(testing_loader)):\n            \n            ids = batch['ids'].to(device, dtype = torch.long)\n            mask = batch['mask'].to(device, dtype = torch.long)\n            targets = batch['targets'].to(device, dtype = torch.long)\n            \n            # loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=targets)\n            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n            loss, eval_logits = outputs[0],outputs[1]\n            eval_loss += loss.item()\n\n            nb_eval_steps += 1\n            nb_eval_examples += targets.size(0)\n        \n#             if idx % 100==0:\n#                 print(f\"Validation steps: {idx}\")\n              \n            active_logits = eval_logits.view(-1, model.num_labels) # 大小 (batch_size * seq_len, num_labels)\n            flattened_predictions = torch.argmax(active_logits, axis=1) # 大小 (batch_size * seq_len,)\n            eval_preds.append(flattened_predictions)\n            \n    # predictions = [ids_to_labels[id.item()] for id in eval_preds]\n    return eval_preds"},{"cell_type":"code","execution_count":122,"metadata":{"id":"706A40EFD4F141BFBDCC01C2ADAF92DC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["10000it [02:55, 57.01it/s]\n"]}],"source":"test_preds=predict()"},{"cell_type":"code","execution_count":123,"metadata":{"id":"AF86DDB7F1114425A591546F1BB9AAA4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["10000"]},"execution_count":123,"metadata":{},"output_type":"execute_result"}],"source":"len(test_preds)"},{"cell_type":"code","execution_count":142,"metadata":{"id":"611C2D1A4D08494C8E3B6EE9A3EAA6D1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["['O',\n"," 'P',\n"," 'P',\n"," 'O',\n"," '闪',\n"," '充',\n"," '充',\n"," '电',\n"," '器',\n"," '[SEP]',\n"," 'X',\n"," '9',\n"," '0',\n"," '7',\n"," '0',\n"," '[SEP]',\n"," 'X',\n"," '9',\n"," '0',\n"," '7',\n"," '7',\n"," '[SEP]',\n"," 'R',\n"," '5',\n"," '[SEP]',\n"," '快',\n"," '充',\n"," '头',\n"," '通',\n"," '用',\n"," '手',\n"," '机',\n"," '数',\n"," '据',\n"," '线',\n"," '[SEP]',\n"," '套',\n"," '餐',\n"," '【',\n"," '2',\n"," '.',\n"," '4',\n"," '充',\n"," '电',\n"," '头',\n"," '+',\n"," '数',\n"," '据',\n"," '线',\n"," '[SEP]',\n"," '】',\n"," '[SEP]',\n"," '安',\n"," '卓',\n"," '[SEP]',\n"," '1',\n"," '.',\n"," '5',\n"," 'm']"]},"execution_count":142,"metadata":{},"output_type":"execute_result"}],"source":"test_data['sentence'][0].split()"},{"cell_type":"code","execution_count":141,"metadata":{"id":"2C79FEF48E0C4B3487327838E78BABD5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["['B-37',\n"," 'I-37',\n"," 'I-37',\n"," 'I-37',\n"," 'B-11',\n"," 'I-11',\n"," 'B-4',\n"," 'I-4',\n"," 'I-4',\n"," 'O',\n"," 'B-38',\n"," 'I-38',\n"," 'I-38',\n"," 'I-38',\n"," 'I-38',\n"," 'O',\n"," 'B-38',\n"," 'I-38',\n"," 'I-38',\n"," 'I-38',\n"," 'I-38',\n"," 'O',\n"," 'B-38',\n"," 'I-38',\n"," 'O',\n"," 'B-4',\n"," 'I-4',\n"," 'I-4',\n"," 'B-11',\n"," 'I-11',\n"," 'B-40',\n"," 'I-40',\n"," 'B-4',\n"," 'I-4',\n"," 'I-4',\n"," 'O',\n"," 'O',\n"," 'O',\n"," 'O',\n"," 'B-18',\n"," 'I-18',\n"," 'I-18',\n"," 'B-4',\n"," 'I-4',\n"," 'I-4',\n"," 'O',\n"," 'B-4',\n"," 'I-4',\n"," 'I-4',\n"," 'O',\n"," 'O',\n"," 'O',\n"," 'B-37',\n"," 'I-37',\n"," 'O',\n"," 'B-18',\n"," 'I-18',\n"," 'I-18',\n"," 'O']"]},"execution_count":141,"metadata":{},"output_type":"execute_result"}],"source":"[ids_to_labels[id.item()] for id in test_preds[0]][1:60]"},{"cell_type":"code","execution_count":151,"metadata":{"id":"758963B302BD402883F88A967D43DD79","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# test_sents[0]"},{"cell_type":"code","execution_count":150,"metadata":{"id":"7F0367A4E29941ADAA830249F78934F8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# y_pred[0][1:len(test_sents[0])+1]"},{"cell_type":"code","execution_count":156,"metadata":{"id":"9153150C67CB4DF78268014C491F9545","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"y_preds=[]\nfor pred in test_preds:\n    y_preds.append([ids_to_labels[id.item()] for id in pred])"},{"cell_type":"code","execution_count":143,"metadata":{"id":"C34AF0889704421CA07D3F1B2A51577B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"test_file='data/preliminary_test_a/sample_per_line_preliminary_A.txt'\ntest_sents=[]\nwith open(test_file, 'r', encoding='utf-8') as f:\n    for line in f.read().split('\\n'):\n        test_sents.append(line)"},{"cell_type":"code","execution_count":157,"metadata":{"id":"C6ABBD69F43E4D7BA24A6A4EAD889BEE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"list_results=[]\nfor sent, ner_tag in zip(test_sents, y_preds):\n    line_result=[]\n    for word, tag in zip(sent, ner_tag[1:len(test_sents)+1]):\n        line_result.append((word,tag))\n    list_results.append(line_result)    "},{"cell_type":"code","execution_count":158,"metadata":{"id":"60AA65B6AC034EEB9CED761423CDA8B0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"with open('crf.txt','w',encoding='utf-8') as f:\n    for i,line_result in enumerate(list_results):\n        for word,tag in line_result:\n            f.write(f'{word} {tag}\\n')\n        if i<len(list_results)-1:\n            f.write('\\n')"},{"cell_type":"code","execution_count":91,"metadata":{"colab":{},"colab_type":"code","id":"zPDla1mmZiax","outputId":"f78e6a27-642c-40b8-cc05-7ed0f936fa3f","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["oppo 闪 充 充 电 器 x9070 x9077 r5 快 充 头 通 用 手 机 数 据 线 套 餐 【 2 . 4 充 电 头 + 数 据 线 】 安 卓 1 . 5m\n","['B-37', 'B-11', 'I-4', 'B-4', 'I-4', 'I-4', 'B-38', 'I-38', 'I-38', 'B-38', 'I-38', 'I-38', 'B-38', 'I-38', 'B-4', 'I-4', 'I-4', 'B-11', 'I-11', 'B-40', 'I-40', 'B-4', 'I-4', 'I-4', 'O', 'O', 'O', 'B-18', 'I-18', 'I-18', 'B-4', 'I-4', 'I-4', 'O', 'B-4', 'I-4', 'I-4', 'O', 'B-37', 'I-37', 'B-18', 'I-18', 'O']\n"]}],"source":"# sentence ='OPPO闪充充电器[SEP]X9070[SEP]X9077[SEP]R5[SEP]快充头通用手机数据线[SEP]套餐【2.4充电头+数据线[SEP]】[SEP]安卓[SEP]1.5m'\n\n\n\n# inputs = tokenizer(sentence, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n\n# # 加载到gpu\n# ids = inputs[\"input_ids\"].to(device)\n# mask = inputs[\"attention_mask\"].to(device)\n# # 输入到模型\n# outputs = model(ids, mask)\n# logits = outputs[0]\n\n# active_logits = logits.view(-1, model.num_labels) # 大小 (batch_size * seq_len, num_labels)\n# flattened_predictions = torch.argmax(active_logits, axis=1) # 大小 (batch_size*seq_len,) \n\n# tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n# token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n# wp_preds = list(zip(tokens, token_predictions)) # tuple = (wordpiece, prediction)\n\n# word_level_predictions = []\n# for pair in wp_preds:\n#   if (pair[0].startswith(\" ##\")) or (pair[0] in ['[CLS]', '[SEP]', '[PAD]']):\n#     # skip prediction\n#     continue\n#   else:\n#     word_level_predictions.append(pair[1])\n\n# # 拼接文本\n# str_rep = \" \".join([t[0] for t in wp_preds if t[0] not in ['[CLS]', '[SEP]', '[PAD]']]).replace(\" ##\", \"\")\n# print(str_rep)\n# print(word_level_predictions)"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sqDklprSqB5d","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"#### **保存模型**"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VuUdX_fImswO","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"保存模型词汇表 、模型权重、配置文件，之后可以用 `from_pretrained()` \n\n"},{"cell_type":"code","execution_count":57,"metadata":{"colab":{},"colab_type":"code","id":"sDZtSsKKntuI","outputId":"79b2c502-fbd3-46b3-eb54-f6d53d9563d1","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["All files saved\n","This tutorial is completed\n"]}],"source":"import os\n\ndirectory = \"./model\"\n\nif not os.path.exists(directory):\n    os.makedirs(directory)\n\n# 保存tokenizer\ntokenizer.save_vocabulary(directory)\n# 保存权重和配置文件\nmodel.save_pretrained(directory)\nprint('All files saved')\nprint('This tutorial is completed')"},{"cell_type":"code","execution_count":null,"metadata":{"id":"51D549804EE54327ACF71BC877A00668","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":null,"metadata":{"id":"ED19C9E5C2874D289B9EC15DF43829B5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":4}