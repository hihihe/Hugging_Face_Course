{"cells":[{"cell_type":"markdown","metadata":{"id":"AF0069D7C75D4CE49234156A3A557E0E"},"source":["## Bert 简介\n","\n","BERT(Bidirectional Encoder Representation from Transformers)是2018年10月由Google AI研究院提出的一种预训练模型，该模型在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩: 全部两个衡量指标上全面超越人类，并且在11种不同NLP测试中创出SOTA表现，包括将GLUE基准推高至80.4% (绝对改进7.6%)，MultiNLI准确度达到86.7% (绝对改进5.6%)，成为NLP发展史上的里程碑式的模型成就。\n","\n","BERT的网络架构使用的是《Attention is all you need》中提出的多层Transformer结构，如 图1 所示。其最大的特点是抛弃了传统的RNN和CNN，通过Attention机制将任意位置的两个单词的距离转换成1，有效的解决了NLP中棘手的长期依赖问题。Transformer的结构在NLP领域中已经得到了广泛应用。\n","\n","- 原论文：[BERT: Pre-training of Deep Bidirectional Transformers forLanguage Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n","- 官方源码：[google-research/bert](https://github.com/google-research/bert/blob/master/modeling.py)\n","- 博客笔记：[Bert介绍](https://paddlepedia.readthedocs.io/en/latest/tutorials/pretrain_model/bert.html)\n","- Transformers笔记：[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n","\n","\n","## BERT框架\n","BERT整体框架包含pre-train和fine-tune两个阶段。pre-train阶段模型是在无标注的标签数据上进行训练，fine-tune阶段，BERT模型首先是被pre-train模型参数初始化，然后所有的参数会用下游的有标注的数据进行训练。\n","\n","ERT是用了Transformer的encoder侧的网络，encoder中的Self-attention机制在编码一个token的时候同时利用了其上下文的token，其中‘同时利用上下文’即为双向的体现，而并非想Bi-LSTM那样把句子倒序输入一遍。\n","\n","在它之前是GPT，GPT使用的是Transformer的decoder侧的网络，GPT是一个单向语言模型的预训练过程，更适用于文本生成，通过前文去预测当前的字。\n","\n","## Embedding\n","\n","\n","Embedding由三种Embedding求和而成：\n","\n","Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务\n","\n","Segment Embeddings用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务\n","\n","Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的\n","\n","其中[CLS]表示该特征用于分类模型，对非分类模型，该符号可以省去。[SEP]表示分句符号，用于断开输入语料中的两个句子。\n","\n","BERT在第一句前会加一个[CLS]标志，最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务等。因为与文本中已有的其它词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义。 具体来说，self-attention是用文本中的其它词来增强目标词的语义表示，但是目标词本身的语义还是会占主要部分的，因此，经过BERT的12层（BERT-base为例），每次词的embedding融合了所有词的信息，可以去更好的表示自己的语义。而[CLS]位本身没有语义，经过12层，句子级别的向量，相比其他正常词，可以更好的表征句子语义。\n","\n","## Transformer Encoder\n","\n","BERT是用了Transformer的encoder侧的网络，如上图的transformer的Encoder部分，关于transformer的encoder的详细介绍可以参考链接：https://paddlepedia.readthedocs.io/en/latest/tutorials/pretrain_model/transformer.html\n","\n","在Transformer中，模型的输入会被转换成512维的向量，然后分为8个head，每个head的维度是64维，但是BERT的维度是768维度，然后分成12个head，每个head的维度是64维，这是一个微小的差别。Transformer中position Embedding是用的三角函数，BERT中也有一个Postion Embedding是随机初始化，然后从数据中学出来的。\n","\n","BERT模型分为24层和12层两种，其差别就是使用transformer encoder的层数的差异，BERT-base使用的是12层的Transformer Encoder结构，BERT-Large使用的是24层的Transformer Encoder结构。"]},{"cell_type":"markdown","metadata":{"id":"6FEE12AA450D40F484E1BFAF38F43A9E"},"source":["## HuggingFace介绍"]},{"cell_type":"markdown","metadata":{"id":"E2A42865DFF4415DAD6B5A14FC04140B"},"source":["- huggingface github：https://github.com/huggingface/transformers\n","  > 查文档，看源码，理解模型(Bert src/transformers/models)和具体实现方式\n"," \n","- 社区网站：https://huggingface.co/\n","  > Models Datasets Docs Spaces  Solutions\n","  \n","- 论坛：https://discuss.huggingface.co/\n","\n"," > Bug 或者问题 资料"]},{"cell_type":"markdown","metadata":{"id":"225D62234833439D9DFC326A51CD0CB6"},"source":["## 预训练模型下载方式：\n","\n","\n","```\n","git lfs install\n","git clone https://huggingface.co/hfl/chinese-roberta-wwm-ext\n","# if you want to clone without large files – just their pointers\n","# prepend your git clone with the following env var:\n","GIT_LFS_SKIP_SMUDGE=1\n","```\n","\n","```\n","\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n","\n","model = AutoModelForMaskedLM.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n","\n","```"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"E260645CADBD474F83D3029DFC72E42C"},"outputs":[],"source":["# from transformers import BertTokenizer,BertModelForMaskedLM\n","\n","# tokenizer = BertTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n","\n","# model = BertModelForMaskedLM.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"7A85FF9C30614A0A91ABB38A0A11B6D3"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5985b6e51ba0495fa5198a1327f026b5","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/174 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"403188e27d73401daefe6ac3c833f355","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/729 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"585d05427a4b4e55b196e83a9e3cdff1","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/107k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7fca037cacee43e3b8a04150edadea67","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b0fde5c4562465ba475e879b83b47ea","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/15.4M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer, AutoModelForMaskedLM\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"ckiplab/albert-tiny-chinese\")\n","\n","model = AutoModelForMaskedLM.from_pretrained(\"ckiplab/albert-tiny-chinese\")"]},{"cell_type":"markdown","metadata":{"id":"E0509AEE449B45B5B196AD317ED3AFD3"},"source":["## 导入包"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"881C3BF3990842E59610C86646ECBE45"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n","Requirement already satisfied: transformers in f:\\programdata\\anaconda3\\lib\\site-packages (4.17.0)\n","Requirement already satisfied: sacremoses in f:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.0.46)\n","Requirement already satisfied: packaging>=20.0 in f:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n","Requirement already satisfied: regex!=2019.12.17 in f:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in f:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.11.6)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in f:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.2.1)\n","Requirement already satisfied: pyyaml>=5.1 in f:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in c:\\users\\yanqiang\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.26.0)\n","Requirement already satisfied: tqdm>=4.27 in f:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\yanqiang\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (1.22.3)\n","Requirement already satisfied: filelock in f:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in f:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\yanqiang\\appdata\\roaming\\python\\python39\\site-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: colorama in f:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\yanqiang\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\yanqiang\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (1.26.7)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\yanqiang\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yanqiang\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (3.3)\n","Requirement already satisfied: joblib in f:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in f:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (8.0.3)\n","Requirement already satisfied: six in c:\\users\\yanqiang\\appdata\\roaming\\python\\python39\\site-packages (from sacremoses->transformers) (1.15.0)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"3910CE3FA9C3470985E4FF98B27E01A1","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["from transformers import AutoConfig,AutoModel,AutoTokenizer,AdamW,get_linear_schedule_with_warmup,logging\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset,SequentialSampler,RandomSampler,DataLoader\n","\n","# from transformers import AutoConfig,AutoModel,AutoTokenizer"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"C7F1D36B335F4A7BA5EA532E58E66CA9","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["# 预训练模型名称\n","MODEL_NAME=\"bert-base-chinese\"\n","# MODEL_NAME=\"roberta-large\"\n"]},{"cell_type":"markdown","metadata":{"id":"7C5517E11A3248328E90F537FA943D5E","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## config"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"99A0FB0E66D441059431F08D7AB36B84","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["#  预训练模型配置\n","config = AutoConfig.from_pretrained(MODEL_NAME)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"9524E94B47BB4E048E23A159CD41007B","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["BertConfig {\n","  \"_name_or_path\": \"bert-base-chinese\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 21128\n","}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["config"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"91F8E742D30E4C67834D0791C10CC886"},"outputs":[],"source":["config.num_labels=12"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"B3704DB4095942AB8AE099FA1426FB7B"},"outputs":[],"source":["# config"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ACD60E6E15504E9AA1633322007A634E"},"outputs":[{"data":{"text/plain":["transformers.models.bert.configuration_bert.BertConfig"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["type(config)"]},{"cell_type":"markdown","metadata":{"id":"41BA813FF0EB4A09859CB33FF79DBCFB","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## tokenizer\n","\n","参考文档：https://huggingface.co/transformers/v4.6.0/main_classes/tokenizer.html"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"0AF447FF5A404767997746C2B32F65BD","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["PreTrainedTokenizerFast(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# 加载tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer"]},{"cell_type":"markdown","metadata":{"id":"74BBA0C0E3FA40548AAC34705BC722D6","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"source":["一些特殊符号：['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"43E8B4D8B3AC4F64A3C1EAEF9226FCD7","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["[100, 102, 0, 101, 103]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.all_special_ids"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"B0080E37CC8346408E26CBB428753435","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.all_special_tokens"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"55354112F78A4E8780692543E13091DA","jupyter":{},"scrolled":true,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["# tokenizer.vocab"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"3DAF15627FCD4958A2B716537ABB9C06","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["21128"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# 词汇表大小\n","tokenizer.vocab_size"]},{"cell_type":"markdown","metadata":{"id":"17FAAE0A360A4CB0B2B60E3CF428523F","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"source":["### 将文本转为词汇表id\n","\n","- 方法1\n","```\n","    def encode(\n","        self,\n","        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n","        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n","        add_special_tokens: bool = True,\n","        padding: Union[bool, str, PaddingStrategy] = False,\n","        truncation: Union[bool, str, TruncationStrategy] = False,\n","        max_length: Optional[int] = None,\n","        stride: int = 0,\n","        return_tensors: Optional[Union[str, TensorType]] = None,\n","        **kwargs\n","    ) -> List[int]:\n","        \"\"\"\n","        Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n","\n","        Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n","\n","        Args:\n","            text (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`):\n","                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using the\n","                ``tokenize`` method) or a list of integers (tokenized string ids using the ``convert_tokens_to_ids``\n","                method).\n","            text_pair (:obj:`str`, :obj:`List[str]` or :obj:`List[int]`, `optional`):\n","                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n","                the ``tokenize`` method) or a list of integers (tokenized string ids using the\n","                ``convert_tokens_to_ids`` method).\n","        \"\"\"\n","```"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"BD523FF0160340ABADFFD9BC8BBF15B8","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["[101, 2769, 1762, 1266, 776, 2339, 868, 102]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["text=\"我在北京工作\"\n","token_ids=tokenizer.encode(text)\n","token_ids"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"5DF25B8DA8DA44DC86067A8ECE39116A","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["list"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["type(token_ids)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"FE7A6C6EC3D14F918D61AF8CF9655502","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["['[CLS]', '我', '在', '北', '京', '工', '作', '[SEP]']"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# 将id转为原始字符\n","tokenizer.convert_ids_to_tokens(token_ids)"]},{"cell_type":"markdown","metadata":{"id":"7BDA7905556B43AC8877A1718C89FC90","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"source":["padding的模式"]},{"cell_type":"markdown","metadata":{"id":"ED78DF27402246AD8F12419EECAF1275","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"source":["```\n","padding (:obj:`bool`, :obj:`str` or :class:`~transformers.file_utils.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n","                 Select a strategy to pad the returned sequences (according to the model's padding side and padding\n","                 index) among:\n","\n","                * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a\n","                  single sequence if provided).\n","                * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n","                  maximum acceptable input length for the model if that argument is not provided.\n","                * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n","                  different lengths).\n","```"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"C1D7D84B6148407D85E207BF40D7E603","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["[101, 2769, 1762, 1266, 776, 2339, 868, 102]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# 加入参数\n","token_ids=tokenizer.encode(text,padding=True,max_length=30,add_special_tokens=True)\n","token_ids"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"C872E2F5E8DA4BD686A73F7BE7187C0E","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["[101,\n"," 2769,\n"," 1762,\n"," 1266,\n"," 776,\n"," 2339,\n"," 868,\n"," 102,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0]"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# 加入参数\n","token_ids=tokenizer.encode(text,padding=\"max_length\",max_length=30,add_special_tokens=True)\n","token_ids"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"E0D68B850E3C4A649BAE9CE2B0B7A07B","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["tensor([[ 101, 2769, 1762, 1266,  776, 2339,  868,  102,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0]])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["token_ids=tokenizer.encode(text,padding=\"max_length\",max_length=30,add_special_tokens=True,return_tensors='pt')\n","token_ids"]},{"cell_type":"markdown","metadata":{"id":"1BD6C4D3471B4A6DBA8D092E626EE445","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"source":["- 方法2 encode_plus\n","```\n","def encode_plus(\n","        self,\n","        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n","        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n","        add_special_tokens: bool = True,\n","        padding: Union[bool, str, PaddingStrategy] = False,\n","        truncation: Union[bool, str, TruncationStrategy] = False,\n","        max_length: Optional[int] = None,\n","        stride: int = 0,\n","        is_split_into_words: bool = False,\n","        pad_to_multiple_of: Optional[int] = None,\n","        return_tensors: Optional[Union[str, TensorType]] = None,\n","        return_token_type_ids: Optional[bool] = None,\n","        return_attention_mask: Optional[bool] = None,\n","        return_overflowing_tokens: bool = False,\n","        return_special_tokens_mask: bool = False,\n","        return_offsets_mapping: bool = False,\n","        return_length: bool = False,\n","        verbose: bool = True,\n","        **kwargs\n","    ) -> BatchEncoding:\n"," ```"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"05E0D2866AE1487B8859254282A0A170","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[ 101, 2769, 1762, 1266,  776, 2339,  868,  102,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","            0,    0,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0]])}"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["token_ids=tokenizer.encode_plus(\n","    text,padding=\"max_length\",\n","    max_length=30,\n","    add_special_tokens=True,\n","    return_tensors='pt',\n","    return_token_type_ids=True,\n","    return_attention_mask=True\n",")\n","token_ids"]},{"cell_type":"markdown","metadata":{"id":"CD4F524B710A4448857B0054E73D9ED8","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## Model"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"A563AF48A8804B8093686DD9414E11AC","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["model=AutoModel.from_pretrained(MODEL_NAME)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"7F61A65E550744F580D1E80476C58D3B","scrolled":true,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["BertModel(\n","  (embeddings): BertEmbeddings(\n","    (word_embeddings): Embedding(21128, 768, padding_idx=0)\n","    (position_embeddings): Embedding(512, 768)\n","    (token_type_embeddings): Embedding(2, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): BertEncoder(\n","    (layer): ModuleList(\n","      (0): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): BertLayer(\n","        (attention): BertAttention(\n","          (self): BertSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): BertSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): BertIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): BertOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): BertPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"66161C44C8D54FE28B7299F6831337F1","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["# outputs=model(token_ids['input_ids'],token_ids['token_type_ids'])\n","outputs=model(token_ids['input_ids'],token_ids['attention_mask'])\n","\n","# outputs=model(token_ids['input_ids'],token_ids['attention_mask'],token_ids['token_type_ids'])\n"]},{"cell_type":"code","execution_count":35,"metadata":{"collapsed":false,"id":"59D20E54E2854177880765DBCF581724","jupyter":{"outputs_hidden":false},"scrolled":true,"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.2670, -0.0858,  0.2122,  ..., -0.0070,  0.9425, -0.3466],\n","         [ 0.5193, -0.3700,  0.4482,  ..., -1.0237,  0.7864, -0.1775],\n","         [-0.1792, -0.7018,  1.0653,  ..., -0.3034,  1.0692,  0.0429],\n","         ...,\n","         [-0.0568, -0.1166,  0.2944,  ..., -0.1114,  0.0260, -0.2406],\n","         [-0.2842,  0.0047,  0.4074,  ..., -0.0445, -0.1530, -0.2477],\n","         [ 0.0038, -0.0741,  0.2955,  ..., -0.2048,  0.0951, -0.2106]]],\n","       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 0.9986,  0.9999,  0.9988,  0.9545, -0.6417,  0.5586,  0.3451,  0.6832,\n","          0.9936, -0.9965,  1.0000,  0.9999,  0.0969, -0.9015,  0.9994, -0.9996,\n","         -0.0634,  1.0000,  0.9828,  0.5460,  0.9992, -1.0000, -0.9602, -0.9486,\n","         -0.8842,  0.9878,  0.9769,  0.0949, -0.9995,  0.9895,  0.9659,  0.9994,\n","          0.9980, -0.9999, -0.9976,  0.5098, -0.7977,  0.9948, -0.7914, -0.9849,\n","         -0.9965, -0.5981,  0.3857, -0.9975, -0.9579,  0.4100, -1.0000, -1.0000,\n","          0.8568,  0.9991, -0.1765, -1.0000,  0.9296, -0.9318,  0.8056,  0.9725,\n","         -0.9998,  0.8912,  1.0000,  0.3592,  0.9997, -0.7306, -0.6022, -0.9998,\n","          1.0000, -0.9999, -0.9528,  0.1521,  0.9995,  1.0000, -0.9858,  0.4340,\n","          1.0000,  0.9561, -0.7498,  0.9997, -0.9917,  0.6525, -1.0000, -0.5290,\n","          1.0000,  0.9993, -0.9347,  0.8421, -0.9891, -0.9999, -0.9998,  0.9999,\n","         -0.5783,  0.8760,  0.9945, -0.9977, -1.0000,  0.9980, -0.9983, -0.9987,\n","         -0.8752,  0.9981, -0.3967, -0.8975, -0.5133,  0.9742, -0.9992, -0.9991,\n","          0.9994,  0.9994,  0.7277, -0.9995,  0.9999,  0.7907, -1.0000, -0.9462,\n","         -1.0000,  0.2198, -0.9616,  0.9996,  0.4455, -0.3929,  0.9995, -0.9991,\n","          0.7031, -0.9999, -0.7935, -0.9974,  0.9999,  0.9999,  0.9985, -0.9997,\n","          0.9998,  1.0000,  0.9194,  0.9896, -0.9930,  0.9952, -0.1052, -0.9834,\n","          0.7176, -0.9664,  1.0000,  0.9652,  0.9823, -0.9853,  0.9957, -0.9980,\n","          0.9999, -1.0000,  0.9945, -1.0000, -0.9994,  0.9953,  0.9923,  1.0000,\n","         -0.7978,  0.9999, -0.9812, -0.9999,  0.9990, -0.0079,  0.9991, -0.9999,\n","          0.9872,  0.8773, -0.8599,  0.7851, -1.0000,  0.9999, -0.8774,  1.0000,\n","          0.9998, -0.8900, -0.9732, -0.9988,  0.9746, -0.9995, -0.9984,  0.9864,\n","         -0.3062,  0.9885, -0.9927, -0.9211,  0.7024, -0.8854, -0.9998,  0.9979,\n","         -0.1070, -0.2068,  0.6250,  0.8880,  0.9973,  0.9898, -0.7060,  0.9999,\n","         -0.0964,  0.9962,  0.9989, -0.0794, -0.7561, -0.9706, -1.0000,  0.3083,\n","          0.9999, -0.7450, -0.9987,  0.9098, -1.0000,  0.9353, -0.2246,  0.5185,\n","         -0.9900, -0.9999,  0.9999, -0.9718, -0.9958,  0.6067, -0.9118,  0.3253,\n","         -1.0000,  0.9202,  0.9909, -0.8688,  0.5344, -0.7166, -0.9953,  0.9309,\n","         -0.8199,  0.9348,  0.9977,  1.0000,  0.9804, -0.7467, -0.9335,  1.0000,\n","          0.5077, -1.0000,  0.5815, -0.7935, -0.7349,  0.9998, -0.9990,  0.9095,\n","          1.0000,  0.9921,  1.0000, -0.2125, -0.9989, -0.9970,  1.0000,  0.9978,\n","          0.9998, -0.9985, -0.9991,  0.6060, -0.1385, -1.0000, -0.9962, -0.8801,\n","          0.9911,  1.0000,  0.2897, -0.9998, -0.2624, -0.9993,  1.0000, -0.8487,\n","          1.0000,  0.9556, -0.8725, -0.9962,  0.8722, -0.5077, -0.9997, -0.2779,\n","         -0.9996, -0.9924, -0.9999,  0.9055, -0.9990, -1.0000,  0.8632,  0.9999,\n","          0.9105, -0.9998,  0.9996,  0.9957, -0.9611, -0.9996,  0.9823, -1.0000,\n","          1.0000, -0.9969,  0.6207, -0.0030, -0.9880, -0.8604,  0.9991,  0.9997,\n","         -0.9974, -0.9256, -0.8272, -0.9999, -0.7311,  0.8521,  0.0231,  0.7647,\n","         -0.9838, -0.9336,  0.8415, -0.9954, -0.9999, -0.9192,  1.0000, -0.4956,\n","          1.0000,  0.4524,  1.0000,  0.9832, -0.9993,  0.9930,  0.8250, -0.5943,\n","         -0.7908, -0.9861,  0.8129,  0.2001,  0.5161, -0.9995,  0.9997,  0.9983,\n","          0.9893,  0.9763,  0.3462, -0.4559,  0.9393, -0.9982,  0.9976, -0.9996,\n","         -0.7520,  0.9971,  0.9999,  0.9999,  0.7595, -0.8876,  0.9727, -0.9980,\n","          0.9970, -0.9974,  0.9985, -0.9960,  0.9693, -0.7504, -0.9917,  1.0000,\n","          0.9545, -0.6712,  1.0000, -0.9418,  0.9384,  0.9999,  0.9206,  0.9717,\n","          0.6311,  0.9999, -0.9986, -0.9966, -0.9973, -0.9944, -0.9988, -1.0000,\n","          0.4478, -0.9976, -0.9626, -0.9599,  0.5757, -0.0107, -0.7348,  0.0048,\n","          0.0723,  0.8022, -0.9708,  0.2892,  0.9310, -0.9980, -0.9384, -1.0000,\n","         -0.9981,  0.9888,  0.9992, -0.9997,  0.9997, -1.0000, -0.9987,  0.9901,\n","          0.2053, -0.5843,  0.9998, -0.9999,  0.9686,  1.0000,  1.0000,  0.9991,\n","          0.9997, -0.9751, -0.9999, -0.9994, -0.9999, -1.0000, -0.9994,  0.7674,\n","          0.7939, -1.0000, -0.9327,  0.9427,  1.0000,  0.9453, -0.9987,  0.8275,\n","         -0.9995, -0.9830,  0.9995, -0.6096, -0.9989,  0.9999, -0.1734,  1.0000,\n","         -0.8638,  0.9956,  0.9765,  0.7885,  0.9677, -1.0000,  0.7434,  1.0000,\n","          0.5149, -0.9999, -0.5679, -0.9572, -1.0000, -0.1615,  0.9307,  0.9999,\n","         -0.9999, -0.6308, -0.9919,  0.3437,  0.9118,  0.9999,  0.9988,  0.8609,\n","          0.3412,  0.9425,  0.1690,  0.9997,  0.4484, -0.9968,  0.9974, -0.2034,\n","          0.5577, -1.0000,  0.9962,  0.4399,  0.9999,  0.9959,  0.6560, -0.9489,\n","         -0.9596,  0.9954,  1.0000, -0.9612,  0.9706, -0.9990, -1.0000, -0.9989,\n","         -0.0476, -0.7789, -0.9785, -0.9992,  0.8798,  0.9559,  1.0000,  0.9999,\n","          0.9957, -0.7819, -0.9561,  0.9869,  0.0119,  0.9998, -0.7133, -1.0000,\n","         -0.9949, -0.9999,  0.9996, -0.9068, -0.9097, -0.9300, -0.3992,  0.8845,\n","         -0.9999, -0.8416, -0.9979,  0.4116,  1.0000, -0.9875,  0.9986, -0.9986,\n","         -0.0395,  0.7331,  0.9024,  0.9995, -0.5490, -0.6971, -0.7122,  0.8567,\n","          0.9874,  0.9989, -0.9868,  0.8329,  0.9981, -0.9835,  0.9991,  0.6488,\n","          0.7209,  0.9834,  1.0000,  0.3964,  0.9979,  0.8983,  0.9999,  0.9999,\n","         -0.9403,  0.6022,  0.8283, -0.8373, -0.1218,  0.9771,  0.9999,  0.6683,\n","         -0.9757, -0.9997,  0.9984,  0.9961,  1.0000,  0.7415,  0.9946, -0.5225,\n","          0.9588,  0.8054,  0.7780,  0.1452,  0.4877,  0.9282,  0.9990, -1.0000,\n","         -1.0000, -1.0000,  1.0000,  0.9999, -0.6069, -1.0000,  0.9994, -0.6409,\n","          0.9728,  0.9938,  0.4333, -0.8666,  0.9610, -0.9995, -0.0485,  0.2587,\n","          0.3155,  0.7848,  0.9992, -0.9998, -0.6526,  1.0000,  0.0809,  0.9999,\n","          0.4925, -0.9816,  0.9979, -0.9703, -0.9998, -0.9115,  0.9998,  0.9994,\n","         -0.6118, -0.3596,  0.9993, -0.9996,  0.9999, -0.9999,  0.8994, -0.9990,\n","          0.9999, -0.9854, -0.9989, -0.5286,  0.1115,  0.9979, -0.5575,  0.9999,\n","         -0.7099, -0.9667, -0.4315, -0.9133, -0.9996, -0.9925,  0.1584, -0.9999,\n","          0.8137, -0.6677, -0.1643, -0.9849, -0.9998,  0.9999, -0.8938, -0.9912,\n","          0.9999, -0.9979, -1.0000,  0.7306, -0.9942, -0.5475,  0.9840,  0.6176,\n","          0.4018, -1.0000,  0.5113,  0.9995, -0.9994, -0.9433, -0.9860, -0.9887,\n","          0.2204,  0.9866,  0.9670, -0.0998,  0.3975, -0.2984,  0.8272,  0.6054,\n","          0.4439, -0.9957, -0.9461, -0.9810, -0.9991, -0.9991, -0.9999,  1.0000,\n","          0.9998,  0.9999,  0.7350, -0.8119,  0.7291,  0.9982, -0.9996, -0.5762,\n","          0.7971,  0.9614, -0.5536, -0.9997, -0.6525, -1.0000, -0.6792, -0.2272,\n","         -0.9713,  0.5998,  1.0000,  0.9999, -0.9997, -0.9987, -0.9992, -0.9950,\n","          0.9997,  0.9985,  0.9994, -0.8969, -0.7800,  0.9759,  0.1705, -0.1565,\n","         -0.9984, -0.9964, -0.9998,  0.7632, -0.9969, -0.9995,  0.9998,  0.9996,\n","          0.6582, -0.9999, -0.8716,  0.9998,  0.9992,  1.0000,  0.9521,  0.9998,\n","         -0.9946,  0.9990, -0.9999,  1.0000, -1.0000,  1.0000,  1.0000,  0.9882,\n","          0.9990, -0.9772,  0.9573,  0.1751, -0.3707,  0.9621, -0.6839, -0.9851,\n","          0.8741,  0.9970, -0.8459,  1.0000,  0.9007,  0.6603,  0.5746,  0.9532,\n","          0.8250, -0.2215, -0.9997,  0.9931,  0.9996,  0.9992,  1.0000,  0.9771,\n","          0.9999, -0.9744, -0.9995,  0.9920, -0.8009,  0.5036, -0.9990,  0.9999,\n","          1.0000, -0.9989, -0.7738,  0.5874,  0.4543,  0.9999,  0.9995,  0.9998,\n","          0.9350, -0.1731,  0.9999, -0.9996,  0.4741, -0.9847, -0.9183,  1.0000,\n","         -0.8106,  0.9993, -0.9607,  1.0000, -0.9767,  0.9727,  0.9982,  0.9664,\n","         -0.9946,  1.0000,  0.0554, -0.9968, -0.9992, -0.9921, -0.9907,  0.8621]],\n","       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["outputs"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"F9EC5A17510044819754052C485A9256","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([1, 30, 768])"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["last_hidden_state=outputs[0]\n","outputs[0].shape # last_hidden_state"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"5614FFF0493B4A5A856B4FB5F157096E","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([1, 768])"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["outputs[1].shape # pooler_output # 整个句子的Pooler output"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"F7EFA3D1CE1C478984AA5047216F17A4","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([1, 768])"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["cls_embeddings=last_hidden_state[:,0] # 第一个字符CLS的embedding表示\n","last_hidden_state[:,0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"954EDEEE90C740A9934FFF5710503D33","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"1D6D3D81B69A44D8802BCD16D115CD64","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"source":["## 对Bert输出进行变换"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"D77C4F85554C44578CAD45ACA5A53F93","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.6.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 21128\n","}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["config"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"32A648DB951C4425A9CA5EB15ADA52F9","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["config.update({\n","            'output_hidden_states':True\n","            }) "]},{"cell_type":"code","execution_count":26,"metadata":{"id":"03154256382F40DF81F7E0CE34D0004A","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}],"source":["model=AutoModel.from_pretrained(MODEL_NAME,config=config)\n","\n","outputs=model(token_ids['input_ids'],token_ids['token_type_ids'])"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"AC551CF1664348D38127CF2CD63F47CA","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["outputs.keys()"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"6FA2A6DD40194AB88D1F0161763630BD","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([1, 30, 768])"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["outputs['last_hidden_state'].shape"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"3A872FFAE51149219275025379FE8938","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([1, 768])"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["outputs['pooler_output'].shape"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"9ACACE59391A404D84C44B3F56F2D4AB","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["13"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["len(outputs['hidden_states'])"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"AD010BA40140445388E51DF3300BDB18","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[{"data":{"text/plain":["torch.Size([1, 30, 768])"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["outputs['hidden_states'][-1].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":false,"id":"20C85E423BBD42E0B3952FCFAE786ACC","jupyter":{"outputs_hidden":false},"notebookId":"61dabcf0d43cbc00178d5db9","slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A278C900A65540338700D0FE5438EBD4","jupyter":{},"notebookId":"61dabcf0d43cbc00178d5db9","slideshow":{"slide_type":"slide"},"tags":[]},"outputs":[],"source":["致Great：1185918903"]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":4}