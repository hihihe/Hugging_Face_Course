{"cells":[{"cell_type":"markdown","metadata":{"id":"4362F4C5189441F7971F10A84292942A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"本文我们将运用 Transformers 库来完成翻译任务。翻译是典型的 Seq2Seq (sequence-to-sequence) 任务，即对于给定的词语序列，输出一个对应的词语序列。翻译任务不仅与文本摘要任务很相似，而且我们可以将本文的操作应用于其他的 Seq2Seq 任务，例如：\n\n风格转换 (Style transfer)：将采用某种风格书写的文本转换为另一种风格，例如将文言文转换为白话文、将莎士比亚式英语转换为现代英语；\n生成式问答 (Generative question answering)：对于给定的问题，基于上下文生成对应的答案。\n如果有足够多的语料，我们可以从头训练一个翻译模型，但是微调预训练好的翻译模型会更快，比如将 mT5、mBART 等多语言模型微调到特定的语言对。\n\n本文我们将微调一个翻译模型进行英到罗马尼亚翻译，该模型已经基于大规模的 Opus 语料库对翻译任务进行了预训练，因此可以直接用于翻译。而通过我们的微调，可以进一步提升该模型在特定语料上的性能。"},{"cell_type":"code","execution_count":13,"metadata":{"id":"tmYJBBbwyiX3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"import os\nos.environ[\"WANDB_DISABLED\"]=\"true\""},{"cell_type":"markdown","metadata":{"id":"k1hOFHi-x5UZ","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# 安装所需要的包"},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SqADZ2Sgx5Ua","outputId":"60c95827-a686-45eb-c201-e23bc8b03b2b","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nRequirement already satisfied: datasets in /opt/conda/lib/python3.6/site-packages (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.6/site-packages (4.18.0)\nRequirement already satisfied: sacrebleu in /opt/conda/lib/python3.6/site-packages (2.0.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (1.1.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.96)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets) (2.26.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets) (1.19.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets) (0.24.2)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.6/site-packages (from datasets) (4.64.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from datasets) (2022.1.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets) (21.3)\nRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets) (0.8)\nRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets) (0.3.4)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets) (1.7.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets) (0.4.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.6/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.6/site-packages (from datasets) (0.10.6)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets) (0.70.12.2)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.0.12)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets) (2.1.10)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (2021.5.30)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (3.2)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets) (1.26.6)\nRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from responses<0.19->datasets) (1.15.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.6/site-packages (from tqdm>=4.62.1->datasets) (5.4.0)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.6/site-packages (from sacrebleu) (0.8.9)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.6/site-packages (from sacrebleu) (2.4.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.6/site-packages (from sacrebleu) (0.4.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from sacrebleu) (2019.6.8)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.49)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.12.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from transformers) (3.8.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: idna-ssl>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets) (1.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets) (5.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets) (19.3.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets) (3.1.0)\nRequirement already satisfied: pytz>=2011k in /opt/conda/lib/python3.6/site-packages (from pandas->datasets) (2019.1)\nRequirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets) (2.8.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf->transformers) (49.2.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (0.13.2)\n\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.3.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}],"source":"! pip install datasets transformers sacrebleu torch sentencepiece transformers[sentencepiece] -i https://pypi.tuna.tsinghua.edu.cn/simple"},{"cell_type":"markdown","metadata":{"id":"qN5RtJdM8xi5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"Make sure your version of Transformers is at least 4.11.0 since the functionality was introduced in that version"},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5xKFwtNEx5Uc","outputId":"1206f370-b7a8-4a34-bc6d-36f1a731be6a","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"4.18.0\n","name":"stdout"}],"source":"import transformers\nprint(transformers.__version__)"},{"cell_type":"markdown","metadata":{"id":"DKkJGYY19I11","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# 在翻译任务上微调模型"},{"cell_type":"markdown","metadata":{"id":"AT9VnMFn9O0_","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"在这个笔记本中，我们将看到如何为英语到汉语的翻译任务微调一个Hugggingface Transformers 模型。使用的数据为 WMT 数据集，这是一个机器翻译数据集，由各种来源的集合组成，包括新闻评论和议会会议记录。\n\n下面是使用预训练模型的示例翻译文本\n参考链接: https://huggingface.co/Helsinki-NLP/opus-mt-en-zh\nhttps://huggingface.co/Helsinki-NLP/opus-mt-en-ro?text=My+name+is+Sarah+and+I+live+in+London\n\n![](https://img-blog.csdnimg.cn/3331f29f447a401aade229fb356a9708.png)"},{"cell_type":"markdown","metadata":{"id":"wks6Nz8Z-3Qf","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"下面内容我们主要讲如何使用Datasets来加载翻译数据集，以及使用Trainer API 实现模型微调。\n\n\n只要该模型在 Transformers 库中具有sequence-to-sequence版本，那么我们下面的方法也可以加载各种类型的模型。 在这里，我们选择了 Helsinki-NLP/opus-mt-en-ro模型权重。"},{"cell_type":"code","execution_count":4,"metadata":{"id":"MX7DsEmOx5Ud","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"outputs":[],"source":"model_checkpoint = \"Helsinki-NLP/opus-mt-en-ro\""},{"cell_type":"markdown","metadata":{"id":"Fu45aGHIAQqM","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"### 加载数据集\n\n我们将使用 [datasets](https://github.com/huggingface/datasets/tree/master/datasets/wmt16) 库下载数据并获取我们需要用于评估的指标（将我们的模型与基准）。 这可以通过函数 load_dataset 和 load_metric 轻松完成。 我们在这里使用 WMT 数据集的英语/中文的部分数据集.\n\n\n神经机器翻译领域国际上最常用的数据集是WMT，很多机器翻译任务基于这个数据集进行训练，Google的工程师们基于WMT16 en-de准备了一个脚本：wmt16_en_de.sh。这个脚本先下载数据，再使用Moses Tokenizer，清理训练数据，并使用BPE生成32,000个Subword的词汇表。可以使用梯子直接下载预处理后的文件：\nhttps://github.com/huggingface/datasets/tree/master/datasets/wmt16"},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424,"referenced_widgets":["34fd15427c7d4f4189c9ce38924107b5","04672fbdc8754babbb7bb3bfb7bcfd2e","53ec358cc4784c25a50e17bd8764fd56","c916612f156b457f828410fdc85c8def","de1e4b0f94f74b4380de2cb4447352f1","c04bdf05ef684c8fb12ea45833394e77","ed5983a37cba4faea852cf2e13b355b4","f8f12dcd6d164513bae3d4d7835cc45e","adbad0b85f78492583b2c5b291341030","a9aa1538562840f2b969340f1cbf8bbb","5600aa1b802c46b183417910f0e18cc4","fd705f2d192c41f8bbbf5c6270e6ffb0","d8499a1a46144034b56489025aaee86f","b982f93475414c0c8a92601717138dc4","98b513fb020744bfbbe0678daa33f2aa","4295f3dc0a494198b776f682b327924d","17ecb55f59dd411084d8692b80cc8cad","a434f279213545da8a92da73bc320504","e1d75f47a76d4bf8b2941fe543e46242","0c2955e8cff44b5ba998ca584e485e54","eb10554a79a04ecf92517ba96ca956d9","ad65c758c1114adab13e6dbf29f660aa","e3ada02e2bf34b5699475b1eb7801282","9c9af690e08d490dbba5524e35fa6fcb","c5d0a0ee946745609e561ffece8cde76","9d13fcfc7b4e4ef182ef48df08253af4","88c725d411d34463b632ec85fa67ed4a","c4298a4942db4790a357a342c7cc13e7","4d9080a460f34c8abdb3af1fdf0eddce","a8744f13ff43450db14031275a7ba769","edf107491f434e7486765ebda2ad938e","922bfc020eb345c584ab31721a10922c","f9c351511a33475d993fee0d6c1cddba","4b5219e4a1a1400a982acc3065b46e2c"]},"id":"biPo8vFTx5Ue","outputId":"402f80b1-801c-4452-f502-dfcb9d9e9326","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Reusing dataset wmt16 (C:\\Users\\yanqiang\\.cache\\huggingface\\datasets\\wmt16\\ro-en\\1.0.0\\af3c5d746b307726d0de73ebe7f10545361b9cb6f75c83a1734c000e48b6264f)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a6bfb8323da847bfb366a9337b87b1a5","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":"from datasets import load_dataset, load_metric\nraw_datasets = load_dataset(\"wmt16\", \"ro-en\") # ['cs-en', 'de-en', 'fi-en', 'ro-en', 'ru-en', 'tr-en']\nmetric = load_metric(\"sacrebleu\")"},{"cell_type":"markdown","metadata":{"id":"-LqXNw9_BFQe","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"数据集对象为 [datasetdict](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict)，其中包含训练、验证和测试集的每一个键：train、validation、test"},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"379sQa4Ix5Uf","outputId":"8d3baded-0cec-4bbf-fdc4-cdcc7944ee9d","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['translation'],\n","        num_rows: 610320\n","    })\n","    validation: Dataset({\n","        features: ['translation'],\n","        num_rows: 1999\n","    })\n","    test: Dataset({\n","        features: ['translation'],\n","        num_rows: 1999\n","    })\n","})"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":"raw_datasets"},{"cell_type":"code","execution_count":36,"metadata":{"id":"ABF486E6DB4D464FA6A7995EA9934162","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"raw_datasets['train'].to_pandas().to_csv('data/09/train.csv',index=None)\nraw_datasets['validation'].to_pandas().to_csv('data/09/validation.csv',index=None)\nraw_datasets['test'].to_pandas().to_csv('data/09/test.csv',index=None)"},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nkiu4ITXx5Ug","outputId":"1eb6d334-c138-4ddb-c625-9a7f817fdb2a","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["{'translation': {'en': 'Membership of Parliament: see Minutes',\n","  'ro': 'Componenţa Parlamentului: a se vedea procesul-verbal'}}"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":"raw_datasets[\"train\"][0]"},{"cell_type":"markdown","metadata":{"id":"_6hsuFL1Bwa6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"为了大致了解数据的情况，我们用下面函数随机从数据集中选取一些样本"},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":345},"id":"1OE_CXoxx5Uh","outputId":"c25cfc10-df85-49b7-bd06-524ce871ba9d","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>translation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>{'en': 'It has 25 mayoral and 700 city-council candidates.', 'ro': 'Aceasta are 25 de candidaţi la primării şi 700 la consiliile locale.'}</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>{'en': 'This is the question which we will have to ask ourselves once the Commission has finished its analysis.', 'ro': 'Aceasta este întrebarea pe care trebuie să ne-o punem imediat ce Comisia va fi terminat analiza sa.'}</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>{'en': 'Sekulovski: To our great surprise and joy, the second awarded work -- the Museum of Memory and Tolerance in Mexico -- shares a similar story.', 'ro': 'Sekulovski: Spre marea noastră surpriză şi bucurie, a doua lucrare premiată -- Muzeul Memoriei şi Toleranţei din Mexic -- împărtăşeţte o poveste similară.'}</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>{'en': 'He also discussed NATO's continuing engagement in Kosovo, noting that the improving situation has allowed the Alliance to reduce its number of troops there. (Balkan Web, Shekulli, Alsat, Klan - 30/09/10)', 'ro': 'El a discutat despre menţinerea implicării NATO în Kosovo, menţionând că îmbunătăţirea situaţiei a permis Alianţei să îşi reducă numărul de trupe de acolo. (Balkan Web, Shekulli, Alsat, Klan - 30/09/10)'}</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>{'en': 'in writing. - I wish to express my regret in view of the imprisonment of Gilad Shalit.', 'ro': 'în scris. - Doresc să îmi exprim regretul cu privire la încarcerarea lui Gilad Shalit.'}</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":"import datasets\nimport random\nimport pandas as pd\nfrom IPython.display import display, HTML\ndef show_random_elements(dataset, num_examples=5):\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n    picks = []\n    for _ in range(num_examples):\n        pick = random.randint(0, len(dataset)-1)\n        while pick in picks:\n            pick = random.randint(0, len(dataset)-1)\n        picks.append(pick)\n    \n    df = pd.DataFrame(dataset[picks])\n    for column, typ in dataset.features.items():\n        if isinstance(typ, datasets.ClassLabel):\n            df[column] = df[column].transform(lambda i: typ.names[i])\n    display(HTML(df.to_html()))\nshow_random_elements(raw_datasets[\"train\"])"},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oAHbdQH5x5Ui","outputId":"7db0caa9-0fba-43dd-85c6-08d7eecd1c57","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n","Produces BLEU scores along with its sufficient statistics\n","from a source against one or more references.\n","\n","Args:\n","    predictions: The system stream (a sequence of segments).\n","    references: A list of one or more reference streams (each a sequence of segments).\n","    smooth_method: The smoothing method to use. (Default: 'exp').\n","    smooth_value: The smoothing value. Only valid for 'floor' and 'add-k'. (Defaults: floor: 0.1, add-k: 1).\n","    tokenize: Tokenization method to use for BLEU. If not provided, defaults to 'zh' for Chinese, 'ja-mecab' for\n","        Japanese and '13a' (mteval) otherwise.\n","    lowercase: Lowercase the data. If True, enables case-insensitivity. (Default: False).\n","    force: Insist that your tokenized input is actually detokenized.\n","\n","Returns:\n","    'score': BLEU score,\n","    'counts': Counts,\n","    'totals': Totals,\n","    'precisions': Precisions,\n","    'bp': Brevity penalty,\n","    'sys_len': predictions length,\n","    'ref_len': reference length,\n","\n","Examples:\n","\n","    >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n","    >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n","    >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n","    >>> results = sacrebleu.compute(predictions=predictions, references=references)\n","    >>> print(list(results.keys()))\n","    ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n","    >>> print(round(results[\"score\"], 1))\n","    100.0\n","\"\"\", stored examples: 0)"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":"metric"},{"cell_type":"markdown","metadata":{"id":"2C113F92541B4E32A321ABC07D91805F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"[SacreBLEU]指出传统计算BLEU的方式存在三个问题，其中最主要的问题是已有的计算方式需要用户自己提供tokenize过的结果，甚至还要提供tokenize过的参考译文，而不同人tokenize的方式不同，产生的结果就会不同（例如是不是会把复合词中间的连接符-分开，UNK如何计算等等）。理想的方式是用户提供detokenize后的结果，而且完全不碰参考译文，而中间的数据处理过程完全交由评估脚本自动处理。为此，作者提供了sacrebleu这个Python包，其不仅实现了上述功能，而且内置了WMT近几期比赛所有方向的参考译文下载，并保证自身输出的结果和WMT官方评测结果一致。因此，建议都使用SacreBLEU来评估模型输出\n\n- [神经翻译笔记5扩展c. 机器翻译系统的常见评价指标](https://zhuanlan.zhihu.com/p/258207437)\n- [A Call for Clarity in Reporting BLEU Scores](https://arxiv.org/pdf/1804.08771v2.pdf)\n- [sacreBLEU](https://github.com/mjpost/sacrebleu)"},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G24hShAwx5Uj","outputId":"163c8294-85e6-4fc5-ff66-3f6cc62affe5","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["{'score': 0.0,\n"," 'counts': [4, 2, 0, 0],\n"," 'totals': [4, 2, 0, 0],\n"," 'precisions': [100.0, 100.0, 0.0, 0.0],\n"," 'bp': 1.0,\n"," 'sys_len': 4,\n"," 'ref_len': 4}"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":"fake_preds = [\"hello there\", \"general kenobi\"]\nfake_labels = [[\"hello there\"], [\"general kenobi\"]]\nmetric.compute(predictions=fake_preds, references=fake_labels)"},{"cell_type":"markdown","metadata":{"id":"-6HmkA7uCWRa","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# 数据预处理\n\n在将这些文本输入模型之前，我们需要对它们进行预处理。我们使用的是 Transformers Tokenizer 来处理文本，它将对输入进行分词（包括将标记转换为它们在预训练词汇表中的相应 ID）并将转换为模型需要的输入格式\n\n下面我们使用 AutoTokenizer.from_pretrained 方法实例化我们的分词器，这将确保：\n\n- 我们得到一个与我们想要使用的模型架构相对应的分词器，\n- 我们下载预训练这个特定检查点时使用的词汇。\n- 该词汇表将被缓存，因此下次我们运行单元时不会再次下载它。\n\n如果想手动下载模型，可以指定本地模型路径"},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["db4d60783a2b4cc9ac647f53524e9201","23e6dd2b833d44aabc5b78a83d7ef136","8aff67fa78c640bf83598dd7fccc5b97","97ed312717014144af97d250fcd74d9d","7b69f1226cf5457aa164146a890a2c5e","d5ce787580584a40883fdac45df7917a","1962a04347894250993d2eb3d2946e5c","d5e1f379a1084c6e978a633698b4b02a","b689909adc244204af714b9a19e158c0","8c32a5f5cb724610a4ea8a5b42be5b28","44db6a1a111148ceaa804f7302b0629b","7e8253e0de7f426c9f9af4b15775c660","adfbc16f0b6144b6ab06544ed060f8fe","1425686f748e45b9921c501d0e5ce904","632fb54ffc9b44b98d082d097c17d7de"]},"id":"agDlgrOix5Uj","outputId":"d52e9acb-10d0-464f-c9f1-1947b7a7b0e6","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"Downloading:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58b0432b756443f984fbba8fe2cb0f06"}},"transient":{}},{"output_type":"display_data","metadata":{},"data":{"text/plain":"Downloading:   0%|          | 0.00/1.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acae422b5d09433983cad87e74f23b78"}},"transient":{}},{"output_type":"display_data","metadata":{},"data":{"text/plain":"Downloading:   0%|          | 0.00/770k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2999ee068bb9434ea5270ed66082a6f7"}},"transient":{}},{"output_type":"display_data","metadata":{},"data":{"text/plain":"Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d897331f86049d8aa7e86feca6ee3e1"}},"transient":{}},{"output_type":"display_data","metadata":{},"data":{"text/plain":"Downloading:   0%|          | 0.00/1.33M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86c130136039468ba82378f599ffc585"}},"transient":{}}],"source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"},{"metadata":{"id":"44EDF283022E4366833E33944DFDA3A3","notebookId":"625269a10b3035001750f356","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"PreTrainedTokenizer(name_or_path='Helsinki-NLP/opus-mt-en-ro', vocab_size=59543, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'})"},"transient":{}}],"source":"tokenizer","execution_count":6},{"metadata":{"id":"F4D0B08CE4104FC7A3469C683813B403","notebookId":"625269a10b3035001750f356","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"eos：end of sentence 句子的结束"},{"cell_type":"markdown","metadata":{"id":"zbih6yhKG9JB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"下面可以直接在一个句子或一对句子上调用分词方法："},{"cell_type":"code","execution_count":26,"metadata":{"id":"7964AA0D9FE84148909DBB9F157A8022","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [125, 778, 3, 63, 141, 9191, 23, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":"tokenizer(\"Hello, this one sentence!\")"},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j9XTBM27x5Uk","outputId":"2bd42d79-8437-41b1-e1b4-80723a18f614","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': [[125, 778, 3, 63, 141, 9191, 23, 0], [187, 32, 716, 9191, 2, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":"tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"},{"metadata":{"id":"71D94EE0C6E44823B043B1FC91592AFC","notebookId":"625269a10b3035001750f356","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['▁He', 'llo', ',', '▁this', '▁one', '▁sentence', '!', '</s>']"},"transient":{}}],"source":"tokenizer.convert_ids_to_tokens(tokenizer(\"Hello, this one sentence!\")['input_ids'])","execution_count":17},{"cell_type":"markdown","metadata":{"id":"Lf2HfvKdHCeS","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"为了给模型准备好翻译的targets，我们使用as_target_tokenizer来控制targets所对应的特殊token："},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qvYg6BXJx5Ul","outputId":"06d8f836-7a16-4fdc-a23e-9d0973135a55","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [[10334, 1204, 3, 15, 8915, 27, 452, 59, 29579, 581, 23, 0], [235, 1705, 11, 32, 8, 1205, 5305, 59, 29579, 581, 2, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"]}],"source":"with tokenizer.as_target_tokenizer():\n    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))"},{"cell_type":"code","execution_count":27,"metadata":{"id":"9C4B031F4AB84EE0866DF80120F3F1B2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': [10334, 1204, 3, 15, 8915, 27, 452, 59, 29579, 581, 23, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","tokens: ['▁Hel', 'lo', ',', '▁', 'this', '▁o', 'ne', '▁se', 'nten', 'ce', '!', '</s>']\n"]}],"source":"with tokenizer.as_target_tokenizer(): \n    print(tokenizer(\"Hello, this one sentence!\"))\n    model_input = tokenizer(\"Hello, this one sentence!\")\n    tokens = tokenizer.convert_ids_to_tokens(model_input['input_ids'])\n    # 打印看一下special toke\n    print('tokens: {}'.format(tokens))"},{"cell_type":"markdown","metadata":{"id":"AgjkiP5wHNc0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"下面是预处理文本的函数。 我们只需使用参数 truncation=True 将它们提供给标记器。 这将确保输入比所选模型可以处理的更长的输入将被截断为模型接受的最大长度。 稍后将处理填充（在数据整理器中），因此我们将示例填充到批处理中的最长长度，而不是整个数据集。"},{"cell_type":"code","execution_count":11,"metadata":{"id":"e1fnOpO6x5Um","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"outputs":[],"source":"prefix = \"\"\nmax_input_length = 128\nmax_target_length = 128\nsource_lang = \"en\"\ntarget_lang = \"ro\"\ndef preprocess_function(examples):\n    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n    # 输入做分词编码\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n    # Setup the tokenizer for targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs"},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9uXJ6SFex5Un","outputId":"b536ebfa-e98c-47d2-df9c-ae59689b91c2","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"outputs":[],"source":"preprocess_function(raw_datasets['train'][:2])"},{"metadata":{"id":"135403B2B5214C2BA450F79D7AC8C7A4","notebookId":"625269a10b3035001750f356","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"{'input_ids': [[393, 4462, 14, 1137, 53, 216, 28636, 0], [24385, 14, 28636, 14, 4646, 4622, 53, 216, 28636, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[42140, 494, 1750, 53, 8, 59, 903, 3543, 9, 15202, 0], [36199, 6612, 9, 15202, 122, 568, 35788, 21549, 53, 8, 59, 903, 3543, 9, 15202, 0]]}"},{"cell_type":"markdown","metadata":{"id":"h8ZDByENHTtO","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"要将这个函数应用于我们数据集中的所有句子对，我们只需使用我们之前创建的数据集对象的 map 方法。 这会将函数应用于数据集中所有拆分的所有元素，因此我们的训练、验证和测试数据将在一个命令中进行预处理"},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["f56b5b8abcfc415fbf46ca0f2fe619d0","ef09f1189d75431591bebf33d288dacc","12e50a27f9f24575bd86305f3b609095","24cc9a4f2e4a411296dd6822329e5815","f83d5ec459fe4dfd9ebf46fcca9895ee","382f2bc5fc944482861293a496fb03ed","330c9a9e97c9436f9d29bb12c586253a","af955c59afc54adc8f1072d371b688bc","5b4edc877e524610bb9ecc8103763250","d3190da3507f4ab4be03027179566508","7dac8e5e55ce4e638942c4299b377e3a","41467eb21e744e66a47790e274defb66","c0e50be8469a429fb8852fe3eb3a2ede"]},"id":"uZlsJFZnx5Uo","outputId":"e39c565b-e6cb-489b-a5ca-a565624b89cf","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4775406855e437f800edabe77d307e3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/611 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9758921f17a430689122da1cf100c7d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8b48cf36456f43a1807fdab554ad55cc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":"tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"},{"cell_type":"markdown","metadata":{"id":"b4kA3jpsHcFE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"## 微调模型"},{"cell_type":"markdown","metadata":{"id":"ooZLnaC1HhBq","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"现在我们的数据已经准备好了，我们可以下载预训练模型并对其进行微调。 由于我们的任务是序列到序列的类型，我们使用 AutoModelForSeq2SeqLM 类。 与分词器一样，from_pretrained 方法将为我们下载并缓存模型。"},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["001c2cf8d21d4703bc190149a82d06ab","c8c29e7a13804b6c9e97c3ff425bd81d","13c6c16b336c404fa84cba2ee42debde","31bbbf7e34a14335b628add663c71642","a23d83e9f0ea4ac8b57ec5f1614414dc","ae117dc6e1fb41eca964e34601c6aa6c","7616cd08bb494b8f988b9e893bb8bb25","064aa5db0924410bb17646220304faea","c0e574d69ffb426b87d3445d26e02926","c2c7fbedceba45aa822c96d7f1cb6134","48eb8b6feb8a47b79fbd924a7a5c6df1"]},"id":"IvrIjd-Lx5Uq","outputId":"07457f36-49c9-419d-b0cc-dbe8429c401a","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fdd77e39197e4c17bbb393daf51d18f5","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/287M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":"from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"},{"cell_type":"markdown","metadata":{"id":"jPagNnmWHoQW","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"要实例化 Seq2SeqTrainer，我们需要再定义三件事。 最重要的是 [Seq2SeqTrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments)，这是一个包含自定义训练的所有属性的类。 它需要一个文件夹名称，用于保存模型的检查点，所有其他参数都是可选的："},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qk2p1KEwx5Ur","outputId":"28118e90-17a6-460a-8694-3b50cc6660c5","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]}],"source":"batch_size = 16\nmodel_name = model_checkpoint.split(\"/\")[-1]\nargs = Seq2SeqTrainingArguments(\n    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    weight_decay=0.01,#优化器\n    save_total_limit=3, # \n    num_train_epochs=1,\n    predict_with_generate=True    \n)"},{"cell_type":"markdown","metadata":{"id":"t9UUwvXCH8Op","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"在这里，我们将评估设置为在每个 epoch 结束时进行，调整学习率，使用在单元格顶部定义的 batch_size 并自定义权重衰减。 由于 Seq2SeqTrainer 会定期保存模型并且我们的数据集非常大，我们告诉它最多保存 3 次。 最后，我们使用 predict_with_generate 选项（正确生成摘要）并激活混合精度训练（更快一点）。\n\n模型将保存在 **{model_name}-finetuned-{source_lang}-to-{target_lang}** 目录下\n\n然后，我们需要一种特殊的数据整理器，它不仅可以将输入填充到批处理中的最大长度，还可以填充标签："},{"cell_type":"code","execution_count":35,"metadata":{"id":"qV9wfuvZx5Us","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"},{"cell_type":"markdown","metadata":{"id":"Jw7by60eH_p5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"为我们的 Seq2SeqTrainer 定义的最后一件事是如何从预测中计算指标。 我们需要为此定义一个函数，它将仅使用我们之前加载的指标，并且我们必须进行一些预处理以将预测解码为文本："},{"cell_type":"code","execution_count":33,"metadata":{"id":"mHqp_FFhx5Uu","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"import numpy as np\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    # Some simple post-processing\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    result = {\"bleu\": result[\"score\"]}\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result"},{"cell_type":"markdown","metadata":{"id":"E_XK7px-IDkC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"然后我们只需要将所有这些连同我们的数据集一起传递给 Seq2SeqTrainer："},{"cell_type":"code","execution_count":37,"metadata":{"id":"c7zuF8rLx5Uy","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"trainer = Seq2SeqTrainer(\n    model,# 传入初始化的模型\n    args,# 传入参数\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n)"},{"cell_type":"markdown","metadata":{"id":"W1TskjmQIGKd","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"我们现在可以通过调用 train 方法来微调我们的模型："},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"sonIRAfDx5Uz","outputId":"8a0bc390-6900-424c-f92e-e3a841ead986","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"trainer.train()"},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-SSyPwox5U1","outputId":"c06f1ef4-8a5e-49a7-ef48-3c4997d3623f","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["opus-mt-en-ro-finetuned-en-to-ro\\checkpoint-500\\config.json\n","opus-mt-en-ro-finetuned-en-to-ro\\checkpoint-500\\optimizer.pt\n","opus-mt-en-ro-finetuned-en-to-ro\\checkpoint-500\\pytorch_model.bin\n","opus-mt-en-ro-finetuned-en-to-ro\\checkpoint-500\\rng_state.pth\n","opus-mt-en-ro-finetuned-en-to-ro\\checkpoint-500\\scheduler.pt\n","opus-mt-en-ro-finetuned-en-to-ro\\checkpoint-500\\source.spm\n","opus-mt-en-ro-finetuned-en-to-ro\\checkpoint-500\\special_tokens_map.json\n","opus-mt-en-ro-finetuned-en-to-ro\\checkpoint-500\\target.spm\n","opus-mt-en-ro-finetuned-en-to-ro\\checkpoint-500\\tokenizer_config.json\n","opus-mt-en-ro-finetuned-en-to-ro\\checkpoint-500\\trainer_state.json\n","opus-mt-en-ro-finetuned-en-to-ro\\checkpoint-500\\training_args.bin\n","opus-mt-en-ro-finetuned-en-to-ro\\checkpoint-500\\vocab.json\n","opus-mt-en-ro-finetuned-en-to-ro\\runs\\Apr10_12-34-33_DESKTOP-G5E8965\\events.out.tfevents.1649565367.DESKTOP-G5E8965.20652.0\n","opus-mt-en-ro-finetuned-en-to-ro\\runs\\Apr10_12-34-33_DESKTOP-G5E8965\\1649565367.2931008\\events.out.tfevents.1649565367.DESKTOP-G5E8965.20652.1\n"]}],"source":"import os\nfor dirname, _, filenames in os.walk('opus-mt-en-ro-finetuned-en-to-ro'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))"},{"metadata":{"id":"7C514A477C4F4EA78744A8BACAC26755","notebookId":"625269a10b3035001750f356","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"* training_args.bin：参数持久化文件\n* tokenizer_config.json：Helsinki-NLP/opus-mt-en-ro tokenizer\n* trainer_state.json：模型训练状态"},{"cell_type":"markdown","metadata":{"id":"oeLJF9PJINyR","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"我们的微调模型已经保存在 *opus-mt-en-ro-finetuned-en-to-ro/checkpoint-38000*\n\n加载模型并将一些文本从英语翻译成罗马尼亚语"},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kByQ_vxTx5U2","outputId":"c2fef526-afee-4309-9bb5-aef945b31b56","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Didn't find file opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500\\added_tokens.json. We won't load it.\n","loading file opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500\\source.spm\n","loading file opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500\\target.spm\n","loading file opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500\\vocab.json\n","loading file opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500\\tokenizer_config.json\n","loading file None\n","loading file opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500\\special_tokens_map.json\n"]},{"name":"stdout","output_type":"stream","text":["[]\n"]}],"source":"from transformers import MarianMTModel, MarianTokenizer\nsrc_text = ['My name is Sarah and I live in London']\nmodel_name = 'opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500'\ntokenizer = MarianTokenizer.from_pretrained(model_name)\nprint(tokenizer.supported_language_codes)"},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4h_jIFZgx5U2","outputId":"77d69535-6d1e-486b-df14-382718414698","tags":[],"jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500\\config.json\n","Model config MarianConfig {\n","  \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-ro\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"swish\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"MarianMTModel\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bad_words_ids\": [\n","    [\n","      59542\n","    ]\n","  ],\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 512,\n","  \"decoder_attention_heads\": 8,\n","  \"decoder_ffn_dim\": 2048,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 6,\n","  \"decoder_start_token_id\": 59542,\n","  \"dropout\": 0.1,\n","  \"encoder_attention_heads\": 8,\n","  \"encoder_ffn_dim\": 2048,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 6,\n","  \"eos_token_id\": 0,\n","  \"forced_eos_token_id\": 0,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_length\": 512,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"marian\",\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": false,\n","  \"num_beams\": 4,\n","  \"num_hidden_layers\": 6,\n","  \"pad_token_id\": 59542,\n","  \"scale_embedding\": true,\n","  \"static_position_embeddings\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.17.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 59543\n","}\n","\n","loading weights file opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500\\pytorch_model.bin\n","All model checkpoint weights were used when initializing MarianMTModel.\n","\n","All the weights of MarianMTModel were initialized from the model checkpoint at opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n"]},{"data":{"text/plain":["['Numele meu este Sarah şi locuiesc în Londra.']"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":"model = MarianMTModel.from_pretrained(model_name)\ntranslated = model.generate(**tokenizer(src_text, return_tensors=\"pt\", padding=True))\n[tokenizer.decode(t, skip_special_tokens=True) for t in translated]"},{"cell_type":"markdown","metadata":{"id":"vjn5FnzPI0cS","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"我们的微调模型比预训练模型做得好得多，并且接近谷歌翻译\n**input text** -> My name is Sarah and I live in London\n\n**pre-trained model prediction** -> Numele meu este Sarah şi locuiesc în Londra.\n\n**fine-tune model prediction** -> Numele meu este Sarah şi locuiesc la Londra\n\n**google translator prediction** -> Numele meu este Sarah şi locuiesc la Londra\n\n\n![](https://img-blog.csdnimg.cn/063a6de022c44c4ebb48c470dbcfaa2a.png)\n"},{"metadata":{"id":"03852A1E7F43413B9BFFD06FC0661916","notebookId":"625269a10b3035001750f356","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"### 微调一个英文到中文的翻译模型"},{"cell_type":"markdown","metadata":{"id":"2F98779AE31B4C8EB68EDC3092479E7F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"背景描述\n\n520万个中英文平行语料( 原始数据1.1G，压缩文件596M)\n\n数据说明\n中英文平行语料520万对。每一个对，包含一个英文和对应的中文。中文或英文，多数情况是一句带标点符号的完整的话。\n对于一个平行的中英文对，中文平均有36个字，英文平均有19个单词(单词如“she”)\n数据集划分：数据去重并划分。训练集：516万；验证集：3.9万。\n\n** 结构：**\n\n{\"english\": <english>, \"chinese\": <chinese>}\n\n其中，english是英文句子，chinese是中文句子，中英文一一对应。\n\n** 例子：**\n\n{\"english\": \"In Italy, there is no real public pressure for a new, fairer tax system.\", \"chinese\": \"在意大利，公众不会真的向政府施压，要求实行新的、更公平的税收制度。\"}\n\n\nhttps://www.heywhale.com/mw/dataset/5de5fcafca27f8002c4ca993/content"},{"metadata":{"id":"7346929E3B0D4FBB8EE62596525F666D","notebookId":"625269a10b3035001750f356","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","outputs":[],"source":"# 离线加载\nimport os\n\ndata_path = 'data/09' #数据路径/home/mw/input/task063578\ncache_dir = './data/cache'\ndata_files = {\n    'train': 'translation2019zh_train.json',# 数据集名称 训练集\n    'validation':  'translation2019zh_valid.json', # 验证集\n    # 'test': os.path.join(data_path, 'test.csv') # 测试集\n }\ndatasets = load_dataset(data_path, data_files=data_files, cache_dir=cache_dir)","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":4}