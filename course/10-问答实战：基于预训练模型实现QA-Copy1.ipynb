{"cells":[{"cell_type":"markdown","metadata":{"id":"478A039B9CBB430EB8FC6454B3CB4A82","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"## 自动问答\n自动问答(question answering, QA)最初的定义很简单, 是指对于人们用自然语言提出的问题, 计算机能够自动地给出答案或者答案列表。自动问答是计算机科学学科中信息检索与自然语言处理领域的一个重要研究方向, 主要任务是理解并自动回答用户提出的问题, 构建满足用户提出的检索、推理等需求的自动问答系统。\n\n自动问答不仅要满足用户对自然的信息交互的需求, 也要满足用户通过自动问答获取精确高效的信息的需求。自从深度学习得到广泛应用以来, 自动问答也随之得到了蓬勃的发展, 在各种智能产品上的应用也越来越广泛。\n\n与搜索引擎相比, 自动问答系统为用户提供的答案并非是简单检索排序的文档, 而是更具有语义内涵的自然语言表述。近年来, 随着人工智能的飞速发展, 自动问答已经成为倍受关注且发展前景广阔的研究方向。该研究涉及到信息检索、知识库、深度学习等领域, 对加深自然语言处理的理论与应用研究有着现实意义。"},{"cell_type":"markdown","metadata":{"id":"6238260C09214B54A8BE1269ACE32018","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"## 自动问答的类别"},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"5F3D39B851B341279AA4CDDCF4910561","jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"自动问答技术依据不同的特征, 可以划分为不同的类别。现有的一般分类依据包括数据来源、问答范围、会话管理方式等。\n\n### 依据数据来源区分\n\n根据目标数据源的不同, 已有自动问答技术大致可以分为三类:1)检索式问答; 2)社区问答; 3)知识库问答。\n\n- 检索式问答指利用信息检索领域的技术对问题进行分析、提取特征进行检索得到问题的答案。从特征抽取方式来划分, 可以将检索式问答系统大致划分为基于模式匹配的方法与基于统计文本抽取的方法。深度学习对检索式问答系统的发展有着一定帮助, 并主要应用于从文本中提取答案。\n\n- 社区问答指用户利用问答社区进行互动, 提出问题或回答问题的问答形式。问答社区是社区问答用户活跃的互联网社区。问答社区用户量巨大, 可提供许多问答数据, 这些问答数据覆盖了用户的各种信息需求。此外, 用户的历史行为信息对社区中的问答分析也有重要帮助。社区问答对用户的行为进行分析, 理解用户的行为模式, 为用户的查询提供高质量的回答。社区问答包含三个核心任务, 分别是专家推荐、相似问题检索与答案质量评估。\n\n\n- 知识库问答, 或称为面向知识库的问答, 是通过对自然语言问题进行语义理解与解析, 基于知识库进行知识提取获得答案。知识库是一组结构化数据, 包含多个三元组。知识库问答的中心问题是如何从知识库中获取与问题相关的信息进行回答。主要包括语义解析、信息抽取、向量建模等方法。\n\n### 依据问答范围区分\n\n根据回答范围的不同, 自动问答技术大致可以分为两类:1)开放域问答; 2)垂直域问答。\n\n- 开放域问答指问答不限定于一个特定领域, 可以基于任何领域进行提问回答的问答方式, 它可以使用任何形式的问答系统。由于知识库相关技术的快速发展, 当前的开放域问答以知识库问答为主。\n\n- 垂直域问答, 又叫做限定域问答, 表示问答的背景知识在某个限定领域上的问答。\n\n### 依据会话管理方式区分\n\n根据问答的会话管理方式来划分, 自动问答可以划分为两类:单轮问答和多轮问答(多轮对话)。\n\n- 用户的一次提问与问答系统的一次针对性的回答构成了一轮问答。单轮问答的每轮问答之间相互独立, 不存在关联, 多轮问答的每轮问答之间存在一定的关联性。\n- 多轮问答对问答上下文进行建模, 增加了问答上下文的管理模块, 因此具有更高的难度与研究价值。多轮问答是单轮问答的功能性扩展, 因此单轮问答的所有功能均包含于多轮问答中。"},{"cell_type":"markdown","metadata":{"id":"9257A3E71C7F44FD9020A9BEDD033293","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"## 基于深度学习的问答系统\n\n基于相似性匹配的深度学习方法是深度学习问答系统的一个重要方法。它的核心思想是从观测数据中学习问题与知识的语义表示, 使得正确的回答依据在学到的向量空间中是和问题的最接近的向量。\n\n无论是检索式问答、社区问答还是知识库问答, 深度学习都难以应用到对数据来源的检索中, 因此深度学习问答系统主要研究的方向是, 在提供了数据来源的基础上, 使用深度学习来对用户的提问进行回答。自动问答的这个子任务被称为阅读理解。阅读理解对问答的数据来源要求不高, 既可以使用用户提供的数据进行回答, 也可以与外部知识相结合共同进行回答。然而阅读理解对数据分析与推理功能的要求较高, 因此传统的机器学习方法难以取得优良的结果, 业界普遍采用深度学习方法进行阅读理解。\n\n阅读理解的一般任务形式为:给定一段或多段文本作为数据来源, 输入问题, 经过处理后输出一段文本作为问题的答案。依据答案是否直接来自文本, 可将阅读理解划分为抽取式阅读理解与生成式阅读理解。\n    \n    \n自动问答 (Question Answering, QA) 是经典的 NLP 任务，需要模型基于给定的上下文回答问题，根据产生回答方式的不同可以分为：\n\n- 抽取式 (extractive) 问答：从上下文中截取片段作为回答，类似于前面介绍的序列标注任务；\n- 生成式 (generative) 问答：生成一个文本片段作为回答，类似于翻译和摘要任务。\n\n抽取式问答模型通常采用纯 Encoder 框架（例如 BERT），它更适用于处理事实性问题，例如“谁发明了 Transformer 架构？”；而生成式问答模型则通常采用 Encoder-Decoder 框架（例如 T5、BART），它更适用于处理开放式问题，例如“天空为什么是蓝色的？”。\n\n本文我们将微调一个 BERT 模型来完成抽取式问答任务：对于每一个问题，从给定的上下文中抽取出概率最大的文本片段作为答案。"},{"cell_type":"markdown","metadata":{"id":"CD0CC8671FAE44AB9A1BF8DD4D127B4A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"## 基于transformers实现抽取式问答\n\n我们可以参照Huggingface课程的例子[Question answering](https://huggingface.co/course/chapter7/7?fw=pt)实现一个中文抽取式问答模型，具体步骤如下"},{"cell_type":"markdown","metadata":{"id":"52B65DFF71734468A43153E3EA0027D0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"### 数据集 CMRC2018介绍\n\n本目录包含[第二届“讯飞杯”中文机器阅读理解评测（CMRC 2018）](https://hfl-rc.github.io/cmrc2018/)所使用的数据。本数据集已被计算语言学顶级国际会议[EMNLP 2019](http://emnlp-ijcnlp2019.org)录用。\n\n**Title: A Span-Extraction Dataset for Chinese Machine Reading Comprehension**    \nAuthors: Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, Guoping Hu   \nLink: [https://www.aclweb.org/anthology/D19-1600/](https://www.aclweb.org/anthology/D19-1600/)  \nVenue: EMNLP-IJCNLP 2019\n\n- 开放式挑战排行榜 (new!)\n\n想了解在CMRC 2018数据上表现最好的模型吗？请查阅排行榜。\n[https://ymcui.github.io/cmrc2018/](https://ymcui.github.io/cmrc2018/)\n\n-  CMRC 2018 公开数据集\n\n请通过CodaLab Worksheet下载CMRC 2018公开数据集（训练集，开发集）。\n[https://worksheets.codalab.org/worksheets/0x92a80d2fab4b4f79a2b4064f7ddca9ce](https://worksheets.codalab.org/worksheets/0x92a80d2fab4b4f79a2b4064f7ddca9ce)\n\n- 提交方法\n\n如果你想要在**隐藏的测试集、挑战集上测试你的模型**，请通过以下步骤提交你的模型。\n[https://worksheets.codalab.org/worksheets/0x96f61ee5e9914aee8b54bd11e66ec647/](https://worksheets.codalab.org/worksheets/0x96f61ee5e9914aee8b54bd11e66ec647/)\n\n**需要注意的是，[CLUE](https://github.com/CLUEbenchmark/CLUE)上提供的测试集仅是CMRC 2018的部分子集。正式评测仍需通过上述方法得到完整测试集、挑战集上的结果。**\n\n\n- 通过🤗datasets快速加载\n\n你可以通过[HuggingFace `datasets` library](https://github.com/huggingface/datasets)工具包快速加载数据集：\n\n```python\n!pip install datasets\nfrom datasets import load_dataset\ndataset = load_dataset('cmrc2018')\n```\n关于`datasets`工具包的更多选项和使用细节可以通过这里访问了解：https://github.com/huggingface/datasets\n\n\n数据集分为训练集、验证集和测试集。"},{"cell_type":"markdown","metadata":{"id":"19CACC298FB248148F0453B9093AC2FF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"样本具体内容如下：\n```\n{\n      \"paragraphs\": [\n        {\n          \"id\": \"DEV_1096\", \n          \"context\": \"王天成，中国湖南省双牌县人，著名异议人士、政治学者。1989年毕业于北京大学法律系（现北京大学法学院），获硕士学位，同年留校任助教，兼任北大《中外法学》杂志编辑，旋即晋升为讲师。因参与创建“中国自由民主党”，担任该党宣传部长，王天成1992年10月被捕，1994年被北京市中级人民法院判处有期徒刑5年，剥夺政治权利2年，1997年10月刑满释放。王天成1999年所著文章《论共和国》、《再论共和国》，是中国学者较早系统阐述共和主义的作品，流传甚广，已成为汉语界在该领域的经典之作。王另与人合译有埃德蒙·柏克《自由与传统》、阿克顿勋爵《自由的历史》、路易斯·博洛尔《政治的罪恶》等西方学术名著。2005年11月，王天成发表文章指控武汉大学教授、博士生导师周叶中所著《共和主义之宪政解读》一书（2005年11月人民出版社出版），严重抄袭了其《论共和国》、《再论共和国》。周叶中是中国宪法学研究会副会长，2002年曾到中南海给胡锦涛温家宝等中共领导人讲课。该事件轰动一时，但法院判决周叶中没有抄袭，判决公布后备受非议，成为学界一大公案。王天成2008年赴美，先后在哥伦比亚大学、西北大学、纽约大学从事民主转型、制度设计研究。期间所完成的著作《大转型：中国民主化战略研究框架》（连载于《中国人权双周刊》，完整版2012年在香港出版），在大量民主转型案例研究的基础之上，系统探讨了中国民主转型路径与制度选择关键问题，被一些评论家认为是90年代初至今20余年中国政治学领域最重要的著作。2010年3月代表中国自由民主党与王有才发表联合公告，宣布中国自由民主党与中国民主党共同组建中国民主党全国委员会，4月当选中国民主党全国委员会执行长。2013年1月辞职，回归学术研究。2013年6月天安门民主大学开学，王天成任教务长。\", \n          \"qas\": [\n            {\n              \"question\": \"王天成的籍贯是哪里？\", \n              \"id\": \"DEV_1096_QUERY_0\", \n              \"answers\": [\n                {\n                  \"text\": \"中国湖南省双牌县\", \n                  \"answer_start\": 4\n                }, \n                {\n                  \"text\": \"中国湖南省双牌县\", \n                  \"answer_start\": 4\n                }, \n                {\n                  \"text\": \"湖南省双牌县\", \n                  \"answer_start\": 6\n                }\n              ]\n            }, \n            {\n              \"question\": \"王天成哪一年从北京大学法律系毕业？\", \n              \"id\": \"DEV_1096_QUERY_1\", \n              \"answers\": [\n                {\n                  \"text\": \"1989年\", \n                  \"answer_start\": 26\n                }, \n                {\n                  \"text\": \"1989年\", \n                  \"answer_start\": 26\n                }, \n                {\n                  \"text\": \"1989年\", \n                  \"answer_start\": 26\n                }\n              ]\n            }, \n            {\n              \"question\": \"王天成兼任了哪个杂志的编辑？\", \n              \"id\": \"DEV_1096_QUERY_2\", \n              \"answers\": [\n                {\n                  \"text\": \"《中外法学》\", \n                  \"answer_start\": 70\n                }, \n                {\n                  \"text\": \"《中外法学》\", \n                  \"answer_start\": 70\n                }, \n                {\n                  \"text\": \"《中外法学》\", \n                  \"answer_start\": 70\n                }\n              ]\n            },\n            ...\n          ]\n        }\n      ], \n      \"id\": \"DEV_1096\", \n      \"title\": \"王天成\"\n    }, \n```"},{"cell_type":"markdown","metadata":{"id":"24CD697300824C09965A563EBE4C2ED0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"其中一个问题可能对应多个答案，也就是答案会反复的出现在原文中，在训练的时候每个问题只有一个答案，验证与测试的时候，只需要判断预测答案与所有参考答案一起评估"},{"cell_type":"markdown","metadata":{"id":"FE29FB18000E407DB9C3877603452BD5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"### 创建Dataset"},{"cell_type":"markdown","metadata":{"id":"71E384F9CFE048BFBB2C4673E28C2211","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"source":"和之前任务一样，我们需要编一个自定义数据集CMRC2018，然后用来加载数据并进行处理。主要是需要question和answer，以及答案的开始位置等："},{"cell_type":"code","execution_count":1,"metadata":{"id":"E2F9C7FE748B4D4EAF4E6B9F138F7A19","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"outputs":[],"source":"from torch.utils.data import Dataset\nimport json"},{"cell_type":"code","execution_count":4,"metadata":{"id":"096DE56677ED47D7AA3E38B29D5BDEFD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"outputs":[],"source":"class CMRCDataset(Dataset):\n    def __init__(self, data_file):\n        self.data = self.load_data(data_file)\n    \n    def load_data(self, data_file):\n        Data = {}\n        with open(data_file, 'r', encoding='utf-8') as f:\n            json_data = json.load(f)\n            idx = 0\n            for article in json_data['data']:\n                title = article['title']\n                context = article['paragraphs'][0]['context']\n                for question in article['paragraphs'][0]['qas']:\n                    q_id = question['id']\n                    ques = question['question']\n                    text = [ans['text'] for ans in question['answers']]\n                    answer_start = [ans['answer_start'] for ans in question['answers']]\n                    Data[idx] = {\n                        'id': q_id,\n                        'title': title,\n                        'context': context, \n                        'question': ques,\n                        'answers': {\n                            'text': text,\n                            'answer_start': answer_start\n                        }\n                    }\n                    idx += 1\n        return Data\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# train_data = CMRCDataset('data/10/cmrc2018_train.json')\n# valid_data = CMRCDataset('data/10/cmrc2018_dev.json')\n# test_data = CMRCDataset('data/10/cmrc2018_trial.json')\n\ntrain_data = CMRCDataset('/home/mw/input/task109875/cmrc2018_train.json')\nvalid_data = CMRCDataset('/home/mw/input/task109875/cmrc2018_dev.json')\ntest_data = CMRCDataset('/home/mw/input/task109875/cmrc2018_trial.json')"},{"cell_type":"code","execution_count":5,"metadata":{"id":"645474D58EE2465A9831B27ACF3C7F11","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"{'id': 'TRAIN_186_QUERY_0',\n 'title': '范廷颂',\n 'context': '范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。',\n 'question': '范廷颂是什么时候被任为主教的？',\n 'answers': {'text': ['1963年'], 'answer_start': [30]}}"},"transient":{}}],"source":"train_data[0]"},{"cell_type":"markdown","metadata":{"id":"2613635AB9064EEF8D4294E886952FBF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"我们可以看下数据集的大小："},{"cell_type":"code","execution_count":6,"metadata":{"id":"8794D7BD9DF04FB2B7799ACA9D3665E9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"train_data lengths 10142\nvalid_data lengths 3219\ntest_data lengths 1002\n","name":"stdout"}],"source":"print(\"train_data lengths\",len(train_data))\nprint(\"valid_data lengths\",len(valid_data))\nprint(\"test_data lengths\",len(test_data))"},{"metadata":{"id":"695F8ACBC9AD4468B6581EABE586D62D","notebookId":"62556e9030fa900018de3878","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nCollecting transformers\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8f/e9/c2b4c823b3959d475a570c1bd2df4125478e2e37b96fb967a87933ae7134/transformers-4.18.0-py3-none-any.whl (4.0 MB)\n\u001b[K     |████████████████████████████████| 4.0 MB 1.4 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.26.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.49.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers) (1.7.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2019.6.8)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.3.1)\nCollecting huggingface-hub<1.0,>=0.1.0\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c8/df/1b454741459f6ce75f86534bdad42ca17291b14a83066695f7d2c676e16c/huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n\u001b[K     |████████████████████████████████| 67 kB 1.6 MB/s eta 0:00:011\n\u001b[?25hCollecting numpy>=1.17\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/14/32/d3fa649ad7ec0b82737b92fefd3c4dd376b0bb23730715124569f38f3a08/numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[K     |████████████████████████████████| 14.8 MB 1.7 MB/s eta 0:00:01\n\u001b[?25hCollecting packaging>=20.0\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/05/8e/8de486cbd03baba4deef4142bd643a3e7bbe954a784dc1bb17142572d127/packaging-21.3-py3-none-any.whl (40 kB)\n\u001b[K     |████████████████████████████████| 40 kB 1.0 MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.1.10)\nCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/db/07/5125318be83d03c311125412da13988e422b0c0255c08621637614c40b86/tokenizers-0.12.0-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n\u001b[K     |████████████████████████████████| 6.6 MB 1.3 MB/s eta 0:00:01\n\u001b[?25hCollecting typing-extensions>=3.7.4.3\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/45/6b/44f7f8f1e110027cf88956b59f2fad776cca7e1704396d043f89effd3a0e/typing_extensions-4.1.1-py3-none-any.whl (26 kB)\nCollecting dataclasses\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl (19 kB)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers) (3.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.26.6)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2021.5.30)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.2)\nCollecting sacremoses\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/db/8b/37b90a3848ff71c0d05ebac5ee6d83f1f81e5f57f26b99a83ebff033303b/sacremoses-0.0.49-py3-none-any.whl (895 kB)\n\u001b[K     |████████████████████████████████| 895 kB 1.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (0.13.2)\nInstalling collected packages: typing-extensions, packaging, tokenizers, sacremoses, numpy, huggingface-hub, dataclasses, transformers\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 3.7.4\n    Uninstalling typing-extensions-3.7.4:\n      Successfully uninstalled typing-extensions-3.7.4\n  Attempting uninstall: packaging\n    Found existing installation: packaging 19.0\n    Uninstalling packaging-19.0:\n      Successfully uninstalled packaging-19.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.16.3\n    Uninstalling numpy-1.16.3:\n      Successfully uninstalled numpy-1.16.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nspacy 2.1.4 requires jsonschema<3.1.0,>=2.6.0, but you have jsonschema 3.2.0 which is incompatible.\npaddlepaddle 1.5.0 requires matplotlib<=2.2.4, but you have matplotlib 3.1.1 which is incompatible.\npaddlepaddle 1.5.0 requires nltk<=3.4,>=3.2.2, but you have nltk 3.4.1 which is incompatible.\nmxnet 1.4.1 requires numpy<1.15.0,>=1.8.2, but you have numpy 1.19.5 which is incompatible.\nauto-sklearn 0.5.2 requires scikit-learn<0.20,>=0.19, but you have scikit-learn 0.21.1 which is incompatible.\u001b[0m\nSuccessfully installed dataclasses-0.8 huggingface-hub-0.4.0 numpy-1.19.5 packaging-21.3 sacremoses-0.0.49 tokenizers-0.12.0 transformers-4.18.0 typing-extensions-4.1.1\n\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.3.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}],"source":"!pip install transformers -i https://pypi.tuna.tsinghua.edu.cn/simple","execution_count":9},{"cell_type":"markdown","metadata":{"id":"1EFEA05E46DC48ADB661DA9E1CEA2942","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"### 数据预处理"},{"cell_type":"code","execution_count":10,"metadata":{"id":"DC9EB8233A5E4E588F8FA1A9E31E96A3","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=19.0, style=ProgressStyle(description_w…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"597658cb337b4f059ad15b707a00b8bc"}},"transient":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","metadata":{},"data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=647.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44ea502de5b24722a272c4d9125fc70e"}},"transient":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","metadata":{},"data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=109540.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"437a4911351d42cdab7d5a7c1b87d6f7"}},"transient":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","metadata":{},"data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=268961.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcbb8d601eb74cf6bde85762ff9eee51"}},"transient":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","metadata":{},"data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2.0, style=ProgressStyle(description_wi…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ed0f7d00a924318907449dfea7dd379"}},"transient":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","metadata":{},"data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1b6525ad3394c9ebd00d8799f31bfd1"}},"transient":{}},{"output_type":"stream","text":"\n","name":"stdout"}],"source":"from transformers import AutoTokenizer\n\n# model_checkpoint = '../pretrained_models/chinese-roberta-wwm-ext'\nmodel_checkpoint = 'hfl/chinese-bert-wwm-ext'\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"},{"cell_type":"code","execution_count":11,"metadata":{"id":"A7404840F91B4DC29EA99A1A28AFAC5E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"PreTrainedTokenizerFast(name_or_path='hfl/chinese-bert-wwm-ext', vocab_size=21128, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"},"transient":{}}],"source":"tokenizer"},{"metadata":{"id":"4412B53EFF3345E8BEA5D4249A6B632B","notebookId":"62556e9030fa900018de3878","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"'范廷颂是什么时候被任为主教的？'"},"transient":{}}],"source":"text=train_data[0][\"question\"]\ntext","execution_count":19},{"metadata":{"id":"C87B792CB551480193CFFD359932633A","notebookId":"62556e9030fa900018de3878","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"{'input_ids': [101, 5745, 2455, 7563, 3221, 784, 720, 3198, 952, 6158, 818, 711, 712, 3136, 4638, 8043, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (8, 9), (9, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 15), (0, 0)]}\n","name":"stdout"}],"source":"inputs = tokenizer(\n    text,\n    return_offsets_mapping=True\n)\nprint(inputs)","execution_count":22},{"metadata":{"id":"30E34C48A8DF456186F2040A0909E3F4","notebookId":"62556e9030fa900018de3878","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"'[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP]'"},"transient":{}}],"source":"tokenizer.decode(inputs['input_ids'])","execution_count":23},{"metadata":{"id":"D9DAF02AC1C241418FD789D099EC1A4D","notebookId":"62556e9030fa900018de3878","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['[CLS]',\n '范',\n '廷',\n '颂',\n '是',\n '什',\n '么',\n '时',\n '候',\n '被',\n '任',\n '为',\n '主',\n '教',\n '的',\n '？',\n '[SEP]']"},"transient":{}}],"source":"decode_words=[tokenizer.decode(input_id) for input_id in inputs['input_ids']]\ndecode_words","execution_count":27},{"metadata":{"id":"9B891064214444AE83DD13907C14EA5A","notebookId":"62556e9030fa900018de3878","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['[CLS]']"},"transient":{}}],"source":"start_index=inputs['offset_mapping'][0][0]\nend_index=inputs['offset_mapping'][0][1]+1\ndecode_words[start_index:end_index]","execution_count":33},{"metadata":{"id":"B4DEC3F645FA473E88629789BAA039F9","notebookId":"62556e9030fa900018de3878","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['廷']"},"transient":{}}],"source":"start_index=inputs['offset_mapping'][2][0]+1\nend_index=inputs['offset_mapping'][2][1]+1\ndecode_words[start_index:end_index]","execution_count":36},{"cell_type":"markdown","metadata":{"id":"1F5A306345D74CD89778C4B44E1E3C2B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"选取第一个训练集样本，对上下文context和question进行分词，"},{"cell_type":"code","execution_count":12,"metadata":{"id":"C7A482C6F30B41D990205E1A2698800B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"{'input_ids': [[101, 5745, 2455, 7563, 3221, 784, 720, 3198, 952, 6158, 818, 711, 712, 3136, 4638, 8043, 102, 5745, 2455, 7563, 3364, 3322, 8020, 8024, 8021, 8024, 1760, 1399, 924, 4882, 185, 5735, 4449, 8020, 8021, 8024, 3221, 6632, 1298, 5384, 7716, 1921, 712, 3136, 3364, 3322, 511, 9155, 2399, 6158, 818, 711, 712, 3136, 8039, 8431, 2399, 6158, 3091, 1285, 711, 1921, 712, 3136, 3777, 1079, 2600, 3136, 1277, 2134, 2429, 5392, 4415, 8039, 8447, 2399, 6158, 3091, 1285, 711, 2600, 712, 3136, 8024, 1398, 2399, 2399, 2419, 6158, 3091, 1285, 711, 3364, 3322, 8039, 8170, 2399, 123, 3299, 4895, 686, 511, 5745, 2455, 7563, 754, 9915, 2399, 127, 3299, 8115, 3189, 1762, 6632, 1298, 2123, 2398, 4689, 1921, 712, 3136, 1355, 5683, 3136, 1277, 1139, 4495, 8039, 4997, 2399, 3198, 2970, 1358, 5679, 1962, 3136, 5509, 1400, 8024, 6158, 671, 855, 6632, 1298, 4868, 4266, 2372, 1168, 3777, 1079, 5326, 5330, 1071, 2110, 689, 511, 5745, 2455, 7563, 754, 9211, 2399, 1762, 3777, 1079, 1920, 934, 6887, 7368, 2130, 2768, 4868, 2110, 2110, 689, 511, 5745, 2455, 7563, 754, 8594, 2399, 127, 3299, 127, 3189, 1762, 3777, 1079, 4638, 712, 3136, 2429, 1828, 3232, 7195, 8039, 1350, 1400, 6158, 3836, 1168, 1760, 1957, 2207, 2548, 1065, 2109, 1036, 7368, 3302, 1218, 511, 8707, 2399, 807, 8024, 5745, 2455, 7563, 1762, 3777, 1079, 1828, 1277, 1158, 2456, 4919, 3696, 2970, 2521, 704, 2552, 809, 3119, 2159, 1168, 3777, 1079, 6912, 2773, 4638, 7410, 3696, 511, 9258, 2399, 8024, 3791, 6632, 2773, 751, 5310, 3338, 8024, 6632, 1298, 3696, 712, 1066, 1469, 1744, 2456, 6963, 3777, 1079, 8024, 2496, 3198, 2523, 1914, 1921, 712, 3136, 4868, 5466, 782, 1447, 6845, 5635, 6632, 1298, 4638, 1298, 3175, 8024, 852, 5745, 2455, 7563, 793, 4197, 4522, 1762, 3777, 1079, 511, 5422, 2399, 102], [101, 5745, 2455, 7563, 3221, 784, 720, 3198, 952, 6158, 818, 711, 712, 3136, 4638, 8043, 102, 6632, 2773, 751, 5310, 3338, 8024, 6632, 1298, 3696, 712, 1066, 1469, 1744, 2456, 6963, 3777, 1079, 8024, 2496, 3198, 2523, 1914, 1921, 712, 3136, 4868, 5466, 782, 1447, 6845, 5635, 6632, 1298, 4638, 1298, 3175, 8024, 852, 5745, 2455, 7563, 793, 4197, 4522, 1762, 3777, 1079, 511, 5422, 2399, 5052, 4415, 1760, 5735, 3307, 2207, 934, 7368, 8039, 2668, 1762, 8779, 2399, 1728, 2932, 1310, 934, 7368, 4638, 5632, 4507, 510, 5632, 3780, 1350, 2867, 5318, 3124, 2424, 1762, 934, 7368, 6392, 3124, 3780, 6440, 4638, 6206, 3724, 5445, 6158, 2936, 511, 9155, 2399, 125, 3299, 126, 3189, 8024, 3136, 2134, 818, 1462, 5745, 2455, 7563, 711, 1921, 712, 3136, 1266, 2123, 3136, 1277, 712, 3136, 8024, 1398, 2399, 129, 3299, 8115, 3189, 2218, 818, 8039, 1071, 4288, 7208, 711, 519, 2769, 928, 1921, 712, 4638, 4263, 520, 511, 4507, 754, 5745, 2455, 7563, 6158, 6632, 1298, 3124, 2424, 6763, 4881, 2345, 679, 1914, 8114, 2399, 8024, 1728, 3634, 800, 3187, 3791, 1168, 2792, 2247, 1828, 1277, 6822, 6121, 4288, 4130, 2339, 868, 5445, 683, 3800, 4777, 6438, 5023, 2339, 868, 511, 5745, 2455, 7563, 7370, 749, 7481, 2190, 2773, 751, 510, 6577, 1737, 510, 6158, 2496, 2229, 6833, 2154, 1921, 712, 3136, 833, 5023, 7309, 7579, 1912, 8024, 738, 4908, 2166, 2612, 1908, 934, 7368, 510, 1158, 2456, 1957, 934, 833, 1730, 860, 5023, 511, 8431, 2399, 8024, 3136, 2134, 5735, 3307, 924, 4882, 753, 686, 1762, 1398, 2399, 127, 3299, 8123, 3189, 3091, 1285, 5745, 2455, 7563, 711, 1921, 712, 3136, 3777, 1079, 2600, 3136, 1277, 2134, 2429, 5392, 4415, 809, 1856, 6133, 6421, 3136, 1277, 2600, 712, 3136, 4638, 4958, 5375, 511, 8447, 2399, 124, 3299, 8133, 3189, 102], [101, 5745, 2455, 7563, 3221, 784, 720, 3198, 952, 6158, 818, 711, 712, 3136, 4638, 8043, 102, 5735, 3307, 924, 4882, 753, 686, 1762, 1398, 2399, 127, 3299, 8123, 3189, 3091, 1285, 5745, 2455, 7563, 711, 1921, 712, 3136, 3777, 1079, 2600, 3136, 1277, 2134, 2429, 5392, 4415, 809, 1856, 6133, 6421, 3136, 1277, 2600, 712, 3136, 4638, 4958, 5375, 511, 8447, 2399, 124, 3299, 8133, 3189, 8024, 5745, 2455, 7563, 6158, 3136, 2134, 5735, 3307, 924, 4882, 753, 686, 3091, 1285, 711, 1921, 712, 3136, 3777, 1079, 2600, 3136, 1277, 2600, 712, 3136, 2400, 1076, 1921, 712, 3136, 6446, 2255, 3136, 1277, 2134, 2429, 5392, 4415, 8039, 1398, 2399, 8111, 3299, 8153, 3189, 8024, 5735, 3307, 924, 4882, 753, 686, 3091, 1285, 5745, 2455, 7563, 711, 3364, 3322, 511, 5745, 2455, 7563, 1762, 8396, 2399, 5635, 8285, 2399, 3309, 7313, 1139, 818, 1921, 712, 3136, 6632, 1298, 712, 3136, 1730, 712, 2375, 511, 8263, 2399, 125, 3299, 8153, 3189, 8024, 3136, 2134, 5735, 3307, 924, 4882, 753, 686, 818, 1462, 1921, 712, 3136, 6446, 2255, 3136, 1277, 1076, 1921, 712, 3136, 7770, 2398, 3136, 1277, 1426, 1045, 3345, 712, 3136, 711, 1921, 712, 3136, 3777, 1079, 2600, 3136, 1277, 5392, 4415, 712, 3136, 8039, 1350, 5635, 8232, 2399, 123, 3299, 8131, 3189, 8024, 5745, 2455, 7563, 1728, 5815, 2821, 6791, 1343, 2600, 712, 3136, 5466, 1218, 5445, 5783, 828, 8039, 1426, 1045, 3345, 1398, 3189, 4696, 7370, 1921, 712, 3136, 3777, 1079, 2600, 3136, 1277, 2600, 712, 3136, 5466, 1218, 511, 5745, 2455, 7563, 754, 8170, 2399, 123, 3299, 8130, 3189, 3926, 3247, 1762, 3777, 1079, 4895, 686, 8024, 775, 2399, 8426, 2259, 8039, 1071, 5873, 4851, 754, 1398, 3299, 8153, 3189, 677, 1286, 1762, 1921, 712, 3136, 3777, 1079, 2600, 3136, 1277, 2600, 712, 3136, 2429, 1828, 102], [101, 5745, 2455, 7563, 3221, 784, 720, 3198, 952, 6158, 818, 711, 712, 3136, 4638, 8043, 102, 5466, 1218, 511, 5745, 2455, 7563, 754, 8170, 2399, 123, 3299, 8130, 3189, 3926, 3247, 1762, 3777, 1079, 4895, 686, 8024, 775, 2399, 8426, 2259, 8039, 1071, 5873, 4851, 754, 1398, 3299, 8153, 3189, 677, 1286, 1762, 1921, 712, 3136, 3777, 1079, 2600, 3136, 1277, 2600, 712, 3136, 2429, 1828, 715, 6121, 511, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 0]}\n","name":"stdout"}],"source":"context = train_data[0][\"context\"] # \nquestion = train_data[0][\"question\"]\n\ninputs = tokenizer(\n    question,\n    context,\n    max_length=300,# 最大长度\n    truncation=\"only_second\",# 仅对第二个输入进行截断\n    stride=50,# 滑动窗口大小为50\n    return_overflowing_tokens=True,#设定分词器支持返回重叠 token。\n)\nprint(inputs)"},{"metadata":{"id":"2F83F136CFF84C53851E15DABB3D020B","notebookId":"62556e9030fa900018de3878","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"'范廷颂枢机（，），圣名保禄·若瑟（），是越南罗马天主教枢机。1963年被任为主教；1990年被擢升为天主教河内总教区宗座署理；1994年被擢升为总主教，同年年底被擢升为枢机；2009年2月离世。范廷颂于1919年6月15日在越南宁平省天主教发艳教区出生；童年时接受良好教育后，被一位越南神父带到河内继续其学业。范廷颂于1940年在河内大修道院完成神学学业。范廷颂于1949年6月6日在河内的主教座堂晋铎；及后被派到圣女小德兰孤儿院服务。1950年代，范廷颂在河内堂区创建移民接待中心以收容到河内避战的难民。1954年，法越战争结束，越南民主共和国建都河内，当时很多天主教神职人员逃至越南的南方，但范廷颂仍然留在河内。翌年管理圣若望小修院；惟在1960年因捍卫修院的自由、自治及拒绝政府在修院设政治课的要求而被捕。1963年4月5日，教宗任命范廷颂为天主教北宁教区主教，同年8月15日就任；其牧铭为「我信天主的爱」。由于范廷颂被越南政府软禁差不多30年，因此他无法到所属堂区进行牧灵工作而专注研读等工作。范廷颂除了面对战争、贫困、被当局迫害天主教会等问题外，也秘密恢复修院、创建女修会团体等。1990年，教宗若望保禄二世在同年6月18日擢升范廷颂为天主教河内总教区宗座署理以填补该教区总主教的空缺。1994年3月23日，范廷颂被教宗若望保禄二世擢升为天主教河内总教区总主教并兼天主教谅山教区宗座署理；同年11月26日，若望保禄二世擢升范廷颂为枢机。范廷颂在1995年至2001年期间出任天主教越南主教团主席。2003年4月26日，教宗若望保禄二世任命天主教谅山教区兼天主教高平教区吴光杰主教为天主教河内总教区署理主教；及至2005年2月19日，范廷颂因获批辞去总主教职务而荣休；吴光杰同日真除天主教河内总教区总主教职务。范廷颂于2009年2月22日清晨在河内离世，享年89岁；其葬礼于同月26日上午在天主教河内总教区总主教座堂举行。'"},"transient":{}}],"source":"context","execution_count":37},{"metadata":{"id":"B873778CB43840E58C131154F7A37799","notebookId":"62556e9030fa900018de3878","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"4"},"transient":{}}],"source":"len(inputs['input_ids'])","execution_count":14},{"cell_type":"code","execution_count":13,"metadata":{"id":"AC631AEE902D416DBF80E6218B7C7004","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"815"},"transient":{}}],"source":"len(context)"},{"cell_type":"markdown","metadata":{"id":"28DD15785E954C03BB2968A8282316B0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"对分词之后的id进行还原为字符串字符,这个是预训练模型中的vocab.txt对应的"},{"cell_type":"code","execution_count":9,"metadata":{"id":"90E64E3E6DAA49688986F3375D51C3BE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 范 廷 颂 枢 机 （ ， ） ， 圣 名 保 禄 · 若 瑟 （ ） ， 是 越 南 罗 马 天 主 教 枢 机 。 1963 年 被 任 为 主 教 ； 1990 年 被 擢 升 为 天 主 教 河 内 总 教 区 宗 座 署 理 ； 1994 年 被 擢 升 为 总 主 教 ， 同 年 年 底 被 擢 升 为 枢 机 ； 2009 年 2 月 离 世 。 范 廷 颂 于 1919 年 6 月 15 日 在 越 南 宁 平 省 天 主 教 发 艳 教 区 出 生 ； 童 年 时 接 受 良 好 教 育 后 ， 被 一 位 越 南 神 父 带 到 河 内 继 续 其 学 业 。 范 廷 颂 于 1940 年 在 河 内 大 修 道 院 完 成 神 学 学 业 。 范 廷 颂 于 1949 年 6 月 6 日 在 河 内 的 主 教 座 堂 晋 铎 ； 及 后 被 派 到 圣 女 小 德 兰 孤 儿 院 服 务 。 1950 年 代 ， 范 廷 颂 在 河 内 堂 区 创 建 移 民 接 待 中 心 以 收 容 到 河 内 避 战 的 难 民 。 1954 年 ， 法 越 战 争 结 束 ， 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 [SEP]\n","[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 越 战 争 结 束 ， 越 南 民 主 共 和 国 建 都 河 内 ， 当 时 很 多 天 主 教 神 职 人 员 逃 至 越 南 的 南 方 ， 但 范 廷 颂 仍 然 留 在 河 内 。 翌 年 管 理 圣 若 望 小 修 院 ； 惟 在 1960 年 因 捍 卫 修 院 的 自 由 、 自 治 及 拒 绝 政 府 在 修 院 设 政 治 课 的 要 求 而 被 捕 。 1963 年 4 月 5 日 ， 教 宗 任 命 范 廷 颂 为 天 主 教 北 宁 教 区 主 教 ， 同 年 8 月 15 日 就 任 ； 其 牧 铭 为 「 我 信 天 主 的 爱 」 。 由 于 范 廷 颂 被 越 南 政 府 软 禁 差 不 多 30 年 ， 因 此 他 无 法 到 所 属 堂 区 进 行 牧 灵 工 作 而 专 注 研 读 等 工 作 。 范 廷 颂 除 了 面 对 战 争 、 贫 困 、 被 当 局 迫 害 天 主 教 会 等 问 题 外 ， 也 秘 密 恢 复 修 院 、 创 建 女 修 会 团 体 等 。 1990 年 ， 教 宗 若 望 保 禄 二 世 在 同 年 6 月 18 日 擢 升 范 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 [SEP]\n","[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 若 望 保 禄 二 世 在 同 年 6 月 18 日 擢 升 范 廷 颂 为 天 主 教 河 内 总 教 区 宗 座 署 理 以 填 补 该 教 区 总 主 教 的 空 缺 。 1994 年 3 月 23 日 ， 范 廷 颂 被 教 宗 若 望 保 禄 二 世 擢 升 为 天 主 教 河 内 总 教 区 总 主 教 并 兼 天 主 教 谅 山 教 区 宗 座 署 理 ； 同 年 11 月 26 日 ， 若 望 保 禄 二 世 擢 升 范 廷 颂 为 枢 机 。 范 廷 颂 在 1995 年 至 2001 年 期 间 出 任 天 主 教 越 南 主 教 团 主 席 。 2003 年 4 月 26 日 ， 教 宗 若 望 保 禄 二 世 任 命 天 主 教 谅 山 教 区 兼 天 主 教 高 平 教 区 吴 光 杰 主 教 为 天 主 教 河 内 总 教 区 署 理 主 教 ； 及 至 2005 年 2 月 19 日 ， 范 廷 颂 因 获 批 辞 去 总 主 教 职 务 而 荣 休 ； 吴 光 杰 同 日 真 除 天 主 教 河 内 总 教 区 总 主 教 职 务 。 范 廷 颂 于 2009 年 2 月 22 日 清 晨 在 河 内 离 世 ， 享 年 89 岁 ； 其 葬 礼 于 同 月 26 日 上 午 在 天 主 教 河 内 总 教 区 总 主 教 座 堂 [SEP]\n","[CLS] 范 廷 颂 是 什 么 时 候 被 任 为 主 教 的 ？ [SEP] 职 务 。 范 廷 颂 于 2009 年 2 月 22 日 清 晨 在 河 内 离 世 ， 享 年 89 岁 ； 其 葬 礼 于 同 月 26 日 上 午 在 天 主 教 河 内 总 教 区 总 主 教 座 堂 举 行 。 [SEP]\n"]}],"source":"for ids in inputs[\"input_ids\"]:\n    print(tokenizer.decode(ids))"},{"cell_type":"markdown","metadata":{"id":"89C532CF165B48949EA337BB992E18DB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"下面是 tokenizer可以接受多个样本，也就是多条问题和上下文可以一起输入进去："},{"cell_type":"code","execution_count":15,"metadata":{"id":"771170E2FE2B485486B1366A2334B68B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])\nThe 4 examples gave 14 features.\nHere is where each comes from: [0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3].\n","name":"stdout"}],"source":"contexts = [train_data[idx][\"context\"] for idx in range(4)]\nquestions = [train_data[idx][\"question\"] for idx in range(4)]\n\ninputs = tokenizer(\n    questions,\n    contexts,\n    max_length=300,\n    truncation=\"only_second\",\n    stride=50,\n    return_overflowing_tokens=True,\n    return_offsets_mapping=True\n)\n\nprint(inputs.keys())\nprint(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\nprint(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"},{"metadata":{"id":"58140DAE834A452EBB5C6770E6BC25FB","notebookId":"62556e9030fa900018de3878","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"14"},"transient":{}}],"source":"len(inputs['offset_mapping'])","execution_count":18},{"cell_type":"code","execution_count":11,"metadata":{"id":"4309745858F144159887D25DF04AAAAF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# inputs"},{"cell_type":"markdown","metadata":{"id":"EB6DAC74B4564DE9918C316F58AEE0CE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"可以看到，通过设置 return_overflowing_tokens 和 return_offsets_mapping，编码结果中除了 input_ids、token_type_ids 和attention_mask 以外，还返回了记录 token 到原文映射的 offset_mapping，以及记录分块样本到原始样本映射的 overflow_to_sample_mapping。\n\n从“[0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3].”我们可以看到，第1个样本和和第4个样本被分成了4个新的样本，第2个和第3个样本被分成了3个样本。\n"},{"cell_type":"markdown","metadata":{"id":"933CCD7591B142B0A9671394DA6B86CD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"sequence_ids:https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.BatchEncoding.sequence_ids\n\n返回token来自原始哪个句子：\n\n- None 代表是特殊token，比如CLS，SEP等\n- 0 代表token来自第一个句子\n- 1 代表token来自第二个句子"},{"cell_type":"code","execution_count":12,"metadata":{"id":"FCDEB849E3384F8AA9C6BB18808DF930","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["14"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":"len(inputs[\"offset_mapping\"])"},{"cell_type":"code","execution_count":13,"metadata":{"id":"9722EFDBA82A45B8807AA8E5EE44ECA2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["300"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":"len(inputs[\"offset_mapping\"][0])"},{"cell_type":"code","execution_count":14,"metadata":{"id":"FDEEADBFC96A465BA68995D37E8CD929","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[47, 0, 0, 0, 53, 0, 0, 100, 0, 0, 0, 0, 61, 0]\n","[48, 0, 0, 0, 70, 0, 0, 124, 0, 0, 0, 0, 106, 0]\n"]}],"source":"answers = [train_data[idx][\"answers\"] for idx in range(4)]\nstart_positions = []\nend_positions = []\n\nfor i, offset in enumerate(inputs[\"offset_mapping\"]):\n    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n    answer = answers[sample_idx]\n    start_char = answer[\"answer_start\"][0]# 答案的开始位置\n    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])# 答案的终止位置\n    sequence_ids = inputs.sequence_ids(i) # token 的句子标记\n\n    # Find the start and end of the context\n    # 找到context的开始和终止位置\n    idx = 0 # 记录第一个句子的索引\n    while sequence_ids[idx] != 1:\n        idx += 1\n    context_start = idx # context的开始\n    while sequence_ids[idx] == 1:\n        idx += 1\n    context_end = idx - 1\n\n    # If the answer is not fully inside the context, label is (0, 0)\n    # 如果答案answer没有完全在context，label为（0,0）\n    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n        start_positions.append(0)\n        end_positions.append(0)\n    else:\n        # Otherwise it's the start and end token positions\n        # 下面为答案完全在context上下文中\n        idx = context_start\n        while idx <= context_end and offset[idx][0] <= start_char:\n            idx += 1\n        start_positions.append(idx - 1)\n\n        idx = context_end\n        while idx >= context_start and offset[idx][1] >= end_char:\n            idx -= 1\n        end_positions.append(idx + 1)\n\nprint(start_positions)\nprint(end_positions)"},{"cell_type":"markdown","metadata":{"id":"81F13C87EF1347E988F06AB6DD9598FF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"下面我们做个简单的验证，例如对于第一个新样本，可以看到处理后的答案标签为 (47, 48)，我们将对应的 token 解码并与标注答案进行对比："},{"cell_type":"code","execution_count":15,"metadata":{"id":"920DD7678FB1448C87D7F883191816B1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["真实 answer: 1963年, 构建的labels 为: 1963 年\n"]}],"source":"idx = 0 # 第一个样本\nsample_idx = inputs[\"overflow_to_sample_mapping\"][idx]  # 第一个样本来自的哪个原始样本\nanswer = answers[sample_idx][\"text\"][0]\n\nstart = start_positions[idx] # 答案开始位置\nend = end_positions[idx] # 答案终止位置\nlabeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1]) # 上下文分词id根据转为答案\n\nprint(f\"真实 answer: {answer}, 构建的labels 为: {labeled_answer}\")"},{"cell_type":"markdown","metadata":{"id":"82F61828D2EA4968849DD159FAEC6746","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"### 训练批处理函数"},{"cell_type":"code","execution_count":16,"metadata":{"id":"EDAF2327648F4582B9B88EDF955CAC03","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"from torch.utils.data import DataLoader\n\nmax_length = 384 # 文本最大长度\nstride = 128 # 滑窗大小\n\ndef train_collote_fn(batch_samples):\n    # batch_samples一批样本，在DataLoader执行操作\n    batch_question, batch_context, batch_answers = [], [], [] #\n    for sample in batch_samples:\n        batch_question.append(sample['question'])\n        batch_context.append(sample['context'])\n        batch_answers.append(sample['answers'])\n    \n    # 对一整批问题与上下文进行编码\n    batch_data = tokenizer(\n        batch_question,\n        batch_context,\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding='max_length'\n    )\n    \n    offset_mapping = batch_data.pop('offset_mapping')\n    sample_map = batch_data.pop('overflow_to_sample_mapping')\n\n    start_positions = []\n    end_positions = []\n    \n    # 在context定位答案开始和结束的位置，同上面演示步骤一样\n    for i, offset in enumerate(offset_mapping):\n        sample_idx = sample_map[i]\n        answer = batch_answers[sample_idx]\n        start_char = answer['answer_start'][0]\n        end_char = answer['answer_start'][0] + len(answer['text'][0])\n        sequence_ids = batch_data.sequence_ids(i)\n\n        # 寻找context的开始和结束位置\n        idx = 0\n        while sequence_ids[idx] != 1:\n            idx += 1\n        context_start = idx\n        while sequence_ids[idx] == 1:\n            idx += 1\n        context_end = idx - 1\n\n        # If the answer is not fully inside the context, label is (0, 0)\n        # 如果答案answer没有完全在context，label为（0,0）\n        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n            start_positions.append(0)\n            end_positions.append(0)\n        else:\n            # Otherwise it's the start and end token positions\n            # 下面为答案完全在context上下文中\n            idx = context_start\n            while idx <= context_end and offset[idx][0] <= start_char:\n                idx += 1\n            start_positions.append(idx - 1)\n\n            idx = context_end\n            while idx >= context_start and offset[idx][1] >= end_char:\n                idx -= 1\n            end_positions.append(idx + 1)\n    batch_data['start_positions'] = start_positions\n    batch_data['end_positions'] = end_positions\n    return batch_data\n \ntrain_dataloader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=train_collote_fn)# 批数据大小为4"},{"cell_type":"code","execution_count":17,"metadata":{"id":"65EE52F3432446E3B7C9F2C60DFC80DC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])\n","batch shape: {'input_ids': torch.Size([7, 384]), 'token_type_ids': torch.Size([7, 384]), 'attention_mask': torch.Size([7, 384]), 'start_positions': torch.Size([7]), 'end_positions': torch.Size([7])}\n","{'input_ids': tensor([[ 101, 7988, 7905,  ..., 4923, 2792,  102],\n","        [ 101, 7988, 7905,  ..., 7988, 7905,  102],\n","        [ 101, 7988, 7905,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 9213, 8354,  ...,    0,    0,    0],\n","        [ 101, 1762, 1525,  ..., 1398, 2399,  102],\n","        [ 101, 1762, 1525,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n","        [0, 0, 0,  ..., 1, 1, 1],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 1, 1, 1],\n","        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 1, 1, 1],\n","        [1, 1, 1,  ..., 0, 0, 0]]), 'start_positions': tensor([ 71,   0,   0,  56, 113,   0, 158]), 'end_positions': tensor([161,   0,   0,  58, 151,   0, 172])}\n","train set size: \n","10142 -> 19189\n"]}],"source":"import torch\n\nbatch = next(iter(train_dataloader)) # 选取一批数据\n\n# 每批数据包含的字段有：dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'])\nbatch = {k: torch.tensor(v) for k, v in batch.items()}\nprint(batch.keys())\nprint('batch shape:', {k: v.shape for k, v in batch.items()})\nprint(batch)\n\nprint('train set size: ', )\nprint(len(train_data), '->', sum([len(batch_data['input_ids']) for batch_data in train_dataloader]))"},{"cell_type":"markdown","metadata":{"id":"2B32F1F2BBC94B7B9275CF49DC82D2ED","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"可以看到返回的数据中除了'input_ids', 'token_type_ids', 'attention_mask',还包括答案'start_positions', 'end_positions'，另外就是因为我们对大于最大长度384进行了划船操作，最后所有分词之后的id文本序列个数总和为19189，原始文本个数为10142 "},{"cell_type":"markdown","metadata":{"id":"AEC99BBB6A6A48EEA285E678E88D7A58","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"```\nfrom transformers import default_data_collator\n\ntrain_dataloader = DataLoader(\n    new_train_dataset,\n    shuffle=True,\n    collate_fn=default_data_collator,\n    batch_size=8,\n)\n```"},{"cell_type":"markdown","metadata":{"id":"4BA5499B2ECF422B9FDEC0A1967B7662","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"### 验证/测试批处理函数\n\n"},{"cell_type":"markdown","metadata":{"id":"7B83A06044F948598B3ECE9FB426655A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"对于验证/测试集，我们并不在意模型预测的标签，而是关注预测出的答案文本，这就需要：记录每个样本被分块成了哪几个新样本，从而合并对应的预测结果；在 offset mapping 中标记问题的对应 token，从而在后处理阶段可以区分哪些位置的 token 来自于上下文。\n\n因此，对应于验证集/测试集的批处理函数为："},{"cell_type":"code","execution_count":18,"metadata":{"id":"F72B316AB43D4B89A400E495157C13A7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"def test_collote_fn(batch_samples):\n    batch_id, batch_question, batch_context = [], [], [] # 问题样本id、问题文本，上下文\n    for sample in batch_samples:\n        batch_id.append(sample['id'])\n        batch_question.append(sample['question'])\n        batch_context.append(sample['context'])\n    batch_data = tokenizer(\n        batch_question,\n        batch_context,\n        max_length=max_length,\n        truncation=\"only_second\",\n        stride=stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n    \n    sample_map = batch_data.pop('overflow_to_sample_mapping')\n    example_ids = []\n\n    for i in range(len(batch_data['input_ids'])):\n        sample_idx = sample_map[i]\n        example_ids.append(batch_id[sample_idx])\n\n        sequence_ids = batch_data.sequence_ids(i)\n        offset = batch_data[\"offset_mapping\"][i]\n        batch_data[\"offset_mapping\"][i] = [\n            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n        ]# 上下文offset_mapping\n    batch_data[\"example_id\"] = example_ids\n    return batch_data\n\nvalid_dataloader = DataLoader(valid_data, batch_size=8, shuffle=False, collate_fn=test_collote_fn)"},{"cell_type":"markdown","metadata":{"id":"116D3EAF06E7402F9748F72258B20B3D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"同样我们打印出一个 batch 编码后的数据，并且计算分块后新数据集的大小："},{"cell_type":"code","execution_count":19,"metadata":{"id":"B0082535DCAE41588461772E3987949B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'])\n","['DEV_0_QUERY_0', 'DEV_0_QUERY_0', 'DEV_0_QUERY_1', 'DEV_0_QUERY_1', 'DEV_0_QUERY_2', 'DEV_0_QUERY_2', 'DEV_1_QUERY_0', 'DEV_1_QUERY_0', 'DEV_1_QUERY_1', 'DEV_1_QUERY_1', 'DEV_1_QUERY_2', 'DEV_1_QUERY_2', 'DEV_1_QUERY_3', 'DEV_1_QUERY_3', 'DEV_2_QUERY_0', 'DEV_2_QUERY_0']\n","valid set size: \n","3219 -> 6327\n"]}],"source":"batch = next(iter(valid_dataloader))\nprint(batch.keys())\nprint(batch['example_id'])\n\nprint('valid set size: ')\nprint(len(valid_data), '->', sum([len(batch_data['input_ids']) for batch_data in valid_dataloader]))"},{"cell_type":"markdown","metadata":{"id":"F5370A78C553470B82A3358FF92E325B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"可以看到，编码结果中除了 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping',以及记录分块样本对应 ID 的 example_id。经过分块操作后，整个测试集的样本数量从 3219 增长到了 6327。"},{"cell_type":"markdown","metadata":{"id":"5C009DFF7BC94CFF977A66A9689A5F09","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"## 训练模型\n本文我们直接使用 Transformers 库自带的 AutoModelForQuestionAnswering 函数来构建模型，前面已经通过批处理函数将训练集处理成了特定格式，因此可以直接送入模型进行训练：\n"},{"cell_type":"code","execution_count":20,"metadata":{"id":"D49A5EECF9844357B8D09C8B323D481A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device\n"]}],"source":"from transformers import AutoModelForQuestionAnswering\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Using {device} device')\n"},{"cell_type":"code","execution_count":21,"metadata":{"id":"8177C122FD27423587E3E80E3D6E437F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at hfl/chinese-bert-wwm-ext were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at hfl/chinese-bert-wwm-ext and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":"model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\nmodel = model.to(device)"},{"cell_type":"code","execution_count":22,"metadata":{"id":"610980E1811B4376A9A185D2387DF3FE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The hfl/chinese-bert-wwm-ext model has 199 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","bert.embeddings.word_embeddings.weight                  (21128, 768)\n","bert.embeddings.position_embeddings.weight                (512, 768)\n","bert.embeddings.token_type_embeddings.weight                (2, 768)\n","bert.embeddings.LayerNorm.weight                              (768,)\n","bert.embeddings.LayerNorm.bias                                (768,)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layers ====\n","\n","bert.encoder.layer.11.attention.output.LayerNorm.weight       (768,)\n","bert.encoder.layer.11.attention.output.LayerNorm.bias         (768,)\n","bert.encoder.layer.11.intermediate.dense.weight          (3072, 768)\n","bert.encoder.layer.11.intermediate.dense.bias                (3072,)\n","bert.encoder.layer.11.output.dense.weight                (768, 3072)\n","bert.encoder.layer.11.output.dense.bias                       (768,)\n","bert.encoder.layer.11.output.LayerNorm.weight                 (768,)\n","bert.encoder.layer.11.output.LayerNorm.bias                   (768,)\n","qa_outputs.weight                                           (2, 768)\n","qa_outputs.bias                                                 (2,)\n"]}],"source":"# 获取模型所有的参数\nparams = list(model.named_parameters())\n\nprint('The {} model has {:} different named parameters.\\n'.format(model_checkpoint,len(params)))\n\nprint('==== Embedding Layer ====\\n')\n\nfor p in params[0:5]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== First Transformer ====\\n')\n\nfor p in params[5:21]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== Output Layers ====\\n')\n\nfor p in params[-10:]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"},{"cell_type":"markdown","metadata":{"id":"6F26B2D18B36466D9F152D50FC61BB24","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"### 训练函数\n使用 AutoModelForQuestionAnswering 构造的模型已经封装好了对应的损失函数，计算出的损失会直接包含在模型的输出 outputs 中（可以通过 outputs.loss 获得），因此训练循环为："},{"cell_type":"code","execution_count":23,"metadata":{"id":"EBBCF73B5CFE44B69038166C16EFE5C9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"from tqdm.auto import tqdm\n\ndef train_loop(dataloader, model, optimizer, lr_scheduler, epoch, total_loss):\n    progress_bar = tqdm(range(len(dataloader)))\n    progress_bar.set_description(f'loss: {0:>7f}')\n    finish_batch_num = (epoch-1) * len(dataloader)\n    \n    model.train()\n    for batch, batch_data in enumerate(dataloader, start=1):\n        batch_data = {k: torch.tensor(v).to(device) for k, v in batch_data.items()}\n        outputs = model(**batch_data)\n        loss = outputs.loss\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n\n        total_loss += loss.item()\n        progress_bar.set_description(f'loss: {total_loss/(finish_batch_num + batch):>7f}')\n        progress_bar.update(1)\n    return total_loss"},{"cell_type":"markdown","metadata":{"id":"12B206B92F9D422AA3683DBA7C69C3F0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"### 后处理\n因为最终是根据预测出的答案文本来评估模型的性能，所以在编写验证/测试循环之前，我们先讨论一下问答模型的后处理操作，即怎么将模型的预测结果转换为答案文本。\n\n之前在自动问答任务中已经介绍过，对每个样本，问答模型都会输出两个张量，分别对应答案起始/结束位置的 logits 值，我们回顾一下之前的后处理过程：\n\n- 遮盖掉除上下文之外的其他 token 的起始/结束 logits 值,如CLS或者SEP；\n- 通过 softmax 函数将起始/结束 logits 值转换为概率值，将模型的输出转为答案开始位置和结束位置的概率；\n- 通过计算概率值的乘积估计每一对 (start_token, end_token) 为答案的分数；\n- 输出合理的（例如 start_token 要小于 end_token）分数最大的对作为答案。\n\n本文我们会稍微做一些调整：首先，我们只关心最后预测出的答案文本，因此可以跳过 softmax 函数，直接基于 logits 值来估计答案分数，从原来计算概率值的乘积变成计算 logits 值的和（因为 $\\log(ab) = \\log(a) + \\log(b)$）；其次，为了减少计算量，我们不再为所有可能的 (start_token, end_token) 对打分，而是只计算 logits 值最高的前 n_best 个 token 组成的对。因为每个位置都有可能是开始位置或者结束位置，计算所有的可能计算量很大。\n\n由于我们的 BERT 模型还没有进行微调，因此这里我们选择一个已经预训练好的问答模型 [Chinese RoBERTa-Base Model for QA](https://huggingface.co/uer/roberta-base-chinese-extractive-qa) 进行演示，并且只对验证集上的前 10 个样本进行处理："},{"cell_type":"code","execution_count":24,"metadata":{"id":"D68D8CE36C1E476295F222D5E02D5F65","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"valid_data = CMRCDataset('data/10/cmrc2018_dev.json')\nsmall_eval_set = [valid_data[idx] for idx in range(10)]\n\ntrained_checkpoint = \"uer/roberta-base-chinese-extractive-qa\"\n# trained_checkpoint = \"roberta-base-chinese-extractive-qa\"\n\ntokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\neval_set = DataLoader(small_eval_set, batch_size=4, shuffle=False, collate_fn=test_collote_fn)\n\nimport torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nfrom transformers import AutoModelForQuestionAnswering\ntrained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(device)"},{"cell_type":"code","execution_count":51,"metadata":{"id":"2711F0596FF14A59A029B7FDC6BA0860","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["[{'id': 'DEV_0_QUERY_0',\n","  'title': '战国无双3',\n","  'context': '《战国无双3》（）是由光荣和ω-force开发的战国无双系列的正统第三续作。本作以三大故事为主轴，分别是以武田信玄等人为主的《关东三国志》，织田信长等人为主的《战国三杰》，石田三成等人为主的《关原的年轻武者》，丰富游戏内的剧情。此部份专门介绍角色，欲知武器情报、奥义字或擅长攻击类型等，请至战国无双系列1.由于乡里大辅先生因故去世，不得不寻找其他声优接手。从猛将传 and Z开始。2.战国无双 编年史的原创男女主角亦有专属声优。此模式是任天堂游戏谜之村雨城改编的新增模式。本作中共有20张战场地图（不含村雨城），后来发行的猛将传再新增3张战场地图。但游戏内战役数量繁多，部分地图会有兼用的状况，战役虚实则是以光荣发行的2本「战国无双3 人物真书」内容为主，以下是相关介绍。（注：前方加☆者为猛将传新增关卡及地图。）合并本篇和猛将传的内容，村雨城模式剔除，战国史模式可直接游玩。主打两大模式「战史演武」&「争霸演武」。系列作品外传作品',\n","  'question': '《战国无双3》是由哪两个公司合作开发的？',\n","  'answers': {'text': ['光荣和ω-force', '光荣和ω-force', '光荣和ω-force'],\n","   'answer_start': [11, 11, 11]}},\n"," {'id': 'DEV_0_QUERY_1',\n","  'title': '战国无双3',\n","  'context': '《战国无双3》（）是由光荣和ω-force开发的战国无双系列的正统第三续作。本作以三大故事为主轴，分别是以武田信玄等人为主的《关东三国志》，织田信长等人为主的《战国三杰》，石田三成等人为主的《关原的年轻武者》，丰富游戏内的剧情。此部份专门介绍角色，欲知武器情报、奥义字或擅长攻击类型等，请至战国无双系列1.由于乡里大辅先生因故去世，不得不寻找其他声优接手。从猛将传 and Z开始。2.战国无双 编年史的原创男女主角亦有专属声优。此模式是任天堂游戏谜之村雨城改编的新增模式。本作中共有20张战场地图（不含村雨城），后来发行的猛将传再新增3张战场地图。但游戏内战役数量繁多，部分地图会有兼用的状况，战役虚实则是以光荣发行的2本「战国无双3 人物真书」内容为主，以下是相关介绍。（注：前方加☆者为猛将传新增关卡及地图。）合并本篇和猛将传的内容，村雨城模式剔除，战国史模式可直接游玩。主打两大模式「战史演武」&「争霸演武」。系列作品外传作品',\n","  'question': '男女主角亦有专属声优这一模式是由谁改编的？',\n","  'answers': {'text': ['村雨城', '村雨城', '任天堂游戏谜之村雨城'],\n","   'answer_start': [226, 226, 219]}}]"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":"small_eval_set[:2]"},{"cell_type":"code","execution_count":54,"metadata":{"id":"88747ABEE8AA43B38305E467AC65B0EF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["3"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":"len(eval_set)"},{"cell_type":"code","execution_count":55,"metadata":{"id":"4244B7E043F34C2DB1BC400A6A88C339","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["10 -> 20\n"]}],"source":"print(len(small_eval_set), '->', sum([len(batch_data['input_ids']) for batch_data in eval_set]))"},{"cell_type":"markdown","metadata":{"id":"D2858E98053B43C5827CE5D138C7AA0E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"接下来，与之前任务中的验证/测试循环一样，在 torch.no_grad() 会话下，使用模型对所有分块后的新样本进行预测，并且汇总预测出的起始/结束 logits 值："},{"cell_type":"code","execution_count":49,"metadata":{"id":"F6F96F0629504EC9A4F090AC64C93E97","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"start_logits = []\nend_logits = []\n\ntrained_model.eval()\nfor batch_data in eval_set:\n    del batch_data['offset_mapping']\n    del batch_data['example_id']\n    batch_data = {k: torch.tensor(batch_data[k]).to(device) for k in batch_data.keys()}\n    with torch.no_grad():\n        outputs = trained_model(**batch_data)\n    start_logits.append(outputs.start_logits.cpu().numpy())\n    end_logits.append(outputs.end_logits.cpu().numpy())\n\nimport numpy as np\nstart_logits = np.concatenate(start_logits)\nend_logits = np.concatenate(end_logits)"},{"cell_type":"code","execution_count":57,"metadata":{"id":"A9F34EEDE9C343ED9FA394CDE95D424D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["(20, 384)"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":"start_logits.shape # 每一个位置有一个开始位置预测分数"},{"cell_type":"code","execution_count":56,"metadata":{"id":"05348580A48245A49126CF57ACDA98E6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["array([[-0.32454446, -6.3131785 , -6.3455443 , ..., -6.296451  ,\n","        -5.6250753 , -0.32487208],\n","       [ 4.793381  , -6.331345  , -6.8320727 , ..., -6.8217463 ,\n","        -6.8422074 , -6.840258  ],\n","       [-0.38632843, -6.789579  , -6.5350556 , ..., -6.345919  ,\n","        -5.9616427 , -0.38640732],\n","       ...,\n","       [ 4.8648605 , -7.1324077 , -6.5852427 , ..., -6.6514907 ,\n","        -6.8463683 , -6.8796086 ],\n","       [ 1.9186516 , -6.7040086 , -6.0588236 , ..., -6.1933575 ,\n","        -5.732796  ,  1.918763  ],\n","       [-0.6912477 , -6.3243914 , -6.18748   , ..., -6.591582  ,\n","        -6.6785283 , -6.6339097 ]], dtype=float32)"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":"end_logits# 答案结束位置预测分数"},{"cell_type":"code","execution_count":58,"metadata":{"id":"8DD2A95125854E8990BE2C8AE1DBE596","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["defaultdict(<class 'list'>, {'DEV_0_QUERY_0': [0, 1], 'DEV_0_QUERY_1': [2, 3], 'DEV_0_QUERY_2': [4, 5], 'DEV_1_QUERY_0': [6, 7], 'DEV_1_QUERY_1': [8, 9], 'DEV_1_QUERY_2': [10, 11], 'DEV_1_QUERY_3': [12, 13], 'DEV_2_QUERY_0': [14, 15], 'DEV_2_QUERY_1': [16, 17], 'DEV_2_QUERY_2': [18, 19]})\n"]}],"source":"all_example_ids = []\nall_offset_mapping = []\nfor batch_data in eval_set:\n    all_example_ids += batch_data['example_id']\n    all_offset_mapping += batch_data['offset_mapping']\n\nimport collections\nexample_to_features = collections.defaultdict(list)\nfor idx, feature_id in enumerate(all_example_ids):\n    example_to_features[feature_id].append(idx)\n\nprint(example_to_features)"},{"cell_type":"code","execution_count":60,"metadata":{"id":"1D87FBCCF49B40DFAA1E4F58C210BE06","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["['DEV_0_QUERY_0',\n"," 'DEV_0_QUERY_0',\n"," 'DEV_0_QUERY_1',\n"," 'DEV_0_QUERY_1',\n"," 'DEV_0_QUERY_2',\n"," 'DEV_0_QUERY_2',\n"," 'DEV_1_QUERY_0',\n"," 'DEV_1_QUERY_0',\n"," 'DEV_1_QUERY_1',\n"," 'DEV_1_QUERY_1',\n"," 'DEV_1_QUERY_2',\n"," 'DEV_1_QUERY_2',\n"," 'DEV_1_QUERY_3',\n"," 'DEV_1_QUERY_3',\n"," 'DEV_2_QUERY_0',\n"," 'DEV_2_QUERY_0',\n"," 'DEV_2_QUERY_1',\n"," 'DEV_2_QUERY_1',\n"," 'DEV_2_QUERY_2',\n"," 'DEV_2_QUERY_2']"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":"all_example_ids"},{"cell_type":"code","execution_count":27,"metadata":{"id":"82D8CFB8D9C3444690E6F7769DC58048","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"n_best = 20\nmax_answer_length = 30\ntheoretical_answers = [\n    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n]\npredicted_answers = []\n\nfor example in small_eval_set:# 每一个问题样本 有多个滑窗\n    example_id = example[\"id\"]\n    context = example[\"context\"] # 上下文\n    answers = []\n\n    for feature_index in example_to_features[example_id]:# 每个滑窗\n        start_logit = start_logits[feature_index]# 答案开始位置分数\n        end_logit = end_logits[feature_index]# 答案结束位置分数\n        offsets = all_offset_mapping[feature_index]\n\n        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()# 开始位置排序前n_best的开始位置\n        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist() # 结束位置排序前n_best的开始位置\n        for start_index in start_indexes:# 组合开始位置与结束位置\n            for end_index in end_indexes:\n                if offsets[start_index] is None or offsets[end_index] is None:# 如果开始位置为None 跳过\n                    continue\n                # 如果结束位置小于开始位置或者结束位置减去结束位置大于最大答案长度 跳过\n                if (end_index < start_index or end_index - start_index + 1 > max_answer_length):\n                    continue \n                answers.append(\n                    {\n                        \"start\": offsets[start_index][0],# 答案开始位置\n                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],# 答案具体内容\n                        \"logit_score\": start_logit[start_index] + end_logit[end_index], # 开始与结束分数相加之和\n                    }\n                )\n    if len(answers) > 0:\n        best_answer = max(answers, key=lambda x: x[\"logit_score\"])# 选取开始位置分数+结束位置分数最大的开始-结束组合\n        predicted_answers.append({\n            \"id\": example_id, \n            \"prediction_text\": best_answer[\"text\"], \n            \"answer_start\": best_answer[\"start\"]\n        })\n    else:\n        # 如果候选答案为空，返回空答案\n        predicted_answers.append({\n            \"id\": example_id, \n            \"prediction_text\": \"\", \n            \"answer_start\": 0\n        })"},{"cell_type":"code","execution_count":28,"metadata":{"id":"F64201FCB2EE452FA24B6D14EF087B8C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["DEV_0_QUERY_0\n","pred: 光荣和ω-force\n","label: ['光荣和ω-force', '光荣和ω-force', '光荣和ω-force']\n","DEV_0_QUERY_1\n","pred: 任天堂游戏谜之村雨城\n","label: ['村雨城', '村雨城', '任天堂游戏谜之村雨城']\n","DEV_0_QUERY_2\n","pred: 「战史演武」&「争霸演武」\n","label: ['「战史演武」&「争霸演武」', '「战史演武」&「争霸演武」', '「战史演武」&「争霸演武」']\n","DEV_1_QUERY_0\n","pred: 锣鼓经是大陆传统器乐及戏曲里面常用的打击乐记谱方法\n","label: ['大陆传统器乐及戏曲里面常用的打击乐记谱方法', '大陆传统器乐及戏曲里面常用的打击乐记谱方法', '大陆传统器乐及戏曲里面常用的打击乐记谱方法']\n","DEV_1_QUERY_1\n","pred: 「锣鼓点」\n","label: ['锣鼓点', '锣鼓点', '锣鼓点']\n","DEV_1_QUERY_2\n","pred: 依照角色行当的身份、性格、情绪以及环境，配合相应的锣鼓点。\n","label: ['依照角色行当的身份、性格、情绪以及环境，配合相应的锣鼓点。', '依照角色行当的身份、性格、情绪以及环境，配合相应的锣鼓点。', '依照角色行当的身份、性格、情绪以及环境，配合相应的锣鼓点']\n","DEV_1_QUERY_3\n","pred: 戏曲锣鼓所运用的敲击乐器主要分为鼓、锣、钹和板四类型\n","label: ['鼓、锣、钹和板', '鼓、锣、钹和板', '鼓、锣、钹和板']\n","DEV_2_QUERY_0\n","pred: 全长364.6公里\n","label: ['364.6公里', '364.6公里', '364.6公里']\n","DEV_2_QUERY_1\n","pred: 三茂铁路股份有限公司\n","label: ['三茂铁路股份有限公司', '三茂铁路股份有限公司', '三茂铁路股份有限公司']\n","DEV_2_QUERY_2\n","pred: 1903年\n","label: ['1903年', '1903年', '1903年']\n"]}],"source":"for pred, label in zip(predicted_answers, theoretical_answers):\n    print(pred['id'])\n    print('pred:', pred['prediction_text'])\n    print('label:', label['answers']['text'])"},{"cell_type":"markdown","metadata":{"id":"B9C250D2D6734CC6A74F69461E66AA67","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"可以看到，由于我们选择的 Chinese RoBERTa-Base Model for QA 模型本身的预训练数据就包含了 CMRC 2018，因此模型的预测结果还是不错的。\n\n在成功获取到预测的答案片段之后，就可以对模型的性能进行评估了。这里我们对 CMRC 2018 自带的评估脚本进行修改，使其支持本文模型的输出格式："},{"cell_type":"code","execution_count":36,"metadata":{"id":"D650353232844EDC97691C3B8E0DE5ED","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"import re\nimport sys\nfrom transformers import AutoTokenizer\n\n# model_checkpoint = \"bert-base-cased\"\n# model_checkpoint = '../pretrained_models/chinese-roberta-wwm-ext'\ntrained_checkpoint = \"uer/roberta-base-chinese-extractive-qa\"\ntokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n\ntokenize = lambda x: tokenizer(x).tokens()[1:-1]\n\n# import nltk\n# tokenize = lambda x: nltk.word_tokenize(x)\n\n# split Chinese with English\ndef mixed_segmentation(in_str, rm_punc=False):\n    in_str = str(in_str).lower().strip()\n    segs_out = []\n    temp_str = \"\"\n    sp_char = ['-',':','_','*','^','/','\\\\','~','`','+','=',\n               '，','。','：','？','！','“','”','；','’','《','》','……','·','、',\n               '「','」','（','）','－','～','『','』']\n    for char in in_str:\n        if rm_punc and char in sp_char:\n            continue\n        if re.search(r'[\\u4e00-\\u9fa5]', char) or char in sp_char:\n            if temp_str != \"\":\n                # ss = nltk.word_tokenize(temp_str)\n                ss = tokenize(temp_str)\n                segs_out.extend(ss)\n                temp_str = \"\"\n            segs_out.append(char)\n        else:\n            temp_str += char\n\n    #handling last part\n    if temp_str != \"\":\n        # ss = nltk.word_tokenize(temp_str)\n        ss = tokenize(temp_str)\n        segs_out.extend(ss)\n\n    return segs_out\n\n# remove punctuation\ndef remove_punctuation(in_str):\n    in_str = str(in_str).lower().strip()\n    sp_char = ['-',':','_','*','^','/','\\\\','~','`','+','=',\n               '，','。','：','？','！','“','”','；','’','《','》','……','·','、',\n               '「','」','（','）','－','～','『','』']\n    out_segs = []\n    for char in in_str:\n        if char in sp_char:\n            continue\n        else:\n            out_segs.append(char)\n    return ''.join(out_segs)\n\n# find longest common string\ndef find_lcs(s1, s2):\n    m = [[0 for i in range(len(s2)+1)] for j in range(len(s1)+1)]\n    mmax = 0\n    p = 0\n    for i in range(len(s1)):\n        for j in range(len(s2)):\n            if s1[i] == s2[j]:\n                m[i+1][j+1] = m[i][j]+1\n                if m[i+1][j+1] > mmax:\n                    mmax=m[i+1][j+1]\n                    p=i+1\n    return s1[p-mmax:p], mmax\n\ndef calc_f1_score(answers, prediction):\n    f1_scores = []\n    for ans in answers:\n        ans_segs = mixed_segmentation(ans, rm_punc=True)\n        prediction_segs = mixed_segmentation(prediction, rm_punc=True)\n        lcs, lcs_len = find_lcs(ans_segs, prediction_segs)\n        if lcs_len == 0:\n            f1_scores.append(0)\n            continue\n        precision     = 1.0*lcs_len/len(prediction_segs)\n        recall         = 1.0*lcs_len/len(ans_segs)\n        f1             = (2*precision*recall)/(precision+recall)\n        f1_scores.append(f1)\n    return max(f1_scores)\n\ndef calc_em_score(answers, prediction):\n    em = 0\n    for ans in answers:\n        ans_ = remove_punctuation(ans)\n        prediction_ = remove_punctuation(prediction)\n        if ans_ == prediction_:\n            em = 1\n            break\n    return em\n\ndef evaluate(predictions, references):\n    f1 = 0\n    em = 0\n    total_count = 0\n    skip_count = 0\n    pred = dict([(data['id'], data['prediction_text']) for data in predictions])\n    ref = dict([(data['id'], data['answers']['text']) for data in references])\n    for query_id, answers in ref.items():\n        total_count += 1\n        if query_id not in pred:\n            sys.stderr.write('Unanswered question: {}\\n'.format(query_id))\n            skip_count += 1\n            continue\n        prediction = pred[query_id]\n        f1 += calc_f1_score(answers, prediction)\n        em += calc_em_score(answers, prediction)\n    f1_score = 100.0 * f1 / total_count\n    em_score = 100.0 * em / total_count\n    return {\n        'avg': (em_score + f1_score) * 0.5, \n        'f1': f1_score, \n        'em': em_score, \n        'total': total_count, \n        'skip': skip_count\n    }"},{"cell_type":"code","execution_count":37,"metadata":{"id":"D4F24F95922C4A4183F2C49795F4439A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["F1: 90.91 EM: 70.00 AVG: 80.46\n","\n"]}],"source":"result = evaluate(predicted_answers, theoretical_answers)\nprint(f\"F1: {result['f1']:>0.2f} EM: {result['em']:>0.2f} AVG: {result['avg']:>0.2f}\\n\")"},{"cell_type":"markdown","metadata":{"id":"D8E4040B60B447DA9726F68A4BA54B3C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"### 测试参数\n熟悉了后处理操作之后，编写验证/测试循环就很简单了，只需对上面的这些步骤稍作整合即可。这里由于我们还需要使用到样本的原始文本，因此将数据集也作为参数传入："},{"cell_type":"code","execution_count":44,"metadata":{"id":"79FDE323B16F43D697002A356AD037EA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"import collections\n\nn_best = 20\nmax_answer_length = 30\n\ndef test_loop(dataloader, dataset, model, mode='Test'):\n    assert mode in ['Valid', 'Test']\n\n    all_example_ids = []\n    all_offset_mapping = []\n    for batch_data in dataloader:\n        all_example_ids += batch_data['example_id']\n        all_offset_mapping += batch_data['offset_mapping']\n\n    model.eval()\n    start_logits = []\n    end_logits = []\n    for batch_data in tqdm(dataloader):\n        del batch_data['offset_mapping']\n        del batch_data['example_id']\n        batch_data = {k: torch.tensor(batch_data[k]).to(device) for k in batch_data.keys()}\n        with torch.no_grad():\n            outputs = model(**batch_data)\n        start_logits.append(outputs.start_logits.cpu().numpy())\n        end_logits.append(outputs.end_logits.cpu().numpy())\n    start_logits = np.concatenate(start_logits)\n    end_logits = np.concatenate(end_logits)\n    \n    example_to_features = collections.defaultdict(list)\n    for idx, feature_id in enumerate(all_example_ids):\n        example_to_features[feature_id].append(idx)\n    \n    theoretical_answers = [\n        {\"id\": dataset[idx][\"id\"], \"answers\": dataset[idx][\"answers\"]} for idx in range(len(dataset))\n    ]\n    predicted_answers = []\n    for idx in tqdm(range(len(dataset))):\n        example_id = dataset[idx][\"id\"]\n        context = dataset[idx][\"context\"]\n        answers = []\n\n        # Loop through all features associated with that example\n        for feature_index in example_to_features[example_id]:\n            start_logit = start_logits[feature_index]\n            end_logit = end_logits[feature_index]\n            offsets = all_offset_mapping[feature_index]\n\n            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if offsets[start_index] is None or offsets[end_index] is None:\n                        continue\n                    if (end_index < start_index or end_index-start_index+1 > max_answer_length):\n                        continue\n                    answers.append({\n                        \"start\": offsets[start_index][0], \n                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]], \n                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n                    })\n        # Select the answer with the best score\n        if len(answers) > 0:\n            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n            predicted_answers.append({\n                \"id\": example_id, \n                \"prediction_text\": best_answer[\"text\"], \n                \"answer_start\": best_answer[\"start\"]\n            })\n        else:\n            predicted_answers.append({\n                \"id\": example_id, \n                \"prediction_text\": \"\", \n                \"answer_start\": 0\n            })\n    result = evaluate(predicted_answers, theoretical_answers)\n    print(f\"{mode} F1: {result['f1']:>0.2f} EM: {result['em']:>0.2f} AVG: {result['avg']:>0.2f}\\n\")\n    return result,predicted_answers"},{"cell_type":"markdown","metadata":{"id":"DE50D4D8D22847D7BD87CD4521528DC8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"### 保存和加载模型\n与之前一样，我们会根据模型在验证集上的性能来调整超参数以及选出最好的模型，然后将选出的模型应用于测试集进行评估。这里继续使用 AdamW 优化器，并且通过 get_scheduler() 函数定义学习率调度器：\n\n"},{"cell_type":"markdown","metadata":{"id":"175D9FC297AB46F1880EBF4FE5F98209","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"原生训练：hfl/chinese-bert-wwm-ext"},{"cell_type":"code","execution_count":32,"metadata":{"id":"15D3E3A6AB3E4EC89D436B1CC0F19A29","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","-------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["F:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ec39dcee94142ffb8ff863adc3ef4a6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2536 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d86279a24e5b4e6ba338ddfd5ce0ff1f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/403 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3cf4138c51fb426e9bdeb62bad90434b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3219 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Valid F1: 84.86 EM: 65.80 AVG: 75.33\n","\n","saving new weights...\n","\n","Epoch 2/3\n","-------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d45f47802d0f4bfd8eb0f5089a838e72","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2536 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b35d3d8e980a44f7bc9c2aa6c17e5e11","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/403 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64ade976a437431aa2633bb353aa6e1d","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3219 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Valid F1: 86.91 EM: 68.65 AVG: 77.78\n","\n","saving new weights...\n","\n","Epoch 3/3\n","-------------------------------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9301112e88dc45f69601917b9f5a4859","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2536 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f54b8eebf1cf4a00a2249f8976d8873a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/403 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"233c8acdceef4324917b0fcc4916add3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3219 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Valid F1: 85.65 EM: 66.29 AVG: 75.97\n","\n","Done!\n"]}],"source":"from transformers import AdamW, get_scheduler\n\nlearning_rate = 2e-5\nepoch_num = 3\n\noptimizer = AdamW(model.parameters(), lr=learning_rate)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=epoch_num*len(train_dataloader),\n)\n\ntotal_loss = 0.\nbest_avg_score = 0.\nfor t in range(epoch_num):\n    print(f\"Epoch {t+1}/{epoch_num}\\n-------------------------------\")\n    total_loss = train_loop(train_dataloader, model, optimizer, lr_scheduler, t+1, total_loss)\n    valid_scores = test_loop(valid_dataloader, valid_data, model, mode='Valid')\n    avg_score = valid_scores['avg']\n    if avg_score > best_avg_score:\n        best_avg_score = avg_score\n        print('saving new weights...\\n')\n        torch.save(model.state_dict(), f'epoch_{t+1}_valid_avg_{avg_score:0.4f}_model_weights.bin')\nprint(\"Done!\")"},{"cell_type":"code","execution_count":35,"metadata":{"id":"D72D57B79611477B878DBDD376F6B19E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["{'avg': 75.97387284638347,\n"," 'f1': 85.65386560578341,\n"," 'em': 66.29388008698353,\n"," 'total': 3219,\n"," 'skip': 0}"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":"valid_scores"},{"cell_type":"code","execution_count":45,"metadata":{"id":"55FCEB083E3C4474915E0B045F4D04B0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43a13b41c8564daf9c52fa6da8d57670","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/126 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a14f37928d94668a8601f1b20a38f9b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1002 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Test F1: 69.34 EM: 33.73 AVG: 51.54\n","\n"]}],"source":"test_data = CMRCDataset('data/10/cmrc2018_trial.json')\ntest_dataloader = DataLoader(test_data, batch_size=8, shuffle=False, collate_fn=test_collote_fn)\n\nmodel.load_state_dict(torch.load('epoch_2_valid_avg_77.7824_model_weights.bin'))\nscores,predicted_answers=test_loop(test_dataloader, test_data, model, mode='Test')"},{"cell_type":"code","execution_count":46,"metadata":{"id":"5EBB3812AD4B49DA84D707F79F5B3317","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["{'avg': 51.53681296175643,\n"," 'f1': 69.34109099337314,\n"," 'em': 33.73253493013972,\n"," 'total': 1002,\n"," 'skip': 0}"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":"scores"},{"cell_type":"markdown","metadata":{"id":"C61CE15316AF4D6B92BF6A5263B18409","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"predicted_answers为测试集预测结果列表，里面包含了一个问题的对应id，预测答案文本内容、以及答案的开始位置"},{"cell_type":"code","execution_count":48,"metadata":{"id":"079BD72B0C624CD799E0677DC648CC6B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["[{'id': 'TRIAL_800_QUERY_0', 'prediction_text': '踢爆', 'answer_start': 182},\n"," {'id': 'TRIAL_800_QUERY_1',\n","  'prediction_text': '4分钟内不得进行配对(每次中离+4分钟)',\n","  'answer_start': 301},\n"," {'id': 'TRIAL_800_QUERY_2',\n","  'prediction_text': '水枪、小枪、锤子或是水炸弹',\n","  'answer_start': 88},\n"," {'id': 'TRIAL_800_QUERY_3',\n","  'prediction_text': '经典、热血、狙击等模式进行游戏。',\n","  'answer_start': 278},\n"," {'id': 'TRIAL_154_QUERY_0',\n","  'prediction_text': '澳洲南部岛屿及西岸地区',\n","  'answer_start': 59},\n"," {'id': 'TRIAL_154_QUERY_1',\n","  'prediction_text': '由于牠们每季在袋鼠岛都大量繁殖，破坏了针鼹岛上的生活环境',\n","  'answer_start': 71},\n"," {'id': 'TRIAL_154_QUERY_2',\n","  'prediction_text': '1628年船难的生还者在西澳发现的',\n","  'answer_start': 115},\n"," {'id': 'TRIAL_154_QUERY_3', 'prediction_text': '8公斤重', 'answer_start': 186},\n"," {'id': 'TRIAL_154_QUERY_4',\n","  'prediction_text': '有可能是一种神奇药及青霉素的改良。',\n","  'answer_start': 217},\n"," {'id': 'TRIAL_598_QUERY_0',\n","  'prediction_text': '日本战国时代至安土桃山时代',\n","  'answer_start': 23},\n"," {'id': 'TRIAL_598_QUERY_1',\n","  'prediction_text': '在永禄5年 （1562年）元服',\n","  'answer_start': 223},\n"," {'id': 'TRIAL_598_QUERY_2',\n","  'prediction_text': '天正16年（1588年）',\n","  'answer_start': 326},\n"," {'id': 'TRIAL_598_QUERY_3',\n","  'prediction_text': '在天正16年（1588年）把家督让予嫡子康直并在京都隐居',\n","  'answer_start': 540},\n"," {'id': 'TRIAL_598_QUERY_4',\n","  'prediction_text': '收家康的七男松千代为康直的养子并令其继承深谷藩1万石。',\n","  'answer_start': 612},\n"," {'id': 'TRIAL_662_QUERY_0',\n","  'prediction_text': '泰国的首都曼谷的乍都节县',\n","  'answer_start': 9},\n"," {'id': 'TRIAL_662_QUERY_1', 'prediction_text': '泰国国家铁路局', 'answer_start': 58},\n"," {'id': 'TRIAL_662_QUERY_2',\n","  'prediction_text': '它的面积约为0.304平方公里',\n","  'answer_start': 107},\n"," {'id': 'TRIAL_662_QUERY_3',\n","  'prediction_text': '展示泰国的机车及车厢。',\n","  'answer_start': 245},\n"," {'id': 'TRIAL_863_QUERY_0', 'prediction_text': '法国', 'answer_start': 47},\n"," {'id': 'TRIAL_863_QUERY_1',\n","  'prediction_text': '自1983年至今，一直兼任埃纳省地区首府韦尔万（Vervins）市市长',\n","  'answer_start': 170}]"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":"predicted_answers[:20]"},{"cell_type":"markdown","metadata":{"id":"2FA153EF20194BCF9D2BEC2A05C407D0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"## 参考资料"},{"cell_type":"markdown","metadata":{"id":"371AFFAF2F7A4ED890207BA1FC87F015","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"- [Hugging Face 的 Transformers 库快速入门（九）：抽取式问答](https://xiaosheng.run/2022/04/02/transformers-note-9.html)\n- [自然语言处理领域中的自动问答研究进展](http://www.xml-data.org/whdy/html/19bfe585-f918-4b28-bbcf-3431d84a570b.htm)"},{"cell_type":"code","execution_count":null,"metadata":{"id":"41686F92B8114D0A895FA1DA614B5F81","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":4}