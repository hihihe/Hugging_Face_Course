{"cells":[{"cell_type":"markdown","metadata":{"id":"81AD056C697E473F8867B46E6BBB1AF0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"【NLP最佳实践】Huggingface Transformers实战教程：\n\nhttps://www.heywhale.com/home/activity/detail/61dd2a3dc238c000186ac330"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TuLR0BMJhIub","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"# 基于 ALBERT 进行句子相似性判断\n\n- 二分类\n- 回归"},{"cell_type":"markdown","metadata":{"id":"7D1A935CCE4F4114BE2750C18E36D88A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"## 句子相似度计算的几种方法\n1.句子相似度介绍：\n句子相似度–指的是两个句子之间相似的程度。在NLP中有很大的用处，譬如对话系统，文本分类、信息检索、语义分析等，它可以为我们提供检索信息更快的方式，并且得到的信息更加准确。\n\n2.句子相似计算的方法概括：\n句子相似度计算主要分为：\n\n基于统计的方法：\n    莱文斯坦距离（编辑距离）\n\n    BM25\n\n    TFIDF计算\n\n    TextRank算法中的句子相似性\n\n基于深度学习的方法：\n\n    基于Word2Vec的余弦相似度\n\n    DSSM（Deep Structured Semantic Models）\n"},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"15F547E0D96B46A5A0C00481D7A75D60","jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"source":"## sentence-similarity\n\nGitHub：https://github.com/yanqiangmiffy/sentence-similarity\n\n问题句子相似度计算，即给定客服里用户描述的两句话，用算法来判断是否表示了相同的语义。\n## 句子相似度判定\n今年和去年前后相继出现了多个关于句子相似度判定的比赛，即得定两个句子，用算法判断是否表示了相同的语义或者意思。\n`其中第4、5这个2个比赛数据集格式比较像，请见` [sent_match](https://github.com/yanqiangmiffy/sent_match),`，2、3 的数据集格式比较像，本仓库基于2、3数据集做实验`\n\n下面是比赛的列表：\n- 1 [Quora Question Pairs](https://www.kaggle.com/c/quora-question-pairs)\n\n> The goal of this competition is to predict which of the provided pairs of questions contain two questions with the same meaning. \n\n> [数据集](https://www.kaggle.com/c/quora-question-pairs/data)未经过脱敏处理,用真实的英文单词标识\n\n- 2 [ ATEC学习赛：NLP之问题相似度计算](https://dc.cloud.alipay.com/index#/topic/intro?id=8)\n> 问题相似度计算，即给定客服里用户描述的两句话，用算法来判断是否表示了相同的语义。\n\n> [数据集](https://dc.cloud.alipay.com/index#/topic/data?id=8)未经过脱敏处理\n> 示例：\n> 1. “花呗如何还款” --“花呗怎么还款”：同义问句\n> 2. “花呗如何还款” -- “我怎么还我的花被呢”：同义问句\n> 3. “花呗分期后逾期了如何还款”-- “花呗分期后逾期了哪里还款”：非同义问句\n> 对于例子a，比较简单的方法就可以判定同义；对于例子b，包含了错别字、同义词、词序变换等问题，两个句子乍一看并不类似，想正确判断比较有挑战；对于例子c，两句> 话很类似，仅仅有一处细微的差别 “如何”和“哪里”，就导致语义不一致。\"\"\"\n\n- 3 [CCKS 2018 微众银行智能客服问句匹配大赛](https://biendata.com/competition/CCKS2018_3/leaderboard/)\n> 与基于Quora的的的语义等价判别相同，本次评测任务的主要目标是针对中文的真实客服语料，进行问句意图匹配。集给定两个语句，要求判定两者意图是否相同或者相近。所有语料来自原始的银行领域智能客服日志，并经过了筛选和人工的意图匹配标注。\n\n>  [数据集](https://biendata.com/competition/CCKS2018_3/data/)经过脱敏处理\n> 输入：一般几天能通过审核\\ t一般审核通过要多久\n> 输出：1\n\n- 4 [CHIP 2018-第四届中国健康信息处理会议（CHIP）](https://biendata.com/competition/chip2018/)\n> 本次评测任务的主要目标是针对中文的真实患者健康咨询语料，进行问句意图匹配。给定两个语句，要求判定两者意图是否相同或者相近。所有语料来自互联网上患者真实> 的问题，并经过了筛选和人工的意图匹配标注。平安云将为报名的队伍提供GPU的训练环境。\n\n>  [数据集](https://biendata.com/competition/chip2018/data/)经过脱敏处理，问题由数字标示\n> 训练集包含20000条左右标注好的数据（经过脱敏处理，包含标点符号），供参赛人员进行训练和测试。 测试集包含10000条左右无label的数据（经过脱敏处理，包含标点> 符号）。选手需要对测试集数据的label进行预测并提交。测试集数据作为AB榜的评测依据。\n\n- 5 [第三届魔镜杯大赛](https://ai.ppdai.com/mirror/goToMirrorDetail?mirrorId=1)\n> 智能客服聊天机器人场景中，待客户提出问题后，往往需要先计算客户提出问题与知识库问题的相似度，进而定位最相似问题，再对问题给出答案。本次比赛的题目便是问 > 题相似度算法设计。\n\n>  [数据集](https://ai.ppdai.com/mirror/goToMirrorDetail?mirrorId=1)经过脱敏处理，问题由数字标示\n>  为保护用户隐私并保证比赛的公平公正，所有原始文本信息都被编码成单字ID序列和词语ID序列。单字包含单个汉字、英文字母、标点及空格等；词语包含切词后的中> 文词语、英文单词、标点及空格等。单字ID和词语ID存在于两个不同的命名空间，即词语中的单字词或者标点，和单字中的相同字符及相同标点不一定有同一个ID。其> > 中，单字序列以L开头，词语序列以W开头。\n"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SZIHhCYNhN-H","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 实验简介\n\n我们将在本笔记本中学习如何为**句子对分类**任务微调 ALBERT 和其他基于 BERT 的模型。这个 PyTorch 实现利用 Hugging face *transformers* 和 *datasets* 库来下载预训练模型、实现快速研究实验、访问数据集和评估指标。\n\n该任务是语义文本相似性问题的一部分。你有两对句子，你想对它们之间的文本交互进行建模。\n\n本笔记本中使用的数据集是 Microsoft Research Paraphrase Corpus (MRPC)，它是 GLUE 基准测试的一部分：您有两个句子，您想预测一个句子是否是另一个句子的释义。评价指标为 F1 和准确率。\n\n您应该能够在验证集上达到 **91.19** 作为 F1 分数（ALBERT 论文中报告的分数是 90.9）和 **87.5** 作为准确度。微调每个 epoch 需要 35 秒，推理需要 2 秒。\n\n"},{"cell_type":"markdown","metadata":{"id":"2B9FDB05DB0249768384C42FD3041211","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"GLUE共有九个任务，分别是CoLA、SST-2、MRPC、STS-B、QQP、MNLI、QNLI、RTE、WNLI。可以分为三类，分别是单句任务，相似性和释义任务:\n    \n- 单句任务：CoLA、SST-2\n- 相似性任务：MRPC、STS-B、QQP\n- 释义任务：MNLI、QNLI、RTE、WNLI"},{"cell_type":"markdown","metadata":{"id":"1E897E1DC1EA481F948B2ECC027D643A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"通常来说，NLP可以分为自然语言理解（NLU）和自然语言生成（NLG）。在NLU方面，我们拿时下最流行的GLUE(General Language Understanding Evaluation)排行榜举例，其上集合了九项NLU的任务，分别是\n\n- CoLA(The Corpus of Linguistic Acceptability):纽约大学发布的有关语法的数据集，该任务主要是对一个给定句子，判定其是否语法正确，因此CoLA属于单个句子的文本二分类任务；\n- SST(The Stanford Sentiment Treebank)，是斯坦福大学发布的一个情感分析数据集，主要针对电影评论来做情感分类，因此SST属于单个句子的文本分类任务（其中SST-2是二分类，SST-5是五分类，SST-5的情感极性区分的更细致）；\n- MRPC(Microsoft Research Paraphrase Corpus)，由微软发布，判断两个给定句子，是否具有相同的语义，属于句子对的文本二分类任务；\n- STS-B(Semantic Textual Similarity Benchmark)，主要是来自于历年SemEval中的一个任务（同时该数据集也包含在了SentEval），具体来说是用1到5的分数来表征两个句子的语义相似性，本质上是一个回归问题，但依然可以用分类的方法做，因此可以归类为句子对的文本五分类任务；\n- QQP(Quora Question Pairs)，是由Quora发布的两个句子是否语义一致的数据集，属于句子对的文本二分类任务；\n- MNLI(Multi-Genre Natural Language Inference)，同样由纽约大学发布，是一个文本蕴含的任务，在给定前提（Premise）下，需要判断假设（Hypothesis）是否成立，其中因为MNLI主打卖点是集合了许多不同领域风格的文本，因此又分为matched和mismatched两个版本的MNLI数据集，前者指训练集和测试集的数据来源一致，而后者指来源不一致。该任务属于句子对的文本三分类问题。\n- QNLI（Question Natural Language Inference)，其前身是SQuAD 1.0数据集，给定一个问句，需要判断给定文本中是否包含该问句的正确答案。属于句子对的文本二分类任务；\n- RTE(Recognizing Textual Entailment)，和MNLI类似，也是一个文本蕴含任务，不同的是MNLI是三分类，RTE只需要判断两个句子是否能够推断或对齐，属于句子对的文本二分类任务；\n- WNLI(Winograd Natural Language Inference)，也是一个文本蕴含任务，不过似乎GLUE上这个数据集还有些问题；"},{"cell_type":"code","execution_count":null,"metadata":{"id":"D01A5877F7A644FDB5FF28CF67CCD488","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# 中文语言理解测评基准(CLUE)\n# https://www.cluebenchmarks.com/classification.html"},{"cell_type":"markdown","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"D49CFBB019EE400A861281EA6485F7E4","jupyter":{},"slideshow":{"slide_type":"slide"},"trusted":true},"source":"本教程的主要特点是：\n- 端到端 ML 实施（训练、验证、预测、评估）\n- 轻松适应您自己的数据集\n- 促进其他基于 BERT 的模型（BERT、ALBERT、...）的快速实验\n- 使用有限的计算资源进行快速训练（混合精度、梯度累积……）\n- 多 GPU 执行\n- 分类决策的阈值选择（不一定是 0.5）\n- 冻结 BERT 层，只更新分类层权重或更新所有权重\n- 种子设置，可复现结果\n\n#### 目录\n\n1. [安装库和导入](#section01)\n\n2. [加载数据集](#section02)\n\n3. [类和函数](#section03)\n\n4. [参数](#section04)\n\n5. [训练与验证](#section05)\n\n6. [预测](#section06)\n\n7. [评价](#section07)\n\n8. [实验思路](#section08)\n"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YIv7rF4V6lyE","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 安装并导入所需要的库"},{"cell_type":"code","execution_count":1,"metadata":{"id":"5BB83222D8C241D28D7F3C43C97541CC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# !pip install datasets\n# !pip install transformers"},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"colab_type":"code","id":"KE_TpNaSZQ5n","jupyter":{},"outputId":"e100b424-e5d9-45e5-ba3a-abe466edeb16","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"import torch\nimport torch.nn as nn\nimport os\nimport matplotlib.pyplot as plt\nimport copy\nimport torch.optim as optim\nimport random\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\nfrom datasets import load_dataset, load_metric\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # "},{"cell_type":"code","execution_count":3,"metadata":{"id":"B9E384BD26AF41CC9337D4AA0A46F6A2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# 查看运行资源信息\n# !pip -q install gputil\n# !pip -q install psutil\n# !pip -q install humanize"},{"cell_type":"markdown","metadata":{"id":"7178F2FF6DC145FF8836A1D6672CD897","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"psutil可以用来做系统监控，性能分析，进程管理。 支持的系统有Linux, Windows, OSX, FreeBSD and Sun Solaris，32和64位系统都支持，同时支持pyhton2.4到3.4。"},{"cell_type":"code","execution_count":16,"metadata":{"id":"B215AF8B08FF410794CCAA6C17C30722","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2022-03-30-21:15:28\n","物理CPU个数: 8\n","cup使用率: 93.2%\n","物理内存： 31.95 G\n","剩余物理内存： 15.28 G\n","物理内存使用率： 52 %\n","系统启动时间: 2022-03-26 23:34:30\n","当前有1个用户，分别是 yanqiang\n","网卡接收流量 4412.50 Mb 网卡发送流量 16697.36 Mb\n","-----------------------------磁盘信息---------------------------------------\n","系统磁盘信息：[sdiskpart(device='C:\\\\', mountpoint='C:\\\\', fstype='NTFS', opts='rw,fixed', maxfile=255, maxpath=260), sdiskpart(device='D:\\\\', mountpoint='D:\\\\', fstype='NTFS', opts='rw,fixed', maxfile=255, maxpath=260), sdiskpart(device='E:\\\\', mountpoint='E:\\\\', fstype='NTFS', opts='rw,fixed', maxfile=255, maxpath=260)]\n","总容量：464G\n","已用容量：318G\n","可用容量：145G\n","总容量：531G\n","已用容量：358G\n","可用容量：172G\n","总容量：399G\n","已用容量：259G\n","可用容量：140G\n"]}],"source":"# !/usr/bin/env python\n# -*- coding: utf-8 -*-\nimport psutil\nimport datetime\nimport time\n \n# 当前时间\nnow_time = time.strftime('%Y-%m-%d-%H:%M:%S', time.localtime(time.time()))\nprint(now_time)\n# 查看cpu的信息\nprint (\"物理CPU个数: %s\" % psutil.cpu_count(logical=False))\ncpu = (str)(psutil.cpu_percent(1)) + '%'\nprint (u\"cup使用率: %s\" % cpu)\n# 查看内存信息,剩余内存.free  总共.total\nfree = str(round(psutil.virtual_memory().free/(1024.0*1024.0*1024.0), 2))\ntotal = str(round(psutil.virtual_memory().total/(1024.0*1024.0*1024.0), 2))\nmemory = int(psutil.virtual_memory().total-psutil.virtual_memory().free)/float(psutil.virtual_memory().total)\nprint(u\"物理内存： %s G\" % total)\nprint (u\"剩余物理内存： %s G\" % free)\nprint (u\"物理内存使用率： %s %%\" % int(memory*100))\n# 系统启动时间\nprint (u\"系统启动时间: %s\" % datetime.datetime.fromtimestamp(psutil.boot_time()).strftime(\"%Y-%m-%d %H:%M:%S\"))\n# 系统用户\nusers_count = len(psutil.users())\nusers_list = \",\".join([u.name for u in psutil.users()])\nprint (u\"当前有%s个用户，分别是 %s\" % (users_count, users_list))\n# 网卡，可以得到网卡属性，连接数，当前流量等信息\nnet = psutil.net_io_counters()\nbytes_sent = '{0:.2f} Mb'.format(net.bytes_recv / 1024/1024)\nbytes_rcvd = '{0:.2f} Mb'.format(net.bytes_sent / 1024/1024)\nprint (u\"网卡接收流量 %s 网卡发送流量 %s\" % (bytes_rcvd, bytes_sent))\nio = psutil.disk_partitions()\ndel io[-1]\nprint ('-----------------------------磁盘信息---------------------------------------')\nprint (\"系统磁盘信息：\"+str(io))\nfor i in io:\n    o = psutil.disk_usage(i.device)\n    print (\"总容量：\"+str(int(o.total/(1024.0*1024.0*1024.0)))+\"G\")\n    print (\"已用容量：\"+str(int(o.used/(1024.0*1024.0*1024.0)))+\"G\")\n    print (\"可用容量：\"+str(int(o.free/(1024.0*1024.0*1024.0)))+\"G\")\n# print('-----------------------------进程信息-------------------------------------')\n# # 查看系统全部进程\n# for pnum in psutil.pids():\n#     p = psutil.Process(pnum)\n#     print (\"进程名 %-20s  内存利用率 %-18s 进程状态 %-10s 创建时间 %-10s \"% (p.name(), p.memory_percent(), p.status(),  p.create_time()))"},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"colab_type":"code","id":"aF8IWdEowPP-","jupyter":{},"outputId":"cba1400c-7447-48d9-d71c-3381e7315f70","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Gen RAM Free: 16.4 GB  | Proc size: 467.8 MB\n","GPU RAM Free: 14057MB | Used: 10282MB | Util  42% | Total 24576MB\n"]}],"source":"# Check that we are using 100% of GPU memory footprint support libraries/code\n# from https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb\n\n\nimport psutil\nimport humanize\nimport os\nimport GPUtil as GPU\nGPUs = GPU.getGPUs()\n# XXX: only one GPU on Colab and isn’t guaranteed\ngpu = GPUs[0]\ndef printm():\n process = psutil.Process(os.getpid())\n print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\nprintm()"},{"cell_type":"code","execution_count":null,"metadata":{"id":"B928B744771F45C9979439538D09BB4D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":""},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lpKx43Iq6znw","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 加载数据集"},{"metadata":{"id":"E3FEC2E39C914C819989605B2481E66A","notebookId":"624d3eefbd5bb00017f2ba8d","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"### 加载方式1"},{"cell_type":"code","execution_count":18,"metadata":{"colab":{},"colab_type":"code","id":"x0hfGEe2LB9s","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using the latest cached version of the module from C:\\Users\\yanqiang\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\glue\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad (last modified on Thu Mar 24 22:33:16 2022) since it couldn't be found locally at glue., or remotely on the Hugging Face Hub.\n","Reusing dataset glue (C:\\Users\\yanqiang\\.cache\\huggingface\\datasets\\glue\\mrpc\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39569a8237d444b7a32d8047df8d2f2a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":"# 加载mrpc数据集\ndataset = load_dataset('glue', 'mrpc')"},{"cell_type":"code","execution_count":21,"metadata":{"id":"247FE38DB8A9460397F9795CAA30A2E2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# dataset"},{"cell_type":"code","execution_count":22,"metadata":{"id":"70B04F02D9584B80B5E9F5EADD39A301","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# 将数据保存为DataFrame格式，并保存成csv格式\ndataset['train'].to_pandas().to_csv('data/04/train.csv',index=None)\ndataset['test'].to_pandas().to_csv('data/04/test.csv',index=None)\ndataset['validation'].to_pandas().to_csv('data/04/validation.csv',index=None)"},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4346B3D26AF44199E99F606BBC38221","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":""},{"cell_type":"code","execution_count":23,"metadata":{"id":"4030B703046E478DBF8465C7106CF3E5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# 将数据转为DataFrame格式\ndf_train = dataset['train'].to_pandas()\ndf_val = dataset['test'].to_pandas()\ndf_test = dataset['validation'].to_pandas()"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RsjCBsJFF4Kn","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"如果想使用自己的数据集，可以将其上传到笔记本左侧。\n\n目前只处理 csv 文件，您需要上传三个文件：训练数据、验证数据和测试数据（带或不带标签）\n\n这是一个从 csv 文件加载数据的脚本，其标题如下：\n- 句子1\n- 句子2\n- 标签"},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"colab_type":"code","id":"nNs2FWNJSLSq","jupyter":{},"outputId":"3157c71e-8b2d-4a86-9c3d-7433be8f7022","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(3668, 4)\n","(1725, 4)\n","(408, 4)\n"]}],"source":"print(df_train.shape)\nprint(df_val.shape)\nprint(df_test.shape)"},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"colab_type":"code","id":"irj7itV0UCF_","jupyter":{},"outputId":"c7aedad8-f1c1-4407-f9d0-9c2d0b4b96b6","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence1</th>\n","      <th>sentence2</th>\n","      <th>label</th>\n","      <th>idx</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Amrozi accused his brother , whom he called \" ...</td>\n","      <td>Referring to him as only \" the witness \" , Amr...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Yucaipa owned Dominick 's before selling the c...</td>\n","      <td>Yucaipa bought Dominick 's in 1995 for $ 693 m...</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>They had published an advertisement on the Int...</td>\n","      <td>On June 10 , the ship 's owners had published ...</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Around 0335 GMT , Tab shares were up 19 cents ...</td>\n","      <td>Tab shares jumped 20 cents , or 4.6 % , to set...</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The stock rose $ 2.11 , or about 11 percent , ...</td>\n","      <td>PG &amp; E Corp. shares jumped $ 1.63 or 8 percent...</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           sentence1  \\\n","0  Amrozi accused his brother , whom he called \" ...   \n","1  Yucaipa owned Dominick 's before selling the c...   \n","2  They had published an advertisement on the Int...   \n","3  Around 0335 GMT , Tab shares were up 19 cents ...   \n","4  The stock rose $ 2.11 , or about 11 percent , ...   \n","\n","                                           sentence2  label  idx  \n","0  Referring to him as only \" the witness \" , Amr...      1    0  \n","1  Yucaipa bought Dominick 's in 1995 for $ 693 m...      0    1  \n","2  On June 10 , the ship 's owners had published ...      1    2  \n","3  Tab shares jumped 20 cents , or 4.6 % , to set...      0    3  \n","4  PG & E Corp. shares jumped $ 1.63 or 8 percent...      1    4  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":"df_train.head()"},{"metadata":{"id":"2E3A6DFF69AD446B8B70F3C65B0B3CBC","notebookId":"624d3eefbd5bb00017f2ba8d","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"mdEditEnable":false},"cell_type":"markdown","source":"### 加载方式2 直接读取CSV"},{"metadata":{"id":"C9D3115F6AEA432095FDE4FFC876100D","notebookId":"624d3eefbd5bb00017f2ba8d","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","outputs":[],"source":"import pandas as pd\ndf_train = pd.read_csv('/home/mw/input/task048673/train.csv')\ndf_val = pd.read_csv('/home/mw/input/task048673/validation.csv')\ndf_test = pd.read_csv('/home/mw/input/task048673/test.csv')","execution_count":null},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TWJfh6DV7CB5","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 数据集与模型定义"},{"cell_type":"code","execution_count":28,"metadata":{"colab":{},"colab_type":"code","id":"Tc1GQh7yEm4C","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"class CustomDataset(Dataset):\n\n    def __init__(self, data, maxlen, with_labels=True, bert_model='albert-base-v2'):\n\n        self.data = data  # pandas dataframe\n        #Initialize the tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)  \n\n        self.maxlen = maxlen\n        self.with_labels = with_labels \n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n\n        #根据索引索取DataFrame中句子1余句子2\n        sent1 = str(self.data.loc[index, 'sentence1'])\n        sent2 = str(self.data.loc[index, 'sentence2'])\n\n        # 对句子对分词，得到input_ids、attention_mask和token_type_ids\n        encoded_pair = self.tokenizer(sent1, sent2, \n                                      padding='max_length',  # 填充到最大长度\n                                      truncation=True,  # 根据最大长度进行截断\n                                      max_length=self.maxlen,  \n                                      return_tensors='pt')  # 返回torch.Tensor张量\n        \n        token_ids = encoded_pair['input_ids'].squeeze(0)  # tensor token ids\n        attn_masks = encoded_pair['attention_mask'].squeeze(0)  # padded values对应为 \"0\" ，其他token为1\n        token_type_ids = encoded_pair['token_type_ids'].squeeze(0)  #第一个句子的值为0，第二个句子的值为1 # 只有一句全为0\n\n        if self.with_labels:  # True if the dataset has labels\n            label = self.data.loc[index, 'label']\n            return token_ids, attn_masks, token_type_ids, label  \n        else:\n            return token_ids, attn_masks, token_type_ids"},{"cell_type":"code","execution_count":29,"metadata":{"colab":{},"colab_type":"code","id":"hm0lAXvTZChm","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"class SentencePairClassifier(nn.Module):\n\n    def __init__(self, bert_model=\"albert-base-v2\", freeze_bert=False):\n        super(SentencePairClassifier, self).__init__()\n        #  初始化预训练模型Bert xxx\n        self.bert_layer = AutoModel.from_pretrained(bert_model)\n\n        #  encoder 隐藏层大小\n        if bert_model == \"albert-base-v2\":  # 12M 参数\n            hidden_size = 768\n        elif bert_model == \"albert-large-v2\":  # 18M 参数\n            hidden_size = 1024\n        elif bert_model == \"albert-xlarge-v2\":  # 60M 参数\n            hidden_size = 2048\n        elif bert_model == \"albert-xxlarge-v2\":  # 235M 参数\n            hidden_size = 4096\n        elif bert_model == \"bert-base-uncased\": # 110M 参数\n            hidden_size = 768\n        elif bert_model == \"roberta-base\": # \n            hidden_size = 768\n\n        # 固定Bert层 更新分类输出层\n        if freeze_bert:\n            for p in self.bert_layer.parameters():\n                p.requires_grad = False\n                \n        self.dropout = nn.Dropout(p=0.1)\n        # 分类输出\n        self.cls_layer = nn.Linear(hidden_size, 1)\n\n\n    @autocast()  # 混合精度训练\n    def forward(self, input_ids, attn_masks, token_type_ids):\n        '''\n        Inputs:\n            -input_ids : Tensor  containing token ids\n            -attn_masks : Tensor containing attention masks to be used to focus on non-padded values\n            -token_type_ids : Tensor containing token type ids to be used to identify sentence1 and sentence2\n        '''\n\n        # 输入给Bert，获取上下文表示\n        # cont_reps, pooler_output = self.bert_layer(input_ids, attn_masks, token_type_ids)\n        outputs = self.bert_layer(input_ids, attn_masks, token_type_ids)\n        # last_hidden_state,pooler_output,all_hidden_states 12层\n        # 将last layer hidden-state of the [CLS] 输入到 classifier layer\n        # - last_hidden_state 的向量平均\n        # - 取all_hidden_states最后四层，然后做平均 weighted 平均\n        # - last_hidden_state+lstm\n        # 获取输出\n        logits = self.cls_layer(self.dropout(outputs['pooler_output']))\n\n        return logits"},{"cell_type":"code","execution_count":30,"metadata":{"colab":{},"colab_type":"code","id":"5SrSNNYTjwe8","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"def set_seed(seed):\n    \"\"\" 固定随机种子，保证结果复现\n    \"\"\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n\ndef evaluate_loss(net, device, criterion, dataloader):\n    \"\"\"\n    评估输出\n    \"\"\"\n    net.eval()\n\n    mean_loss = 0\n    count = 0\n\n    with torch.no_grad():\n        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(dataloader)):\n            seq, attn_masks, token_type_ids, labels = \\\n                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n            logits = net(seq, attn_masks, token_type_ids)\n            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n            count += 1\n\n    return mean_loss / count"},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"colab_type":"code","id":"Hl-rhuWsrg01","jupyter":{},"outputId":"469f2b56-d92d-448e-d92c-b68ebe76f531","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["创建models目录\n"]},{"name":"stderr","output_type":"stream","text":["子目录或文件 models 已经存在。\n"]}],"source":"print(\"创建models目录\")\n!mkdir models"},{"cell_type":"code","execution_count":32,"metadata":{"id":"9E1A0BE29364491B945566A4E5342DC9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# bs=2\n# accumu_step=2\n\n# =>bs=2*2=4"},{"cell_type":"code","execution_count":33,"metadata":{"colab":{},"colab_type":"code","id":"I-o6KyaFkU5u","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"def train_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate):\n\n    best_loss = np.Inf\n    best_ep = 1\n    nb_iterations = len(train_loader)\n    print_every = nb_iterations // 5  # 打印频率\n    iters = []\n    train_losses = []\n    val_losses = []\n\n    scaler = GradScaler()\n\n    for ep in range(epochs):\n\n        net.train()\n        running_loss = 0.0\n        for it, (seq, attn_masks, token_type_ids, labels) in enumerate(tqdm(train_loader)):\n\n            # 转为cuda张量\n            seq, attn_masks, token_type_ids, labels = \\\n                seq.to(device), attn_masks.to(device), token_type_ids.to(device), labels.to(device)\n    \n            # 混合精度加速训练\n            with autocast():\n                # Obtaining the logits from the model\n                logits = net(seq, attn_masks, token_type_ids)\n\n                # Computing loss\n                loss = criterion(logits.squeeze(-1), labels.float())\n                loss = loss / iters_to_accumulate  # Normalize the loss because it is averaged\n\n            # Backpropagating the gradients\n            # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n            scaler.scale(loss).backward()\n\n            if (it + 1) % iters_to_accumulate == 0:\n                # Optimization step\n                # scaler.step() first unscales the gradients of the optimizer's assigned params.\n                # If these gradients do not contain infs or NaNs, opti.step() is then called,\n                # otherwise, opti.step() is skipped.\n                scaler.step(opti)\n                # Updates the scale for next iteration.\n                scaler.update()\n                # 根据迭代次数调整学习率。\n                lr_scheduler.step()\n                # 梯度清零\n                opti.zero_grad()\n\n\n            running_loss += loss.item()\n\n            if (it + 1) % print_every == 0:  # Print training loss information\n                print()\n                print(\"Iteration {}/{} of epoch {} complete. Loss : {} \"\n                      .format(it+1, nb_iterations, ep+1, running_loss / print_every))\n\n                running_loss = 0.0\n\n\n        val_loss = evaluate_loss(net, device, criterion, val_loader)  # Compute validation loss\n        print()\n        print(\"Epoch {} complete! Validation Loss : {}\".format(ep+1, val_loss))\n\n        if val_loss < best_loss:\n            print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n            print()\n            net_copy = copy.deepcopy(net)  # # 保存最优模型\n            best_loss = val_loss\n            best_ep = ep + 1\n\n    # 保存模型\n    path_to_model='models/{}_lr_{}_val_loss_{}_ep_{}.pt'.format(bert_model, lr, round(best_loss, 5), best_ep)\n    torch.save(net_copy.state_dict(), path_to_model)\n    print(\"The model has been saved in {}\".format(path_to_model))\n\n    del loss\n    torch.cuda.empty_cache() # 清空显存"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WFy9kQ2-SvQ2","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 超参数设置"},{"cell_type":"code","execution_count":34,"metadata":{"colab":{},"colab_type":"code","id":"b6bzDp4FreS6","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"bert_model = \"albert-base-v2\"  # 'albert-base-v2', 'albert-large-v2', 'albert-xlarge-v2', 'albert-xxlarge-v2', 'bert-base-uncased', ...\nfreeze_bert = False  # 是否冻结Bert\nmaxlen = 128  # 最大长度\nbs = 16  # batch size\niters_to_accumulate = 2  # 梯度累加\nlr = 2e-5  # learning rate\nepochs = 4  # 训练轮数"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y_abThXlSr6n","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 训练与评估"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wwXCoj9_h1hY","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"AdamW优化器介绍 :\nhttps://huggingface.co/transformers/main_classes/optimizer_schedules.html"},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6f797e5377fe4ab281003d537c15f279","9b46878367244033ac0d96f6f83766cd","b9b102512ad548ac96f5560eead35201","cb3cf3d7089347a092499762c552f785","4ae80720349c409bba8dffa6ba429e9e","c74fcaee125642908ab8fa8905f0cf6f","6a229158e0db415b8561dc22a8a4ab47","ef906f1d1e444404bd88e789e14e977a","80d383da347f41a6927d659dc5e1a855","26425569e06e4fc5b32405bcdba37dad","c6897db24c284263879bcc92e6d56f39","7468d3da8c8547d38e146b8adee8948e","e90a00d220bc4546923517f13daf29be","13b7985246244fcb89926039ded8eb20","3e108b6d60874974aaf0eea761e7da59","afe53d24858d446592d1825713664d9b","ac95062870cd4eb3afc93dfcc8ef9a6e","d79473c349744b6ba4d1532889a3dd3b","bac3fa1e8ff04a1680267702e35c89b1","c425744e85d44659b143679eccca5b0f","a9c4247af57f46d485a4f509eafc79e3","6a4d58a9a2a84f09b3d341529de530ed","c07487bb1e8c4415986d31752fa0d7f5","9a003cc981484793b31167d114362ea4"]},"colab_type":"code","id":"VZWGPomoryxy","jupyter":{},"outputId":"25d2f91d-ed8e-4d59-b6cf-cffaff9aaf4b","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading training data...\n","Reading validation data...\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","F:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"," 20%|████████████████▌                                                                | 47/230 [00:08<00:25,  7.32it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 46/230 of epoch 1 complete. Loss : 0.3110983663279077 \n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████████████████████████████████▊                                                | 93/230 [00:15<00:21,  6.40it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 92/230 of epoch 1 complete. Loss : 0.2986520425133083 \n"]},{"name":"stderr","output_type":"stream","text":[" 60%|████████████████████████████████████████████████▎                               | 139/230 [00:22<00:13,  6.56it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 138/230 of epoch 1 complete. Loss : 0.2599153120232665 \n"]},{"name":"stderr","output_type":"stream","text":[" 80%|████████████████████████████████████████████████████████████████▎               | 185/230 [00:30<00:07,  6.25it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 184/230 of epoch 1 complete. Loss : 0.2446880199663017 \n"]},{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 230/230 [00:37<00:00,  6.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 230/230 of epoch 1 complete. Loss : 0.2166263344495193 \n"]},{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:07<00:00, 15.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 1 complete! Validation Loss : 0.42332161493875364\n","Best validation loss improved from inf to 0.42332161493875364\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|████████████████▏                                                                | 46/230 [00:07<00:25,  7.27it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 46/230 of epoch 2 complete. Loss : 0.22176416088705478 \n"]},{"name":"stderr","output_type":"stream","text":[" 41%|█████████████████████████████████                                                | 94/230 [00:14<00:17,  7.96it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 92/230 of epoch 2 complete. Loss : 0.2041886290130408 \n"]},{"name":"stderr","output_type":"stream","text":[" 60%|████████████████████████████████████████████████▎                               | 139/230 [00:21<00:12,  7.39it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 138/230 of epoch 2 complete. Loss : 0.1654406607798908 \n"]},{"name":"stderr","output_type":"stream","text":[" 80%|████████████████████████████████████████████████████████████████▎               | 185/230 [00:28<00:07,  6.15it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 184/230 of epoch 2 complete. Loss : 0.1639021173443483 \n"]},{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 230/230 [00:35<00:00,  6.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 230/230 of epoch 2 complete. Loss : 0.1271723288556804 \n"]},{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:06<00:00, 16.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 2 complete! Validation Loss : 0.34700438214672935\n","Best validation loss improved from 0.42332161493875364 to 0.34700438214672935\n","\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|████████████████▌                                                                | 47/230 [00:07<00:25,  7.29it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 46/230 of epoch 3 complete. Loss : 0.14332275821462923 \n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████████████████████████████████▊                                                | 93/230 [00:14<00:22,  6.16it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 92/230 of epoch 3 complete. Loss : 0.1258775751389887 \n"]},{"name":"stderr","output_type":"stream","text":[" 60%|████████████████████████████████████████████████▎                               | 139/230 [00:21<00:14,  6.30it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 138/230 of epoch 3 complete. Loss : 0.10184601485810202 \n"]},{"name":"stderr","output_type":"stream","text":[" 80%|████████████████████████████████████████████████████████████████                | 184/230 [00:28<00:06,  7.05it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 184/230 of epoch 3 complete. Loss : 0.08539749997789445 \n"]},{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 230/230 [00:35<00:00,  6.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 230/230 of epoch 3 complete. Loss : 0.07792691959311134 \n"]},{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:06<00:00, 15.89it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 3 complete! Validation Loss : 0.392297743095292\n"]},{"name":"stderr","output_type":"stream","text":[" 20%|████████████████▌                                                                | 47/230 [00:07<00:31,  5.77it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 46/230 of epoch 4 complete. Loss : 0.07297414857084336 \n"]},{"name":"stderr","output_type":"stream","text":[" 40%|████████████████████████████████▊                                                | 93/230 [00:14<00:21,  6.30it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 92/230 of epoch 4 complete. Loss : 0.0749326630131058 \n"]},{"name":"stderr","output_type":"stream","text":[" 60%|████████████████████████████████████████████████▎                               | 139/230 [00:21<00:14,  6.18it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 138/230 of epoch 4 complete. Loss : 0.04860836908261737 \n"]},{"name":"stderr","output_type":"stream","text":[" 80%|████████████████████████████████████████████████████████████████▎               | 185/230 [00:28<00:07,  6.09it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 184/230 of epoch 4 complete. Loss : 0.05297895232417985 \n"]},{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 230/230 [00:35<00:00,  6.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Iteration 230/230 of epoch 4 complete. Loss : 0.035685401976756424 \n"]},{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:06<00:00, 15.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 4 complete! Validation Loss : 0.3996199344595273\n","The model has been saved in models/albert-base-v2_lr_2e-05_val_loss_0.347_ep_2.pt\n"]}],"source":"#  固定随机种子 便于复现\nset_seed(1) # 2022 \n\n# 创建训练集与验证集\nprint(\"Reading training data...\")\ntrain_set = CustomDataset(df_train, maxlen, bert_model)\nprint(\"Reading validation data...\")\nval_set = CustomDataset(df_val, maxlen, bert_model)\n# 常见训练集与验证集DataLoader\ntrain_loader = DataLoader(train_set, batch_size=bs, num_workers=0)\nval_loader = DataLoader(val_set, batch_size=bs, num_workers=0)\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnet = SentencePairClassifier(bert_model, freeze_bert=freeze_bert)\n\nif torch.cuda.device_count() > 1:  # if multiple GPUs\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    net = nn.DataParallel(net)\n\nnet.to(device)\n\ncriterion = nn.BCEWithLogitsLoss()\nopti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2)\nnum_warmup_steps = 0 # The number of steps for the warmup phase.\nnum_training_steps = epochs * len(train_loader)  # The total number of training steps\nt_total = (len(train_loader) // iters_to_accumulate) * epochs  # Necessary to take into account Gradient accumulation\nlr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)\n\ntrain_bert(net, criterion, opti, lr, lr_scheduler, train_loader, val_loader, epochs, iters_to_accumulate)"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gw2sOrIvCEZz","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"<!-- 您可以通过浏览notebook左侧的文件下载保存在“models”文件夹中的模型 -->"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nDBtVu7JSbUK","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 预测"},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"colab_type":"code","id":"kAfbt0FjkCfM","jupyter":{},"outputId":"6ffdbfb5-fa8d-4329-9075-79c5152c952b","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Creation of the results' folder...\n"]},{"name":"stderr","output_type":"stream","text":["子目录或文件 results 已经存在。\n"]}],"source":"print(\"Creation of the results' folder...\")\n!mkdir results"},{"cell_type":"code","execution_count":null,"metadata":{"id":"671B435F5A3F445CBF5765E3F4BCF84E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"# [-0.1,0.1,-0.5]"},{"cell_type":"code","execution_count":18,"metadata":{"colab":{},"colab_type":"code","id":"m3r8_npVf30D","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"def get_probs_from_logits(logits):\n    \"\"\"\n    Converts a tensor of logits into an array of probabilities by applying the sigmoid function\n    \"\"\"\n    probs = torch.sigmoid(logits.unsqueeze(-1))\n    return probs.detach().cpu().numpy()\n\ndef test_prediction(net, device, dataloader, with_labels=True, result_file=\"results/output.txt\"):\n    \"\"\"\n    Predict the probabilities on a dataset with or without labels and print the result in a file\n    \"\"\"\n    net.eval()\n    w = open(result_file, 'w')\n    probs_all = []\n\n    with torch.no_grad():\n        if with_labels:\n            for seq, attn_masks, token_type_ids, _ in tqdm(dataloader):# 训练集、验证集\n                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n                logits = net(seq, attn_masks, token_type_ids)\n                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n                probs_all += probs.tolist()\n        else:\n            for seq, attn_masks, token_type_ids in tqdm(dataloader): # 没有标签的测试集\n                seq, attn_masks, token_type_ids = seq.to(device), attn_masks.to(device), token_type_ids.to(device)\n                logits = net(seq, attn_masks, token_type_ids)\n                probs = get_probs_from_logits(logits.squeeze(-1)).squeeze(-1)\n                probs_all += probs.tolist()\n\n    w.writelines(str(prob)+'\\n' for prob in probs_all)\n    w.close()"},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"colab_type":"code","id":"xWoWiw6MlPm-","jupyter":{},"outputId":"31072be1-20ed-43b4-a7c3-50a2d9fd78da","slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading test data...\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.LayerNorm.weight']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["\n","Loading the weights of the model...\n","Predicting on test data...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████████████████████████████████████████████████████████████████████████████| 26/26 [00:01<00:00, 19.23it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Predictions are available in : results/output.txt\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":"path_to_model = 'models/albert-base-v2_lr_2e-05_val_loss_0.347_ep_2.pt'  \n# path_to_model = '/content/models/...'  # You can add here your trained model\n\npath_to_output_file = 'results/output.txt'\n\nprint(\"Reading test data...\")\ntest_set = CustomDataset(df_test, maxlen, bert_model)\ntest_loader = DataLoader(test_set, batch_size=bs, num_workers=0)\n\nmodel = SentencePairClassifier(bert_model)\nif torch.cuda.device_count() > 1:  # if multiple GPUs\n    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n    model = nn.DataParallel(model)\n\nprint()\nprint(\"Loading the weights of the model...\")\nmodel.load_state_dict(torch.load(path_to_model))\nmodel.to(device)\n\nprint(\"Predicting on test data...\")\ntest_prediction(net=model, device=device, dataloader=test_loader, with_labels=True,  # set the with_labels parameter to False if your want to get predictions on a dataset without labels\n                result_file=path_to_output_file)\nprint()\nprint(\"Predictions are available in : {}\".format(path_to_output_file))"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HCVAtClcC1qT","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"预测结果保存到results下面"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ywxq1c8DSiV3","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 评估"},{"cell_type":"code","execution_count":52,"metadata":{"colab":{},"colab_type":"code","id":"6JYwEPtrlBFX","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"outputs":[],"source":"path_to_output_file = 'results/output.txt'  # 预测结果概率文件\n\nlabels_test = df_test['label']  # true labels\n\nprobs_test = pd.read_csv(path_to_output_file, header=None)[0]  # 预测概率\nthreshold = 0.6   # you can adjust this threshold for your own dataset\npreds_test=(probs_test>=threshold).astype('uint8') # predicted labels using the above fixed threshold\n\n# metric = load_metric(\"glue\", \"mrpc\")"},{"cell_type":"code","execution_count":53,"metadata":{"id":"449AB6133FE4459A8BE232785547F430","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.972656</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.153931</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.172852</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.881836</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.091248</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>403</th>\n","      <td>0.063599</td>\n","    </tr>\n","    <tr>\n","      <th>404</th>\n","      <td>0.193604</td>\n","    </tr>\n","    <tr>\n","      <th>405</th>\n","      <td>0.986328</td>\n","    </tr>\n","    <tr>\n","      <th>406</th>\n","      <td>0.089111</td>\n","    </tr>\n","    <tr>\n","      <th>407</th>\n","      <td>0.784180</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>408 rows × 1 columns</p>\n","</div>"],"text/plain":["            0\n","0    0.972656\n","1    0.153931\n","2    0.172852\n","3    0.881836\n","4    0.091248\n","..        ...\n","403  0.063599\n","404  0.193604\n","405  0.986328\n","406  0.089111\n","407  0.784180\n","\n","[408 rows x 1 columns]"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":"pd.read_csv(path_to_output_file, header=None)"},{"cell_type":"code","execution_count":54,"metadata":{"id":"43CE3DFD87A2478A870372F28A2B3783","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"data":{"text/plain":["0      1\n","1      0\n","2      0\n","3      1\n","4      0\n","      ..\n","403    0\n","404    0\n","405    1\n","406    0\n","407    1\n","Name: 0, Length: 408, dtype: uint8"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":"preds_test"},{"cell_type":"code","execution_count":55,"metadata":{"id":"6E76EFDB63C24CF4BE4D4145BAB7CFED","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"from sklearn.metrics import classification_report"},{"cell_type":"code","execution_count":56,"metadata":{"id":"73CE188A77B14ECD9240842B19F46D05","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.74      0.88      0.80       129\n","           1       0.94      0.86      0.90       279\n","\n","    accuracy                           0.87       408\n","   macro avg       0.84      0.87      0.85       408\n","weighted avg       0.88      0.87      0.87       408\n","\n"]}],"source":"print(classification_report(labels_test,preds_test))"},{"cell_type":"code","execution_count":57,"metadata":{"id":"FB03B85246D145978726A43D27CCA4BE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":"from sklearn.metrics import confusion_matrix"},{"cell_type":"code","execution_count":58,"metadata":{"id":"161C78A860ED4287840B5308E38C6A33","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[[113  16]\n"," [ 39 240]]\n"]}],"source":"print(confusion_matrix(labels_test,preds_test))"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4ExhSydG0eeq","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 可以尝试的想法\n\n- 尝试其他预训练模型：https://huggingface.co/models\n- 尝试其他优化器和学习率调度器\n- 调整超参数：批量大小、梯度累积参数 (iters_to_accumulate)、轮数、学习率\n- 更改 *maxlen* 参数（最大值：512）。 如果增加它，训练将需要更长的时间\n- 观察冻结编码器权重并仅更新分类器权重的影响\n- 根据任务和数据集使用其他指标（Precision、Recall、ROC AUC、Precision-recall AUC 等）"},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GZp5xdVwvWVC","jupyter":{},"slideshow":{"slide_type":"slide"},"tags":[],"trusted":true},"source":"## 进一步优化\n\n- 调整代码，以便其他基于 BERT 的模型（如 RoBERTa 和 DistillBERT）也可以针对此任务进行微调\n- 实验向分类层提供最后一层隐藏状态的所有输入标记的平均值或使用多个编码器层而不是池化器输出的其他操作"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5467BBFF23CD4F9ABF11CF6AC3DB3B40","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"outputs":[],"source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":4}